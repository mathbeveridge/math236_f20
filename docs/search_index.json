[["index.html", "MATH 236: Linear Algebra Preface", " MATH 236: Linear Algebra Preface This is the class handbook for Math 236 Linear Algebra at Macalester College. The content here was made by Andrew Beveridge and draws upon material prepared by Tom Halverson and other faculty in the Department of Mathematics, Statistics and Computer Science at Macalester College. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["problem-set-1.html", "Vector 1 Problem Set 1 1.1 Characterize the Solution Set 1.2 Find the Parametric Solution 1.3 Elementary row operations are reversible 1.4 Traffic Flow 1.5 Parabolas Through Two Points", " Vector 1 Problem Set 1 Due: Tuesday November 3 by 11:55pm CST. Upload your solutions to Moodle in a PDF. Please feel free to use RStudio for all row reductions. You can download the Rmd source file for this problem set. 1.1 Characterize the Solution Set The following augmented matrices are in row echelon form. Decide whether the set of solutions is a point, line, plane, or the empty set in 3-space. Briefly justify your answer. \\(\\left[ \\begin{array}{ccc|c} 1 &amp; 3 &amp; -1 &amp; 4 \\\\ 0 &amp; 1 &amp; 4 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 2 \\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{ccc|c} 1 &amp; 3 &amp; -1 &amp; 5 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{ccc|c} 1 &amp; -1 &amp; 0 &amp; -2 \\\\ 0 &amp; 0 &amp; 1 &amp; 7\\\\ 0 &amp; 0 &amp; 0 &amp; 1\\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{ccc|c} 0 &amp; 1 &amp; 0 &amp; 6 \\\\ 0 &amp; 0 &amp; 1 &amp; -2 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right]\\) 1.2 Find the Parametric Solution Each of the following matrices is the reduced row echelon form of the augmented matrix of a system of linear equations. Give the general solution to each system. \\(\\left[ \\begin{array}{cccc|c} 1 &amp; 3 &amp; 0 &amp; -2 &amp; 5\\\\ 0 &amp; 0 &amp; 1 &amp; 4 &amp; -2 \\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{ccccc|c} 1 &amp; 0 &amp; 4 &amp; 0 &amp; 3 &amp; 6\\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; -2&amp; -8 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -1 &amp; 3 \\\\ \\end{array} \\right]\\) \\(\\left[ \\begin{array}{cccc|c} 1 &amp; 4 &amp; 0 &amp; 0 &amp; -2 \\\\ 0 &amp; 0 &amp; 1 &amp; 7 &amp; 6\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right]\\) 1.3 Elementary row operations are reversible In each case below, an elementary row operation turns the matrix \\(A\\) into the matrix \\(B\\). For each of them, Describe the row operation that turns \\(A\\) into \\(B\\), and Describe the row operation that turns \\(B\\) into \\(A\\). Give your answers in the form: “scale \\(R_2\\) by 3” or “swap \\(R_1\\) and \\(R_4\\)” or “replace \\(R_3\\) with \\(R_3 + \\frac{1}{5} R_1\\).” \\[A=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\longrightarrow B=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 0 &amp; 7 &amp; 0 &amp; -4 \\\\ \\end{array} \\right]\\] \\[A=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\longrightarrow B=\\left[ \\begin{array}{cccc} 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\] \\[A=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 2 &amp; 8 &amp; 2 &amp; -4 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\longrightarrow B=\\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 1 &amp; 3 \\\\ 1 &amp; -2 &amp; 2 &amp; 1 \\\\ 1 &amp; 4 &amp; 1 &amp; -2 \\\\ 3 &amp; 1 &amp; 6 &amp; -1 \\\\ \\end{array} \\right]\\] 1.4 Traffic Flow Below you find a section of one-way streets in downtown St Paul, where the arrows indicate traffic direction. The traffic control center has installed electronic sensors that count the numbers of vehicles passing through the 6 streets that lead into and out of this area. Assume that the total flow that enters each intersection equals the the total flow that leaves each intersection (we will ignore parking and staying). Create a system of linear equations to find the possible flow values for the inner streets \\(x_1, x_2, x_3, x_4\\). Use RStudio to find the solution set. Write your answer in parametric form. Your answer to part 2 finds an infinite solution set. In terms of traffic flow, explain why there can be more than one solution. Thinking about cars traveling along streets, why is there some flexibility in the overall flow of cars? Now critique the model as presented. What solutions to the linear system are problematic for our original traffic flow problem? What additional conditions should we require for a valid solution? 1.5 Parabolas Through Two Points You will find the family of all possible parabolas \\[ f(x) = a x^2 + b x + c \\] through two given points. Note that there will be an infinite number of such polynomials: there are three unknowns, but only two constraints. So there will be at least one free variable. Here are the plots of the two different families of parabolas that you will consider. For each pair of points below: Create linear system that represents the problem Use RStudio to find the RREF of the augmented matrix for the system and Write down the general formula for the family of parabolas \\(f(x)\\) that contain the points. Here are the pairs of points. \\(p_1 = (1,0)\\) and \\(p_2 = (5,0)\\) \\(q_1= (1,-3)\\) and \\(q_2 = (5,10)\\) Part (a.i) characterizes the family of parabolas that contain the points \\(p_1\\) and \\(p_2\\). Let’s refer to this family of parabolas as our “\\(p_1,p_2\\) family.” Pick any two functions \\(g_1(x)\\) and \\(g_2(x)\\) from this \\(p_1,p_2\\) family of parabolas. Now consider the parabola \\[ g(x) = g_1(x) - g_2(x). \\] Show that \\(g(1)=0\\) and \\(g(5)=0\\). This means that \\(g(x)\\) is also in our \\(p_1,p_2\\) family! (That’s really cool!) Part (a.ii) characterizes the family of parabolas that contain the points \\(q_1\\) and \\(q_2\\). Let’s refer to this family of parabolas as our “\\(q_1,q_2\\) family.” Pick any two functions \\(h_1(x)\\) and \\(h_2(x)\\) in this \\(q_1,q_2\\) family. Now consider the parabola \\[ h(x) = h_1(x) - h_2(x). \\] Show that \\(h(1) \\neq -3\\) and \\(h(5) \\neq 10\\). This means that \\(h(x)\\) is NOT in our \\(q_1,q_2\\) family. (So this family is not as nice as the previous one.) Now explain why the parabola \\(h(x)\\) from part 3 is actually a member of the \\(p_1, p_2\\) family. (So maybe this family is nicer than we thought!) Remark: This problem is about families of parabolas. But at its core, this is actually a problem about linear algebra. Obviously, we used linear algebra to solve part 1, but there is linear algebra in the other parts as well! We define \\(g(x) = g_1(x) - g_2(x)\\) by taking a linear combination of two functions. (Just like we take linear combinations of vectors.) What leads to the “nice” behavior of the \\(p_1, p_2\\) family in part 2? It’s because the \\(y\\)-values of the constraining points are both 0. We will soon learn about homogeneous linear systems whose right hand sides are all zero. These homogeneous systems have some very special properties. The \\(q_1, q_2\\) family is “not nice” because it a nonhomogenous system. Just as there is a connection betwen the \\(p_1,p_2\\) family and the \\(q_1, q_2\\) family, we will soon learn that there is a connection between homogeneous systems and nonhomogeneous systems. "],["problem-set-2.html", "Vector 2 Problem Set 2 2.1 Parametric Vector Form 2.2 RREF for a linear system 2.3 RREF for a set of vectors 2.4 Removing free variable columns from a matrix 2.5 A square matrix 2.6 Combining solutions to \\(A \\mathsf{x} = \\mathsf{b}\\)", " Vector 2 Problem Set 2 Due: Friday November 6 by 5:00pm CST. Upload your solutions to Moodle in a PDF. Please feel free to use RStudio for all row reductions. You can download the Rmd source file for this problem set. The Problem Set covers sections 1.3, 1.4, 1.5. 2.1 Parametric Vector Form Here is the augmented matrix for a system of linear equations \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\), and its RREF. Give the complete solution to this system in parametric vector form. \\[\\left[ \\begin{array}{ccccc|c} 1 &amp; 1 &amp; -1 &amp; -1 &amp; 2 &amp; 1 \\\\ 1 &amp; 0 &amp; -2 &amp; 1 &amp; 1 &amp; 3 \\\\ -2 &amp; 1 &amp; 5 &amp; 1 &amp; -6 &amp; 2 \\\\ -3 &amp; 0 &amp; 6 &amp; 2 &amp; -8 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 &amp; 2 &amp; -3 &amp; 6 \\\\ 1 &amp; 0 &amp; -2 &amp; -1 &amp; 3 &amp; -1 \\\\ \\end{array} \\right] \\longrightarrow \\left[ \\begin{array}{ccccc|c} 1 &amp; 0 &amp; -2 &amp; 0 &amp; 2 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 &amp; -1 &amp; 2 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; -1 &amp; 2 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\] 2.2 RREF for a linear system Here is the reduced row echelon form of a matrix \\(\\mathsf{A}\\) (you are not given the matrix \\(\\mathsf{A}\\)). \\[ \\mathsf{A} \\longrightarrow \\left[ \\begin{array}{cccc} 1 &amp; -2 &amp; 0 &amp; 4 \\\\ 0 &amp; 0 &amp; 1 &amp; -5 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\] Give the parametric equations of the general solution to the homogenous equation \\(\\mathsf{A} \\mathsf{x} = {\\bf 0}\\). Describe the geometric form of your answer to part (a). For example, you answer should be something like: “it is a plane in \\(\\mathbb{R}^3\\)” or “it is a line in \\(\\mathbb{R}^7\\)” or “it is a point in \\(\\mathbb{R}^4\\).” Suppose that we also know that \\(\\mathsf{A}\\begin{bmatrix} 4 \\\\ 1 \\\\ -3 \\\\ 2 \\\\ \\end{bmatrix} = \\begin{bmatrix} 22 \\\\ -13 \\\\ 7 \\\\ \\end{bmatrix}\\). Then give the general solution to \\(\\mathsf{A} \\mathsf{x}= \\begin{bmatrix} 22 \\\\ -13\\\\ 7 \\\\ \\end{bmatrix}\\) in parametric form. 2.3 RREF for a set of vectors Suppose that we have five vectors \\(\\mathsf{v}_1, \\mathsf{v}_2,\\mathsf{v}_3,\\mathsf{v}_4,\\mathsf{v}_5\\) in \\(\\mathbb{R}^4\\) and that the matrix \\[ A = \\left[ \\begin{array}{ccc} \\mid &amp; \\mid &amp; \\mid &amp; \\mid &amp; \\mid \\\\ \\mathsf{v}_1 &amp; \\mathsf{v}_2 &amp; \\mathsf{v}_3 &amp;\\mathsf{v}_4 &amp;\\mathsf{v}_5 \\\\ \\mid &amp; \\mid &amp; \\mid &amp; \\mid &amp; \\mid \\end{array} \\right] \\] has reduced row echelon form \\[ \\begin{bmatrix} 1 &amp; 0 &amp; -3 &amp; 0 &amp; 2 \\\\ 0 &amp; 1 &amp; 4 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix}. \\] Do the vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4, \\mathsf{v}_5\\) span \\(\\mathbb{R}^4\\)? Justify your answer. Is the vector \\(\\mathsf{v}_3\\) in \\(\\mathrm{span}(\\mathsf{v}_1,\\mathsf{v}_2)\\)? Justify your answer. Pick any \\(\\mathsf{b}\\) in \\(\\mathrm{span}(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4, \\mathsf{v}_5)\\). Is there always a unqiue way to write \\(\\mathsf{b}\\) as a linear combination of \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4, \\mathsf{v}_5\\)? Justify your answer. 2.4 Removing free variable columns from a matrix Consider the matrix \\[ A =\\left[ \\begin{array}{cccccc} 6 &amp; 5 &amp; -3 &amp; 4 &amp; 2 &amp; -9 \\\\ -7 &amp; -6 &amp; 4 &amp; -5 &amp; -7 &amp; 16 \\\\ -4 &amp; -3 &amp; -1 &amp; 0 &amp; -8 &amp; 9 \\\\ 8 &amp; 7 &amp; -5 &amp; 6 &amp; 1 &amp; -12 \\end{array} \\right]. \\] Use RStudio to show that the columns of \\(\\mathsf{A}\\) span \\(\\mathbb{R}^4\\). Write down matrix \\(\\mathsf{A}&#39;\\) that you get by removing the free variable columns from \\(\\mathsf{A}\\). Without using additional calculations on RStudio, explain why the new system \\(\\mathsf{A}&#39; \\mathsf{x} = \\mathsf{b}\\) is consistent and has a unique solution for every choice of \\(\\mathsf{b} \\in \\mathbb{R}^4\\). 2.5 A square matrix Suppose that \\(A\\) is a \\(5\\times 5\\) matrix and \\(\\mathsf{b}\\) is a vector in \\(\\mathbb{R}^5\\) with the property that \\(A\\mathsf{x}=\\mathsf{b}\\) has a unique solution. Explain why the columns of \\(A\\) must span \\(\\mathbb{R}^5\\). Use the reduced row echelon form of \\(A\\) in your explanation. 2.6 Combining solutions to \\(A \\mathsf{x} = \\mathsf{b}\\) Suppose that \\(\\mathsf{x}_1\\) and \\(\\mathsf{x}_2\\) are solutions to \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\) (where \\(\\mathsf{b} \\not= \\mathsf{0}\\)). Decide if any of the following are also solutions to \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\). \\(\\mathsf{x}_1+ \\mathsf{x}_2\\) \\(\\mathsf{x}_1 - \\mathsf{x}_2\\) \\(\\frac{1}{2} ( \\mathsf{x}_1 + \\mathsf{x}_2)\\) \\(\\frac{5}{2} \\mathsf{x}_1 - \\frac{3}{2} \\mathsf{x}_2\\). Under what conditions on \\(c\\) and \\(d\\) is \\(\\mathsf{x} = c \\mathsf{x}_1 + d \\mathsf{x}_2\\) a solution to \\(\\mathsf{A} \\mathsf{x} = \\mathsf{b}\\)? Justify your answer. Let \\(\\mathsf{u}\\) be the vector that points to \\(1/3\\) of the way from the tip of \\(\\mathsf{v}\\) to the tip of \\(\\mathsf{w}\\) as depicted below. Write \\(\\mathsf{u}\\) as a linear combination of \\(\\mathsf{v}\\) and \\(\\mathsf{w}\\) (hint: think about \\(\\mathsf{w} - \\mathsf{v}\\)) If \\(\\mathsf{v}\\) and \\(\\mathsf{w}\\) are solutions to \\(A x = \\mathsf{b}\\) then show that \\(\\mathsf{u}\\) is also a solution to \\(A \\mathsf{x} = \\mathsf{b}\\). "],["problem-set-3.html", "Vector 3 Problem Set 3 3.1 Three vectors in \\(\\mathbb{R}^4\\) 3.2 A Balanced Diet 3.3 A Problem about Span and Linear Dependence 3.4 Properties of Linear Transformations 3.5 Partial Information about a Linear Transformation 3.6 House Renovations", " Vector 3 Problem Set 3 Due: Tuesday November 10 by 11:55pm CST. Upload your solutions to Moodle in a PDF. Please feel free to use RStudio for all row reductions. You can download the Rmd source file for this problem set. The Problem Set covers sections 1.7, 1.8, 1.9. 3.1 Three vectors in \\(\\mathbb{R}^4\\) Consider the vectors \\(\\mathbf{u}\\), \\(\\mathbf{v}\\), and \\(\\mathbf{w}\\) given by \\[ \\qquad \\mathbf{u} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 2 \\\\ -1 \\end{bmatrix}, \\qquad \\mathbf{v} = \\begin{bmatrix} 2 \\\\ -1 \\\\ 3 \\\\ 1 \\end{bmatrix}, \\qquad \\mathbf{w} = \\begin{bmatrix} 1 \\\\ -2 \\\\ 2 \\\\ 1 \\end{bmatrix}. \\] Use RStudio to answer the following questions. Are \\(\\mathbf{u}\\), \\(\\mathbf{v}\\), and \\(\\mathbf{w}\\) linearly independent? Is the vector \\(\\mathbf{b} = \\begin{bmatrix} 4 \\\\ 4 \\\\ 2 \\\\ 4 \\end{bmatrix}\\) in the span of vectors \\(\\mathbf{u}\\), \\(\\mathbf{v}\\), and \\(\\mathbf{w}\\)? Is the vector \\(\\mathbf{b} = \\begin{bmatrix} 2 \\\\ 1 \\\\ 3 \\\\ 1 \\end{bmatrix}\\) in the span of vectors \\(\\mathbf{u}\\), \\(\\mathbf{v}\\), and \\(\\mathbf{w}\\)? 3.2 A Balanced Diet An athlete wants to consume a daily diet of 200 grams of carbohydrates, 60 grams of fats and 160 grams of proteins. Here are some of their favorite foods. Table 3.1: Food Carb/Fat/Protein (grams) food carbs fats proteins almonds 3 8 5 avocado 15 31 4 beans 20 1 8 bread 12 1 2 cheese 1 5 3 chicken 0 13 50 egg 1 5 6 milk 12 8 8 zucchini 6 0 2 Answer the following questions, using RStudio for your calculations. Each response must use two or more of the following terms: linear combination, span, linearly dependent, linearly independent. Explain why they cannot achieve their daily goal by eating only almonds, milk and zucchini. Explain why they cannot achieve their daily goal by eating only almonds, beans and cheese. Find a valid one-day diet consisting of almonds, chicken, and zucchini. 3.3 A Problem about Span and Linear Dependence Let \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3\\) and \\(\\mathsf{w}_1, \\mathsf{w}_2, \\mathsf{w}_3, \\mathsf{w}_4\\) all be vectors in \\(\\mathbb{R}^4\\). Suppose that \\(\\{ \\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3 \\}\\) is a linearly independent set, and that \\[ \\mathrm{span}(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3) = \\mathrm{span}(\\mathsf{w}_1, \\mathsf{w}_2, \\mathsf{w}_3, \\mathsf{w}_4). \\] Prove that \\(\\mathsf{w}_1, \\mathsf{w}_2, \\mathsf{w}_3, \\mathsf{w}_4\\) must be linearly dependent. You can do this by either: Explaning why there must be a nontrivial linear combination \\(c_1 \\mathsf{w}_1 + c_2 \\mathsf{w}_2 + c \\mathsf{w}_3 + c \\mathsf{w}_4 = \\mathbf{0}\\), or Showing that the \\(4 \\times 4\\) matrix \\(\\begin{bmatrix} \\mathsf{w}_1 &amp; \\mathsf{w}_2 &amp; \\mathsf{w}_3 &amp; \\mathsf{w}_4 \\end{bmatrix}\\) does not have a pivot in every column. 3.4 Properties of Linear Transformations Here are the row reductions to reduced row echelon form of 4 matrices. \\[ \\begin{array}{ll} A \\longrightarrow \\begin{bmatrix} 1 &amp; 0 &amp; 5 &amp; -3 &amp; 0\\\\ 0 &amp; 1 &amp; -2 &amp; 8 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix} \\qquad &amp; B \\longrightarrow \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix} \\\\ \\\\ C \\longrightarrow \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} &amp; D \\longrightarrow \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 1 \\end{bmatrix} \\end{array} \\] In each case, if \\(T\\) is the linear transformation given by the matrix product \\(T(x) = M x\\), where \\(M\\) is an \\(m \\times n\\) matrix (\\(m\\) rows and \\(n\\) columns) then \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is a transformation from domain \\(\\mathbb{R}^n\\) to codomain (aka target) \\(\\mathbb{R}^m\\). Determine the appropriate values for \\(n\\) and \\(m\\), and decide whether \\(T\\) is one-to-one and/or onto. Submit your answers in table form, as shown below. \\[ \\begin{array} {|c|c|c|c|c|} \\hline \\text{matrix } M &amp; n &amp; m &amp; \\text{one-to-one?} &amp; \\text{onto?} \\\\ \\hline A &amp;\\phantom{\\Big\\vert XX}&amp;\\phantom{\\Big\\vert XX}&amp;&amp; \\\\ \\hline B &amp;\\phantom{\\Big\\vert XX}&amp;&amp;&amp; \\\\ \\hline C &amp;\\phantom{\\Big\\vert XX}&amp;&amp;&amp; \\\\ \\hline D &amp;\\phantom{\\Big\\vert XX}&amp;&amp;&amp; \\\\ \\hline \\end{array} \\hskip5in \\] 3.5 Partial Information about a Linear Transformation \\(T: \\mathbb{R}^4 \\rightarrow \\mathbb{R}^3\\) is a linear transformation such that: \\[ T\\left(\\begin{bmatrix} 1 \\\\ 2 \\\\ 2 \\\\ 0 \\end{bmatrix} \\right) = \\begin{bmatrix} ~4~ \\\\ -1~ \\\\ 2 \\end{bmatrix} \\quad \\mbox{and} \\quad T\\left(\\begin{bmatrix} 2 \\\\ -1~ \\\\ 1 \\\\ 3 \\end{bmatrix} \\right) = \\begin{bmatrix} ~2~ \\\\ 6 \\\\ -1~ \\end{bmatrix} \\quad \\mbox{and} \\quad T\\left(\\begin{bmatrix} 3 \\\\ 0 \\\\ 2 \\\\ 2 \\end{bmatrix} \\right) = \\begin{bmatrix} 6 \\\\ 5 \\\\ 1 \\end{bmatrix} \\] Do we have enough information to determine whether \\(T\\) is one-to-one? If no, then explain why not. If yes, then do so, along with a justification. Do we have enough information to determine whether \\(T\\) is onto? If no, then explain why not. If yes, then do so, along with a justification. 3.6 House Renovations Find the matrix of a linear transformation \\(T: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) that performs the given transformation of my house. (Hint: use the base, the doorway and the peak of the roof as a guide.) Transformation #1 \\(\\qquad \\qquad\\) Transformation #2 \\(\\qquad \\qquad\\) For your convenience, here is the code to make the original plot of the house. par(pty=&quot;s&quot;) house = cbind(c(0,0), c(0,3/4), c(1/2,3/4), c(1/2,0), c(1,0), c(1,1), c(5/4,1), c(0,2), c(-5/4,1), c(-1,1), c(-1,0), c(0,0)); plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-6,6),ylim=c(-6,6),xlab=&quot;x&quot;,ylab=&quot;y&quot;, asp=1) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) "],["problem-set-4.html", "Vector 4 Problem Set 4 4.1 An Invertible Product of Rectangular Matrices 4.2 Guessing the Inverse Matrix from a Pattern 4.3 LU Decomposition of an Invertible Matrix 4.4 Homogeneous Coordinates", " Vector 4 Problem Set 4 Due: Tuesday November 17 by 11:55pm CST. Upload your solutions to Moodle in a PDF. Please feel free to use RStudio for all row reductions. You can download the Rmd source file for this problem set. Would you like use RMarkdown to type up some or all of your solution? Here is an Rmd template file to get you started. You can “knit to PDF” when you are done. Here are some helpful resources: RMarkdown Basics Math Typsetting using LaTeX syntax The Rmd source for the problem set which has examples of using \\begin{bmatrix} ... \\end{bmatrix} to format matrices. The Problem Set covers sections 2.1, 2.2 and 2.3, and homogeneous coordinates. 4.1 An Invertible Product of Rectangular Matrices Suppose that \\(m \\neq n\\) and that \\(B\\) is a \\(m \\times n\\) matrix and that \\(C\\) is an \\(n \\times m\\) matrix where \\(A =BC\\) is an invertible matrix. Is \\(m &gt; n\\) or is \\(m &lt; n\\)? Explain. Since \\(A\\) is invertible, the Invertible Matrix Theorem tells us the following facts: \\(A\\) has a pivot in every row \\(A\\) has a pivot in every column \\(T(\\mathsf{x}) = A\\mathsf{x}\\) is one-to-one \\(T(\\mathsf{x}) = A\\mathsf{x}\\) is onto The columns of \\(A\\) span \\(\\mathbb{R}^m\\) The columns of \\(A\\) are linearly independent \\(A \\mathsf{x} =\\mathsf{b}\\) has at least one solution for all \\(\\mathsf{b}\\) \\(A \\mathsf{x} =\\mathbf{0}\\) only has the trivial solution What do these conditions guarantee about the \\(m \\times n\\) matrix \\(B\\) and the \\(n \\times m\\) matrix \\(C\\)? Make the appropriate list for each of \\(B\\) and \\(C\\). 4.2 Guessing the Inverse Matrix from a Pattern Use RStudio to find the inverse matrix for each of the following matrices. \\[ \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 \\end{bmatrix} \\qquad \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 &amp; 1 \\\\ \\end{bmatrix} \\qquad \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0\\\\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0\\\\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1\\\\ \\end{bmatrix}. \\] by creating a matrix of the form \\([ \\, A \\, | \\, I \\, ]\\) and putting it into RREF to obtain \\([ \\, I \\, | \\, A^{-1} \\, ]\\). Use the previous part to guess the inverse matrix \\(A^{-1}\\) of the \\(n \\times n\\) matrix \\[ A = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; \\cdots &amp;0&amp;0 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 &amp; \\cdots &amp;0 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 \\\\ \\vdots &amp; &amp; &amp; \\ddots &amp; &amp; \\vdots \\\\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; \\cdots &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; \\cdots &amp; 1 &amp; 1 \\end{bmatrix}. \\] Use this same method to guess the inverse matrix \\(B^{-1}\\) for the \\(n \\times n\\) matrix \\[ B = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 \\\\ 2 &amp; 2 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 \\\\ 3 &amp; 3 &amp; 3 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 \\\\ \\vdots &amp; &amp; &amp; \\ddots &amp; &amp; \\vdots \\\\ n-1 &amp; n-1 &amp; n-1 &amp; n-1 &amp; \\cdots &amp; n-1 &amp; 0 \\\\ n &amp; n &amp; n &amp; n &amp; \\cdots &amp; n &amp; n \\end{bmatrix}. \\] Pro Tip: when R displays a number with a large negative exponent like 3.700743e-17 then this number is \\(3.700743 \\times 10^{-17}\\) which is numerically equivalent to 0. 4.3 LU Decomposition of an Invertible Matrix Let \\(A\\) be a \\(4 \\times 4\\) invertible matrix. Recall that if we run Gaussian Elimination on \\([ \\, A \\, | \\, I \\, ]\\) until the first part is the RREF of matrix \\(A\\), then we get the matrix \\([ \\, I \\, | \\, A^{-1} ]\\). In other words, this procedue is how we calculate \\(A^{-1}\\). But what if we stop this process at the REF of matrix \\(A\\)? Let’s call this resulting matrix \\([ \\, B \\, | \\, C \\, ]\\) where \\(B\\) is the RREF of \\(A\\) where all the pivots are 1, so \\(B\\) looks like \\[ B = \\begin{bmatrix} 1 &amp; b_{12} &amp; b_{13}&amp; b_{14} \\\\ 0 &amp; 1 &amp; b_{23} &amp; b_{24} \\\\ 0 &amp; 0 &amp; 1 &amp; b_{34} \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ \\end{bmatrix} \\] Remark: B is an example of an invertible upper triangular matrix: all the entries below the diagonal are zero. Assume that \\(A\\) is a \\(4 \\times 4\\) matrix such that Gaussian Elimination to REF only uses two types of Elementary Row Operations: (E1) Add a multiple of one row to another (E3) Multiply a row by a nonzero constant. Explain why \\(C\\) has the the form \\[ C = \\begin{bmatrix} c_{11} &amp; 0 &amp; 0 &amp; 0 \\\\ c_{21} &amp; c_{22} &amp; 0 &amp; 0 \\\\ c_{31} &amp; c_{32} &amp; c_{33} &amp; 0 \\\\ c_{41} &amp; c_{42} &amp; c_{43} &amp; c_{44} \\\\ \\end{bmatrix} \\] where the diagonal elements \\(c_{11}, c_{22}, c_{33}, c_{44}\\) are nonzero. Remark: C is an example of an invertible lower triangular matrix: all the entries above the diagonal are zero. Explain why \\(CA=B\\). Next, you will show that the inverse matrix \\(C^{-1}\\) is also lower triangluar. Start with the matrix \\([ \\, C \\, | \\, I \\, ]\\) and perform Gaussian Elimination to put \\(C\\) into RREF to get the matrix \\([ \\, I \\, | \\, C^{-1} ]\\). Considering the lower triangular structure of \\(C\\), explain why putting \\(C\\) into \\(REF\\) actually results in the identity matrix \\(I\\). Use the previous three parts to show that we can factor \\(A=LU\\) where \\(L\\) is an invertible lower triangular matrix and \\(U\\) is an upper triangular matrix. Remark 1: You proved this for a \\(4 \\times 4\\) matrix, but your argument also works for an \\(n \\times n\\) matrix. Remark 2: If Gaussian Elimination of \\(A\\) requires row operation (E2) “exchange two rows” then we can choose to do those row swaps first. This corresponds to multiplying \\(A\\) by an invertible permutation matrix \\(P\\). We then perform Gaussian Elimination on \\(PA\\): we can factor \\(PA=LU\\) where \\(L\\) is lower triangular and \\(U\\) is upper triangular. This is called the \\(LU\\)-decomposition of \\(A\\). Remark 3: It turns out that explicitly calculating \\(A^{-1}\\) can lead to some very problematic rounding errors. So most solvers use the \\(LU\\)-decomposition to solve \\(A \\mathsf{x} = \\mathsf{b}\\) instead of calculating \\(\\mathsf{x} = A^{-1}\\mathsf{b}\\). More specifically, we replace \\(A \\mathsf{x} = \\mathsf{b}\\) with \\(LU \\mathsf{x} = P^{-1} \\mathsf{b}\\) and then solve this in two steps. First, we solve \\(L \\mathsf{y} = P^{-1}\\mathsf{b}\\). Because \\(L\\) is lower triangular, this runs exactly like ``back substitution.’’ Next, we solve \\(U \\mathsf{x} = \\mathsf{y}\\). Because \\(L\\) is upper triangular, this also runs like ``back substitution.’’ Remark 4: So what can go wrong when computing \\(A^{-1}\\)? Sometimes an invertible matrix is ill-conditioned which means that the rounding errors get in the way of the numerical calculation of the inverse. (Basically: you end up dividing by a number really close to 0.) The \\(LU\\) methodology avoids this problem. You can learn more about this in MATH 365 Computational Linear Algebra. 4.4 Homogeneous Coordinates I made the following nature scene with a bird and a palm tree. bird = rbind(c(20, 21.3, 23.3, 21.6, 20.7, 21.7, 20.9, 20.3, 18.9, 18.1, 18.3, 19.6, 19.2, 19.1, 19.2, 19.4, 20), c(20, 21.7, 20.9, 21.1, 19.3, 18.7, 18, 18.9, 18, 15.4, 18.6, 19.6, 20, 20.2, 20.3, 20.3, 20), c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)) fronds = rbind(c(10, 8, 6, 4, 2, 2.5, 4, 6, 9, 10, 10, 9, 7, 5, 2.5, 4, 6, 8, 9, 10, 10, 11, 11, 10.5, 12, 12.5, 12.5, 10, 10, 13, 15, 15.5, 15.5, 14, 12, 10, 10, 12, 14, 14, 13.5, 13, 12, 10), c(16, 16, 16, 15, 11, 15, 17, 17.5, 17, 16, 16, 17.5, 19, 19.5, 18.5, 20, 20.5, 20, 19, 16, 16, 18, 20, 20.5, 20, 19, 18, 16, 16, 17, 16, 15, 13.5, 15, 16, 16, 16, 15, 13, 11, 9, 11, 13, 16), c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)) trunk = rbind(c(10, 9, 8, 7.5, 7.5, 8, 10, 8, 6.5, 6, 5, 5, 6, 7, 10), c(16, 14, 12, 10, 8, 5, 2.5, 2, 2, 3, 6, 9, 12, 14, 16), c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)) # intialize the plot plot(bird[1,],bird[2,],type=&quot;n&quot;,xlim=c(0,25),ylim=c(0,25),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=0:25, v=0:25, col=&quot;gray&quot;, lty=&quot;dotted&quot;) # plot the bird, trunk and fronds polygon(bird[1,], bird[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) polygon(fronds[1,], fronds[2,], col = &quot;orange&quot;, border = &quot;black&quot;) polygon(trunk[1,], trunk[2,], col = &quot;brown&quot;, border = &quot;black&quot;) However, I’ve decided that I want to make some changes. I want to rotate the bird by \\(-\\pi/2\\) around the point \\((20,20)\\) where its head meets its wing. I want to enlarge the fronds (leaves) of the plam tree to be bigger by \\(25\\%\\). I like the trunk just as it is (no changes!), so the fronds should still meet at the point \\((10,16)\\). I want to change the colors to make it happier. For example, here is what I want it to look like (though maybe there is a better color choice). However, I’m not sure how to do this because: I only know how to rotate vectors around the point \\((0,0)\\). So how can I rotate around the point \\((20,20)\\)? Likewise, I only know how to expand vectros from the point \\((0,0)\\). So how can I expand outward from the point \\((10,16)\\)? I heard someone say that translations via homogeneous coordinates will help to solve my problem. But I’m not sure what to do. Help! Please submit: The numerical \\(3 \\times 3\\) matrix birdmap that you used for the bird, and an explanation of how you made it. The numerical \\(3 \\times 3\\) matrix leafmap that you used for the leaves, and an explanation of how you made it. Your resulting plot where you use happier colors of your choice that are unique to you and express your personal style. (An artist statement is optional.) For your convenience, here is some template code for you. Once you fill in the proper transformation, this code will make the final picture. Currently, it updates the colors, but doesn’t change the bird or the fronds (because it is using the identity transformation). ##################### ######## update this code with the appropriate linear transformations birdmap = cbind(c(1,0,0),c(0,1,0),c(0,0,1)) leafmap = cbind(c(1,0,0),c(0,1,0),c(0,0,1)) ##################### ######## you do not need to change this the code below this line newbird = birdmap %*% bird newfronds = leafmap %*% fronds # initialize the plot plot(newbird[1,],newbird[2,],type=&quot;n&quot;,xlim=c(0,25),ylim=c(0,25),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=0:25, v=0:25, col=&quot;gray&quot;, lty=&quot;dotted&quot;) polygon(newbird[1,], newbird[2,], col = &quot;cyan&quot;, border = &quot;blue&quot;) polygon(newfronds[1,], newfronds[2,], col = &quot;green&quot;, border = &quot;black&quot;) polygon(trunk[1,], trunk[2,], col = &quot;brown&quot;, border = &quot;black&quot;) Finally, here is some R code that might be helpful. It overlays the original and the new version, so that you can directly compare them. Here is what the final comparison should look like: # initialize the plot plot(newbird[1,],newbird[2,],type=&quot;n&quot;,xlim=c(0,25),ylim=c(0,25),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=0:25, v=0:25, col=&quot;gray&quot;, lty=&quot;dotted&quot;) # plot the new bird and new fronds polygon(newbird[1,], newbird[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) polygon(newfronds[1,], newfronds[2,], col = &quot;orange&quot;, border = &quot;black&quot;) # plot the original bird, fronds and trunk polygon(bird[1,], bird[2,], col = &quot;cyan&quot;, border = &quot;blue&quot;) polygon(fronds[1,], fronds[2,], col = &quot;green&quot;, border = &quot;black&quot;) polygon(trunk[1,], trunk[2,], col = &quot;brown&quot;, border = &quot;black&quot;) Remark: This truly is how computer animation works! We use different effects for each object, and need to use homogeneous coordinates to get our desired results. "],["problem-set-5.html", "Vector 5 Problem Set 5 5.1 A Subspace from Two Linear Transformations 5.2 Getting Into a Subspace 5.3 Creating a Basis from Another Basis 5.4 A Vector that is in Both Col(A) and Nul}(A) 5.5 Changing Coordinate Systems", " Vector 5 Problem Set 5 Due: Friday November 20 by 11:55pm CST. Upload your solutions to Moodle in a PDF. Please feel free to use RStudio for all row reductions. You can download the Rmd source file for this problem set. The Problem Set covers sections 4.1, 4.2, 4.3 and 4.4. 5.1 A Subspace from Two Linear Transformations Suppose that \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) and \\(S: \\mathbb{R}^n \\to \\mathbb{R}^m\\) are linear transformations. Let \\(V \\subset \\mathbb{R}^n\\) be the set \\[ V = \\{ \\mathsf{v} \\in \\mathbb{R}^n\\mid T(\\mathsf{v}) = S(\\mathsf{v}) \\}. \\] Prove that the set \\(V\\) is a subspace by showing that: If \\(\\mathsf{v} \\in V\\) and \\(\\mathsf{w} \\in V\\) then \\(\\mathsf{v}+\\mathsf{w} \\in V\\) If \\(\\mathsf{v} \\in V\\) and \\(c \\in \\mathbb{R}\\) then \\(c \\mathsf{v} \\in V\\) 5.2 Getting Into a Subspace Let \\(S \\subset \\mathbb{R}^n\\) be a subspace and let \\(\\mathsf{v}, \\mathsf{w} \\in \\mathbb{R}^n\\). For each of the following statements, either give a specific example or explain why it cannot happen. If \\(\\mathsf{v}\\) is in \\(S\\) but \\(\\mathsf{w}\\) is not in \\(S\\), can \\(\\mathsf{v} + \\mathsf{w}\\) be in \\(S\\)? If \\(\\mathsf{v}\\) is not in \\(S\\) and \\(\\mathsf{w}\\) is not in \\(S\\), can \\(\\mathsf{v} + \\mathsf{w}\\) be in \\(S\\)? If \\(\\mathsf{v}\\) is not in \\(S\\) and \\(c\\) is a nonzero constant, can \\(c\\mathsf{v}\\) be in \\(S\\)? 5.3 Creating a Basis from Another Basis Suppose that \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3\\) is a basis of \\(\\mathbb{R}^3\\). Let \\[ \\mathsf{w}_1 = \\mathsf{v}_1, \\quad \\mathsf{w}_2 = \\mathsf{v}_1 + \\mathsf{v}_2, \\quad \\mathsf{w}_3= \\mathsf{v}_1 + \\mathsf{v}_2 + \\mathsf{v}_3. \\] Prove that \\(\\mathsf{w}_1, \\mathsf{w}_2, \\mathsf{w}_3\\) is also a basis for \\(\\mathbb{R}^3\\) as follows: Show that \\(\\mathsf{w}_1, \\mathsf{w}_2, \\mathsf{w}_3\\) are linearly independent as follows: We know that whenever \\(a_1 \\mathsf{v}_1 + a_2 \\mathsf{v}_2 + a_3 \\mathsf{v}_3 = \\mathbf{0}\\), this means that \\(a_1 = a_2 = a_3 = 0\\). Now consider a linear combination \\(b_1 \\mathsf{w}_1 + b_2 \\mathsf{w}_2 + b_3 \\mathsf{w}_3 = \\mathbf{0}\\). Use the previous fact to show that \\(b_1 = b_2 = b_3 = 0\\). Show that \\(\\mathsf{w}_1, \\mathsf{w}_2, \\mathsf{w}_3\\) span \\(\\mathbb{R}^3\\) as follows: We know that for any \\(\\mathsf{b} \\in \\mathbb{R}^3\\), there exist \\(a_1, a_2, a_3 \\in \\mathbb{R}\\) such that \\(\\mathsf{b} = a_1 \\mathsf{v}_1 + a_2 \\mathsf{v}_2 + a_3 \\mathsf{v}_3\\). Use the previous fact to show that there also exist \\(c_1, c_2, c_3 \\in \\mathbb{R}\\) such that \\(\\mathsf{c} = b_1 \\mathsf{w}_1 + b_2 \\mathsf{w}_2 + b_3 \\mathsf{w}_3\\). 5.4 A Vector that is in Both Col(A) and Nul}(A) Give a \\(3 \\times 3\\) matrix \\(A\\) for which the vector \\(\\mathsf{v} = \\begin{bmatrix}3 \\\\ -2 \\\\ 5 \\end{bmatrix}\\) is in both \\(\\mathrm{Col}(A)\\) and \\(\\mathrm{Nul}(A)\\). Be sure to demonstrate that \\(\\mathsf{v} \\in \\mathrm{Col}(A)\\) and \\(\\mathsf{v} \\in \\mathrm{Nul}(A)\\). 5.5 Changing Coordinate Systems Here are three bases for \\(\\mathbb{R}^4\\): \\[\\begin{align} {\\cal S} &amp;= \\left\\{ \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad \\begin{bmatrix} 0 \\\\0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix} \\right\\}, \\\\ \\\\ {\\cal B} &amp;= \\left\\{ \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\quad \\begin{bmatrix} 1 \\\\ 1 \\\\ -1~ \\\\ -1~ \\end{bmatrix}, \\quad \\begin{bmatrix} 1 \\\\ -1~ \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ -1~ \\end{bmatrix} \\right\\}, \\\\ \\\\ {\\cal C} &amp;= \\left\\{ \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad \\begin{bmatrix} 1 \\\\ 1 \\\\ -1~ \\\\ 0 \\end{bmatrix}, \\quad \\begin{bmatrix} 1 \\\\ -1~ \\\\ 0 \\\\ 1 \\end{bmatrix}, \\quad \\begin{bmatrix} 1 \\\\ -1~ \\\\ 0 \\\\ -1~ \\end{bmatrix} \\right\\}. \\end{align}\\] Let \\(P_{\\cal B}\\) be the change-of-coordinates matrix from basis \\({\\cal B}\\) to the standard basis \\({\\cal S}\\), and let \\(P_{\\cal C}\\) be the change-of-coordinates matrix from basis \\({\\cal C}\\) to the standard basis \\({\\cal S}\\). Find \\(P_{\\cal B}^{-1}\\), which is the change-of-coordinates matrix from the standard basis \\({\\cal S}\\) to basis \\({\\cal B}\\). Find \\(P_{\\cal C}^{-1}\\), which is the change-of-coordinates matrix from the standard basis \\({\\cal S}\\) to basis \\({\\cal C}\\). Find the change-of-coordinates matrix from basis \\({\\cal B}\\) to basis \\({\\cal C}\\). Find the change-of-coordinates matrix from basis \\({\\cal C}\\) to basis \\({\\cal B}\\). Consider the vector \\(\\mathsf{v}\\) where \\[ [\\mathsf{v}]_{\\cal B} = \\begin{bmatrix} ~3.5 \\\\ -1.0 \\\\ -0.5 \\\\ ~1.5 \\end{bmatrix}_{\\cal B}.\\] Find \\([ \\mathsf{v} ]_{\\cal C}.\\) Then calculate both \\(P_{\\cal B} [\\mathsf{v}]_{\\cal B}\\) and \\(P_{\\cal C} [\\mathsf{v}]_{\\cal C}\\) to confirm that they both produce the same vector \\(\\mathsf{v} =[\\mathsf{v}]_{\\cal S}\\). Remark: Both \\(\\cal{B}\\) and \\(\\cal{C}\\) are examples of wavelet bases. Wavelets and similar bases are useful for image processing and image compression. "],["problem-set-6.html", "Vector 6 Problem Set 6 6.1 The Square Root of a Matrix? 6.2 A Matrix Mystery 6.3 Upper Triangular Matrix with a Constant Diagonal 6.4 Block Diagonalization of a \\(4 \\times 4\\) Matrix 6.5 Network Analysis of Risk Territories", " Vector 6 Problem Set 6 Due: Friday December 04 by 11:55pm CST. Upload your solutions to Moodle in a PDF. Please feel free to use RStudio for all calculations, including row reduction, matrix multiplication, eigenvector calculation and inverse matrices. You can download the Rmd source file for this problem set. The Problem Set covers sections 5.1, 5.2, 5.3, 5.5 and Network Analysis. 6.1 The Square Root of a Matrix? The matrix \\(A =\\begin{bmatrix} 7 &amp; 2 \\\\ -4 &amp; 1 \\end{bmatrix}\\) has characteristic polynomial \\(\\lambda^2 - 8 \\lambda + 15 = (\\lambda -3)(\\lambda - 5).\\) Describe the eigenspaces of \\(A\\). Diagonalize \\(A\\). Find a matrix that makes sense to call \\(\\sqrt{A}\\). Then show that when you square this matrix, you really do get matrix \\(A\\). 6.2 A Matrix Mystery An unknown \\(3 \\times 3\\) matrix \\(M\\) has eigenvectors and corresponding eigenvalues: \\[ \\mathsf{v}_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 1 \\end{bmatrix}, \\ \\lambda_1 = 1; \\qquad \\mathsf{v}_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix},\\ \\lambda_2 = \\frac{9}{10}; \\qquad \\mathsf{v}_3 = \\begin{bmatrix} -1 \\\\ 1 \\\\ 0 \\end{bmatrix},\\ \\lambda_3 = 0. \\] Find (exactly, if possible, or approximately otherwise) the vector \\(M^{10} \\mathsf{v}\\) where \\(\\mathsf{v} = \\begin{bmatrix}7\\\\3\\\\4\\end{bmatrix}\\). Describe all vectors \\(\\mathsf{v}\\), if there are any, such that \\(M^{n} \\mathsf{v} \\to {\\bf 0}\\) as \\(n \\to \\infty\\). Is it possible to reconstruct \\(M\\) from the evidence given? If so, then do it! If not, explain what further information is needed. 6.3 Upper Triangular Matrix with a Constant Diagonal Let \\(A\\) be a \\(3 \\times 3\\) upper triangular matrix of the form \\[ A = \\begin{bmatrix} d &amp; a &amp; b \\\\ 0 &amp; d &amp; c \\\\ 0 &amp; 0 &amp; d \\end{bmatrix} \\] where \\(d \\neq 0\\) and at least one of \\(a,b,c\\) is nonzero. Explain why \\(A\\) is not diagonalizable. 6.4 Block Diagonalization of a \\(4 \\times 4\\) Matrix The \\(4 \\times 4\\) matrix \\[ A = \\begin{bmatrix} 2 &amp; -1 &amp; 1 &amp; -1 \\\\ 1 &amp; -3 &amp; 3 &amp; 2 \\\\ 2 &amp; -9 &amp; 7 &amp; 0 \\\\ 2 &amp; -4 &amp; 2 &amp; 0 \\end{bmatrix} \\] has complex eigenvalues \\(a \\pm bi\\) and \\(c \\pm di\\). Use RStudio to find the eigenvalues \\(a \\pm bi\\) and \\(c \\pm di\\), as well as “human friendly” eigenvectors \\(\\mathsf{v} = \\mathsf{v}_1 + i \\mathsf{v}_2\\) for eigenvalue \\(a + bi\\), and \\(\\mathsf{w} = \\mathsf{w}_1 + i \\mathsf{w}_2\\) for eigenvalue \\(c + di\\). Hint: the command zapsmall() might be helpful. This matrix A can be factored as \\(A = P B P^{-1}\\) where \\(B\\) is a “block diagonal” matrix of the form \\[ B = \\begin{bmatrix} a &amp; -b &amp; 0 &amp; 0 \\\\ b &amp; a &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; c &amp; -d \\\\ 0 &amp; 0 &amp; d &amp; c \\end{bmatrix} \\] and \\[ P = \\begin{bmatrix} \\mathrm{Im} (\\mathsf{v}) &amp; \\mathrm{Re} (\\mathsf{v}) &amp; \\mathrm{Im} (\\mathsf{w}) &amp; \\mathrm{Re}( \\mathsf{w}) \\end{bmatrix} =\\begin{bmatrix} \\mathsf{v}_2 &amp; \\mathsf{v}_1 &amp; \\mathsf{w}_2 &amp; \\mathsf{w}_1 \\end{bmatrix} \\] where the constants \\(a,b,c,d\\) and the vectors \\(\\mathsf{v}, \\mathsf{w}\\) are as described in part (a). Use RStudio to find the \\(4 \\times 4\\) matrices \\(B\\), \\(P\\) and its inverse \\(P^{-1}\\). Hints: You can use the command solve(M) to find the inverse of matrix \\(M\\). You can confirm that your answer is correct by computing \\(P B P^{-1}\\) and checking that this equals the original matrix \\(A\\). 6.5 Network Analysis of Risk Territories Risk is a classic board game of conflict and diplomacy played on a map of the world. Players try to capture territories by moving their armies through adjancent territories. Here is what the game map looks like, with the countries colored by continent. Risk Game Board (image: wikipedia) Here is a network representation of the gameboard, created from an adjacency matrix. library(igraph) risk &lt;- read.csv(&quot;https://raw.github.com/mathbeveridge/math236_f20/main/data/riskmatrix.csv&quot;) A = data.matrix(risk) countries = names(risk) g=graph_from_adjacency_matrix(A,mode=&#39;undirected&#39;) coords = layout_nicely(g) plot(g, layout=coords, vertex.size = 10, vertex.label.cex=0.75, vertex.color=&#39;khaki&#39;, vertex.frame.color=&quot;black&quot;) Calculate Gould’s Index for this network. Use this Gould’s Index ranking to identify the most central territory in each of the six continents in this worldwide network. Here is a list of territories by continent (Africa, Asia, Australia, Europe, North America, South America) to help you to classify the territories. Turn in a table with six rows. These six rows should be ordered by Gould’s index. Each row should contain: name of the continent name of the territory with the largest Gould Index, the degree of the territory, the Gould’s Index value for the territory. Important: You do not need to show your work for this problem! Just turn the table (handwritten is fine) of your results. "],["problem-set-7.html", "Vector 7 Problem Set 7 7.1 Glucose-Insulin System Model 7.2 Population Interaction Models 7.3 Blue Whale Population Dynamics 7.4 The Power Method for Eigenvalue Calculation", " Vector 7 Problem Set 7 Due: Tuesday December 08 by 11:55pm CST. Upload your solutions to Moodle in a PDF. Please feel free to use RStudio for all calculations, including row reduction, matrix multiplication, eigenvector calculation and inverse matrices. You can download the Rmd source file for this problem set. You can download a Rmd template solution file. This problem set covers Sections 5.5 Complex Eigenvalues and 5.6 Dynamical Systems, as well as Modeling of Ecological Systems. Three out of four questions require you to use RStudio. I’ve provided a PS7 solution template if you want to use it. 7.1 Glucose-Insulin System Model Note: Use the functions get_trajectory(A, start, N) and plot_trajectory(X, title, types) from Modeling of Ecological Systems to create trajectories and plots for this question. The excess glucose level \\(G_t\\) and excess insulin level \\(H_t\\) in a digestive system are determined by the update rule \\[ \\begin{bmatrix} G_{t+1} \\\\ H_{t+1} \\end{bmatrix} = \\begin{bmatrix} 0.998 &amp; -0.169 \\\\ 0.068 &amp; 0.930 \\end{bmatrix} \\begin{bmatrix} G_{t} \\\\ H_{t} \\end{bmatrix} = \\begin{bmatrix} a G_t + b H_t \\\\ c G_t + d H_t \\\\ \\end{bmatrix} \\] a. Suppose that we have an intake of glucose \\(\\begin{bmatrix} G_{0} \\\\ H_{0} \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}.\\) Make a plot of this system and that you can use to estimate how long it will take for this system to return (approximately) to homeostatis, where the values stay steady at \\(\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\). Calculate the eigenvalues \\(\\lambda = a \\pm b i\\) for this matrix. Use them to find the decay rate \\(|\\lambda| = \\sqrt{a^2 + b^2}\\) and the rate of rotation \\(\\theta = \\tan^{-1} (a/b)\\), measured in degrees. You can find this value in RStudio using the command atan(a/b) * 360 / (2 * pi). 7.2 Population Interaction Models The dynamics of two interacting populations \\(X\\) and \\(Y\\) can be modeled using a discrete dynamical system of the form \\[ \\begin{bmatrix} X_{t+1} \\\\ Y_{t+1} \\end{bmatrix} = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix} \\begin{bmatrix} X_{t} \\\\ Y_{t} \\end{bmatrix} = \\begin{bmatrix} a X_t + b Y_t \\\\ c X_t + d Y_t \\\\ \\end{bmatrix} \\] where \\(a,b,c,d\\) are constants that depend on the type of interaction. The constants have the following interpretations: constant qualitative interpretation quantitative interpretation \\(a\\) Does population \\(X\\) thrive in the absence of population \\(Y\\)? If \\(a &gt; 1\\), then \\(X\\) thrives without \\(Y\\). If \\(0 &lt; a &lt; 1\\) then \\(X\\) dies out without \\(Y\\). b What is the effect of population \\(Y\\) on population \\(X\\)? If \\(b &gt; 0\\) then \\(Y\\) helps \\(X\\) to thrive. If \\(b &lt; 0\\) then \\(Y\\) hinders the population growth of \\(X\\). c What is the effect of population \\(X\\) on population \\(Y\\)? If \\(c &gt; 0\\) then \\(X\\) helps \\(Y\\) to thrive. If \\(c &lt; 0\\) then \\(X\\) hinders the population growth of \\(Y\\). d Does population \\(Y\\) thrive in the absence of population \\(X\\)? If \\(d &gt; 1\\), then \\(Y\\) thrives without \\(X\\). If \\(0 &lt; d &lt; 1\\) then \\(Y\\) dies out without \\(X\\). Let’s consider 3 different types of population interaction. Predation: one population feeds off the other. One example: foxes and rabbits. Competition: the populations compete for the same resources. One example: squirrels and chipmunks. Mutual Symbiosis: the populations play complementary roles. Both thrive or decline together. One example: bees and flowers. You will characterize the constants corresponding to these interactions and then analyze some examples. (Note: these population models are oversimplified, of course. We are interested in seeing how eigenvalues and eigenvectors help to make sense of these systems.) For each of the proposed models (predation, competition, mutual symbiosis), characterize the values for the constants \\(a,b,c,d\\) by using the interpretations listed above. For example, you must decide whether \\(a&gt;1\\) or \\(0 &lt; a &lt; 1\\) for each model. Your answer should be written in table form, as shown below. model a b c d predation competition symbiosis Here are three different population models, along with its eigenvalue and eigenvectors. For each one, you must decide whether it is Predation, Competition or Symbiosis. Your answer will be three simple statements like “\\(M\\) is predation.” \\(A = \\begin{bmatrix}1.1 &amp; -0.2 \\\\ -0.3 &amp; 1.2 \\end{bmatrix}, \\qquad \\lambda_1 = 1.4 \\mbox{ with } \\mathsf{v}_1 = \\begin{bmatrix} 1 \\\\ -1.5 \\end{bmatrix}, \\qquad \\lambda_2 = 0.9 \\mbox{ with } \\mathsf{v}_2 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\) \\(B = \\begin{bmatrix}0.5 &amp; 0.4 \\\\ -0.2 &amp; 1.1 \\end{bmatrix}, \\qquad \\lambda_1 = 0.9 \\mbox{ with } \\mathsf{v}_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\qquad \\lambda_2 = 0.7 \\mbox{ with } \\mathsf{v}_2 = \\begin{bmatrix} 1 \\\\ 0.5 \\end{bmatrix}\\) \\(C = \\begin{bmatrix}0.8 &amp; 0.3 \\\\ 0.4 &amp; 0.7 \\end{bmatrix}, \\qquad \\lambda_1 = 1.1 \\mbox{ with } \\mathsf{v}_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\qquad \\lambda_2 = 0.4 \\mbox{ with } \\mathsf{v}_2 = \\begin{bmatrix} 1 \\\\ -0.75 \\end{bmatrix}\\) Here are plots of the vector fields for the three models in part (b). Match the model \\(A, B, C\\) to its corresponding vector field. Your answer will be three simple statements like “\\(M\\) is Vector Field 2.” Vector Field 1 Vector Field 2 Vector Field 3 For each matrix model \\(A,B,C\\), assume that the initial population is \\([X_0, Y_0]^{\\top} = [1, 2.5]^{\\top}\\) where the units are in millions of organisms. Use the corresponding vector field to visualize the trajetory starting at \\([1, 2.5]^{\\top}\\). Describe the trajectory in words. (For example: “At first both populations decrease to \\([0.5, 1]^{\\top}\\). But then they both start increasing and converge to the direction of \\([2,3]^{\\top}\\). They continue to grow forever.” ) Indicate the long-term populations for both organisms (growth, stable, extinction). Connect the behavior you see back to the eigenvalues and eigenvectors of the model 7.3 Blue Whale Population Dynamics Note: Use the functions get_trajectory(A, start, N) and plot_trajectory(X, title, types) from Modeling of Ecological Systems to create trajectories and plots for this question. In the 1930’s (before its virtual extinction and a great change in its survival rates), a researcher studied the blue whale population. Due to the long gestation period, mating habits and migration of the blue whale, a female can produce a calf only once in a two-year period. Thus the age classes for the whale were assumed to be: 0-1 years 2-3 years 4-5 years 6-7 years 8-9 years 10-11 years 12 or more years The Leslie Matrix \\(B\\) for this model is given by (L=rbind(c(0,0,.19,.44,.5,.5,.45), c(.77,0,0,0,0,0,0), c(0,.77,0,0,0,0,0), c(0,0,.77,0,0,0,0), c(0,0,0,.77,0,0,0), c(0,0,0,0,.77,0,0), c(0,0,0,0,0,.77,.78))) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] 0.00 0.00 0.19 0.44 0.50 0.50 0.45 ## [2,] 0.77 0.00 0.00 0.00 0.00 0.00 0.00 ## [3,] 0.00 0.77 0.00 0.00 0.00 0.00 0.00 ## [4,] 0.00 0.00 0.77 0.00 0.00 0.00 0.00 ## [5,] 0.00 0.00 0.00 0.77 0.00 0.00 0.00 ## [6,] 0.00 0.00 0.00 0.00 0.77 0.00 0.00 ## [7,] 0.00 0.00 0.00 0.00 0.00 0.77 0.78 We can see that the vector of fecundities is \\([0,0,0.19,0.44,0.5,0.5,0.45]\\), and that the survival rate for all whales 11 years old or younger is 77%, and for all whales 12 or more years, it is 78%. Starting from a whale population of 1000 in each of the seven age classes, plot the trajectories over a 40 year period. Does your plot of this model predict population growth, population equilibrium or population extinction? Calculate the eigenvectors and eigenvalues of the matrix \\(L\\). Relate the behavior you see to the eigenvalues and eigenvectors. What is the overall growth rate? What are the long-term population ratios? When population is increasing, one might be interested in harvesting a portion of the population for some purpose. We want this harvest rate to be sustainable, meaning that the total population remains at a constant level in the long run. For simplicity, we will not distinguish between classes during the harvest (but this could be added with a minor modification). If a fraction \\(h\\) of the population is harvested each year, where \\(0 &lt; h &lt; 1\\), then the population model becomes \\[{\\bf x}_{t+1} = (1-h) L {\\bf x}_t.\\] Use trial and error to find the sustainable harvest rate \\(h &lt; 1\\) up to 4 decimal places that leads to a stable population of whales. Then plot a trajectory of this system with sustainable harvesting. Choose a long enough time peroid so that your plot shows that the blue whale populations reach a very stable configuration. In the long run, what are the percentages of whales in each age class? How does this compare to the population percentages you found in part (b)? Why does this make sense? 7.4 The Power Method for Eigenvalue Calculation During class, we found eigenvalues by solving the characteristic equation \\(\\det(A−\\lambda I) =0\\). We then found the eigenvectors by finding a basis for \\(\\mathrm{Nul}(A - \\lambda I)\\). This is fine for small matrices, but impractical for large ones. So let’s see how computers find (or really: approximate) eigensystems. We will assume that all of the eigenvectors of \\(A\\) are real and distinct. If you want the full story, then you should take MATH 365 Computational Linear Algebra. This problem explains how the Power Method and Inverse Power Methods work. At certain points, Your Task is identified, and that is what you must turn in for this problem. Parts (a) and (c) will have you confirm that the code below works for the following \\(4 \\times 4\\) matrix \\(A\\). We calculate its eigensystems up front so that you can compare your approximations to the values found by R. A = cbind(c(4,2,4,8), c(2,2,4,4), c(4,4,9,1),c(8,4,1,5)) A ## [,1] [,2] [,3] [,4] ## [1,] 4 2 4 8 ## [2,] 2 2 4 4 ## [3,] 4 4 9 1 ## [4,] 8 4 1 5 eigen(A) ## eigen() decomposition ## $values ## [1] 16.9078855 7.2660095 0.4248061 -4.5987012 ## ## $vectors ## [,1] [,2] [,3] [,4] ## [1,] -0.5515710 0.29143140 0.45675770 0.6341999 ## [2,] -0.3595835 -0.08290935 -0.86242118 0.3464902 ## [3,] -0.5286316 -0.78914121 0.19952131 -0.2408236 ## [4,] -0.5357403 0.53427515 -0.08828003 -0.6478727 Remark. This problem is not supposed to be hard. If you crave more guidance or background while working on this problem, then you can compare to ULA Section 5.2 and Lay Section 5.8.   The Power Method. We find the dominant eigenvalue and eigenvector by repeatedly multiplying some initial vector by \\(A\\) until we converge to the dominant eigenspace. This should sound familiar: it’s the same phenomenon that makes our dynamical systems converge! The one extra detail is that we rescale at each step: we adjust our eigenvector estimate so that its largest entry is 1. Here is the code. It is very simple. estimate_dominant_eigenvector &lt;- function(A, numiter) { n = dim(A)[1] x = rep(1,n) for (i in 1:numiter) { y = A %*% x maxval = max(y) minval = min(y) if (abs(maxval) &gt; abs(minval)) { m = maxval } else { m = minval } x = y/m } est = &#39;estimate&#39; attr(est,&#39;value&#39;) = m attr(est, &#39;vector&#39;) = x return(est) } Your Task: Run this code on the matrix \\(A\\) and start with numiter=5. Then increase this value and watch the eigenvalue estimate converge to the dominant eigenvalue. What is the smallest number of iterations needed so that the estimated eigenvalue matches the eigenvalue returned by eigen(A) (up to 5 decimal places)? Write some code to compare the estimated eigenvector (whose largest entry is 1) with the eigenvector returned by eigen(A). They should match when you rescale.   The Inverse Power Method Explained. We can use the power method to find the other eigenvalues, too. Let’s suppose that we are trying to find eigenvalue \\(\\lambda\\) with eigenvector \\(\\mathsf{v}\\). We just need to start at a guess \\(c\\) that is closer to \\(\\lambda\\) that to any of the other eigenvalue. In other words, if \\(\\mu\\) is any other eigenvale of \\(A\\), we have \\(| \\lambda - c| &lt; | \\mu - c|.\\) Using our guess \\(c\\), we define \\[ B = (A - c I)^{-1}. \\] And then we apply the power method to \\(B\\). You will now show that (1) \\(A-cI\\) really is invertible, and that (2) \\(\\mathsf{v}\\) is the dominant eigenvector of \\(B\\). Your Task: answer the following four short questions. Each answer only requires a sentence or two. Suppose that \\(\\mathsf{v}\\) is an eigenvector of \\(A\\) for eigenvalue \\(\\lambda\\). That is, \\(A \\mathsf{v} = \\lambda \\mathsf{v}\\). Confirm that \\(\\mathsf{v}\\) is an eigenvector for \\(A - cI\\) for eigenvalue \\(\\lambda - c\\). Assume that our guess \\(c\\) is not an eigenvalue of \\(A\\). Prove that \\(A-cI\\) is invertible by showing that \\(\\mathrm{Nul}(A-Ic) = \\mathbf{0}\\). Now that we know that \\(B = (A - c I)^{-1}\\) exists, explain why \\(\\mathsf{v}\\) is an eigenvector of \\(B\\) for eigenvalue \\(\\frac{1}{\\lambda-c}.\\) A similar argument shows that all eigenvalues of \\(B\\) are of the form \\(\\frac{1}{\\mu-c}\\) where \\(\\mu\\) is an eigenvalue of \\(A\\). So now use the fact that “\\(c\\) that is closer to \\(\\lambda\\) that to any of the other eigenvalue” to explain why \\(\\frac{1}{\\lambda-c}\\) is the dominant eigenvalue of \\(B\\).   The Inverse Power Method in Practice. In part (b), you showed why the inverse power method works! Now, here is the code, which is very simple. (Remark: in order to avoid rounding errors, in a true implementation, we would write code that avoids explicitly taking the inverse.) estimate_nearest_eigenvector &lt;- function(A, c, numiter) { n = dim(A)[1] B = A - c * diag(n) Binv = solve(B) est = estimate_dominant_eigenvector(Binv, numiter) val = attr(est, &#39;value&#39;) newval = 1/val+c attr(est,&#39;value&#39;) = newval return (est) } Your Task: Use this code to confirm that we can really find the other three eigenvalues for matrix \\(A\\) above. Run this code using a guess for each of the 3 remaining eigenvalues. Confirm that this code does find the correct eigenvalue, provided that you run for enough iterations. Note what your initial guess is, and how many iterations are required to converge to the nearest eigenvalue (up to 5 decimal places). Remark: These two methods succeed in finding the complete eigensystem when all of the eigenvalues are real and distinct. This is one of the reasons why the eigen() command in RStudio actually uses another method called \\(QR\\)-decomposition to approximate eigensystems. Remark: MATH 365 Computational Linear Algebra goes into the full story about how to approximate eigenvalues and eigenvectors quickly and precisely. "],["problem-set-8.html", "Vector 8 Problem Set 8 8.1 Two Orthogonality Properties 8.2 Bases for a Subspace and its Orthogonal Complement 8.3 Eigensystem of a Symmetric Matrix 8.4 Modeling Fertility in 1888 Switzerland 8.5 Cosine Similarity for US Senators", " Vector 8 Problem Set 8 Due: Tuesday December 15 by 11:55pm CST. Upload your solutions to Moodle in a PDF. Please feel free to use RStudio for all calculations, including row reduction, matrix multiplication, eigenvector calculation, inverse matrices, and the Gram-Schmidt process. You can download the Rmd source file for this problem set. Four out of five questions require you to use RStudio. I’ve provided a PS8 solution template if you want to use it. This problem set covers Sections 6.1 - 6.5 Orthogonality and the explorations on Voting Patterns in the US Senate and Least Squares Approximation. 8.1 Two Orthogonality Properties Give rigorous proofs for each of the following problems. Here “rigorous” means that the heart of your explanation must use equations and calculations to explicitly justify any intuitive reasoning that you provide. Let \\(W\\) be a subspace of \\(\\mathbb{R}^5\\) with orthonormal basis \\(\\mathsf{w}_1, \\mathsf{w}_2, \\mathsf{w}_3\\). Let \\(W^{\\perp}\\) be its orthogonal complement with orthonormal basis \\(\\mathsf{v}_1, \\mathsf{v}_2\\). Prove that if \\(\\mathsf{u}\\) is in both \\(W\\) and \\(W^{\\perp}\\) then \\(\\mathsf{u}\\) must be the zero vector. Let \\(U\\) and \\(V\\) both be \\(n \\times n\\) orthogonal matrices. Prove that the matrix \\(UV\\) is also an orthogonal matrix. 8.2 Bases for a Subspace and its Orthogonal Complement Let \\(W \\subset \\mathbb{R}^6\\) be the set of solutions to \\[\\begin{align} x_1 + x_2 + x_3 + x_4 + x_5 + x_6 &amp;= 0 \\\\ x_1 + x_2 - x_3 + x_4 + x_5 - x_6 &amp;= 0 \\end{align}\\] Find a basis for \\(W\\) by using row reduction on the appropriate matrix. Use your answer from (a) to find a “human readable” orthogonal basis for \\(W\\) where the entries have integer values. Find a basis for \\(W^{\\perp}\\) without using the Gram-Schmidt process. Use your answer from (c) to find a “human readable” orthogonal basis for \\(W^{\\perp}\\) where the entries have integer values. Turn in: Your code and its output. You must make it clear what your final answers are for each problem. 8.3 Eigensystem of a Symmetric Matrix Recall that a square \\(n \\times n\\) matrix is symmetric when \\(A^{\\top} = A\\). We learned that the eigenvectors of a symmetric matric form an orthogonal basis of \\(\\mathbb{R}^n\\). In this problem, you will confirm that this holds for the following symmetric matrix \\[ A = \\begin{bmatrix} 0 &amp; 8 &amp; 10 &amp; -4 \\\\ 8 &amp; 4 &amp; 28 &amp; 6 \\\\ 10 &amp; 28 &amp; 3 &amp; -4 \\\\ -4 &amp; 6 &amp; -4 &amp; -7 \\end{bmatrix} . \\] Find the eigenvalues and eigenvectors of \\(A\\). Confirm that the eigenvectors returned by R are an orthonormal set. (Pro tip: you can do this in one calculation!) Express the vector \\(\\mathsf{v} = \\begin{bmatrix} 2 &amp; -4 &amp; -9 &amp; -2 \\end{bmatrix}^{\\top}\\) as a linear combination of the eigenvectors. Use the fact that the eigenvectors are orthonormal. (Don’t augment and row reduce.) Let \\(P\\) be the matrix of these normalized, orthogonal eigenvectors. Diagonalize \\(A\\) using \\(P\\). Just write out \\(A = P D P^{-1}\\). Congratulations: you have orthogonally diagonalized the symmetric matrix \\(A\\)! Turn in: Your R code and the output for each part. For parts (c) and (d), you need to make it clear what your final answers are. 8.4 Modeling Fertility in 1888 Switzerland RStudio comes with some sample datasets, including swiss, an 1888 dataset of Swiss socio-economic data. In 1888, Switzerland was entering a demographic transition: birth rates were beginning to decrease as the country entered the industrial age. The rows correspond to 47 French-speaking provinces. These provinces were at different stages of industrialization. There are 6 socio-economic columns, all scaled to the range \\([0,100]\\). Fertility fert: standardized fertility rate Agriculture agric: percent of men in agricultural occupations Examination exam: percent of draftees receiving highest mark on army examination Education educ: percent education beyond primary school for draftees Catholic cath: percent Catholic (instead of Protestant) Infant.Mortality mort: percent of live births who live less than one year We want to find the best-fit linear model fert = \\(c_1\\) + \\(c_2\\) agric + \\(c_3\\) exam + \\(c_4\\) educ + \\(c_5\\) cath + \\(c_6\\) mort. Obviously, this data set shows its age. We are attempting to model fertility rates of women by using indirect measures about male occupation and education levels. This is problematic and would have to be addressed in a thorough analysis. We limit our use of this classic data to illustrate the method of least squares. Model this problem as a least-squares approximation of an inconsistent system \\(A \\mathsf{x} = \\mathsf{y}\\). Find the least squares solution \\(\\hat{\\mathsf{x}}\\) and confirm that the residual vector \\(\\mathsf{z}\\) is orthogonal to the columns of \\(A\\). To get you started, here is some convenient code to get the column vectors of the swiss dataset. fert = swiss[,1] agric = swiss[,2] exam = swiss[,3] educ = swiss[,4] cath = swiss[,5] mort = swiss[,6] Turn in: Your code and the output. The residual vector \\(\\mathsf{z}\\) measures the quality of fit of our model. But how do we turn this into a meaningful quantity? One method is to look at the coefficient of determination, which is more commonly refered to as the “\\(R^2\\) value.” Let \\(\\mathsf{y} = [ y_1, y_2, \\ldots, y_n ]^{\\top}\\) be our target vector with least squares solution \\(\\hat{\\mathsf{y}} = A \\hat{\\mathsf{x}}\\) and residual vector \\(\\mathsf{z} = \\mathsf{y} - \\hat{\\mathsf{y}}\\). Let \\[ a = \\frac{1}{n} ( y_1 + y_2 + \\cdots + y_n) \\] be the average of the entries of target vector \\(\\mathsf{y}\\) and let \\(\\mathsf{y}^* = [a, a, \\ldots, a]\\). (We call this vector “y star”, so ystar would be a fine name in R.) The \\(R^2\\) value is \\[ R^2 = 1 - \\frac{\\| \\mathsf{y} - \\hat{\\mathsf{y}} \\| }{\\| \\mathsf{y} - \\mathsf{y}^* \\|} = 1 - \\frac{\\| \\mathsf{z} \\|}{\\| \\mathsf{y} - \\mathsf{y}^* \\|}. \\] This is a number in \\([0,1]\\). If the \\(R^2\\) value is near 1, then our model does a good job at “explaining” the behavior of \\(\\mathsf{y}\\) via a linear combination of the columns of \\(A\\). Find the \\(R^2\\) value for our least squares solution to the Swiss data. Here are some helpful functions: sum(vec) returns the sum of the entries of the vector vec length(vec) returns the number of entries in the vector vec rep(a, n) creates a constant vector of length \\(n\\) where every entry is \\(a\\). Norm(vec) from the pracma package returns the magnitude (Euclidean length) of the vector vec. Turn in: Your code and the \\(R^2\\) value that you calculated. Remark: There is certainly more to say about ethical use of data, as well as \\(R^2\\) values and fitting models to data. To learn more, you should take STAT 155 Introduction to Statistical Modeling. 8.5 Cosine Similarity for US Senators In high dimensional space \\(\\mathbb{R}^n\\) a common measure of similarity between two vectors is cosine similarity: the cosine of the angle \\(\\theta\\) between the vectors. We calculate this value as follows: \\[ \\cos(\\theta) = \\frac{ \\mathsf{v} \\cdot \\mathsf{w}} {\\| \\mathsf{v}\\| \\, \\|\\mathsf{w}\\|} = \\frac{ \\mathsf{v} \\cdot \\mathsf{w}} {\\sqrt{\\mathsf{v} \\cdot \\mathsf{v}} \\sqrt{\\mathsf{w} \\cdot \\mathsf{w}}}. \\] This measure has the following nice properties: \\(-1 \\le \\cos(\\theta) \\le 1\\), \\(\\cos(\\theta)\\) is close to 1 if \\(\\mathsf{u}\\) and \\(\\mathsf{v}\\) are closely aligned, \\(\\cos(\\theta)\\) is close to 0 if \\(\\mathsf{u}\\) and \\(\\mathsf{v}\\) are are orthogonal, \\(\\cos(\\theta)\\) is close to \\(-1\\) if \\(\\mathsf{u}\\) and \\(\\mathsf{v}\\) are polar opposites. Let’s use cosine similarity to compare the 99 US Senators (one senate seat was not filled at the time) from the 109th US Congress (2007-2008). Here is code that loads in the voting records. Each row contains the senator’s name, party affiliation, state, and then their record on 46 resolutions. The votes are encoded as a sequence of 0s, 1s, and -1s, where 1 means a ‘aye’ vote, -1 means a ‘nay’ vote, and 0 means the senator abstained. For example, the row corresponding to Joseph Biden is biden = [\"Biden\", \"D\", \"DE\", -1, -1, 1, 1, ... , 1, -1]. library(readr) senate.vote.file = &quot;https://raw.github.com/mathbeveridge/math236_f20//main/data/SenateVoting109.csv&quot; record &lt;- read_csv(senate.vote.file, col_names = TRUE) ## Rows: 99 Columns: 49 ## ── Column specification ──────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (3): Name, Party, State ## dbl (46): V01, V02, V03, V04, V05, V06, V07, V08, V09, V10, ... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. clinton = record[record$Name == &#39;Clinton&#39;,] collins = record[record$Name == &#39;Collins&#39;,] frist = record[record$Name == &#39;Frist&#39;,] mccain = record[record$Name == &#39;McCain&#39;,] obama = record[record$Name == &#39;Obama&#39;,] reid = record[record$Name == &#39;Reid&#39;,] knitr::kable( head(record[,1:10],10), booktabs = TRUE, caption = &#39;First 10 rows of the 109th US Senate voting records&#39; ) Table 8.1: First 10 rows of the 109th US Senate voting records Name Party State V01 V02 V03 V04 V05 V06 V07 Akaka D HI -1 -1 1 1 1 -1 -1 Alexander R TN 1 1 1 1 1 1 1 Allard R CO 1 1 1 1 1 1 1 Allen R VA 1 1 1 1 1 1 1 Baucus D MT -1 1 1 1 1 1 -1 Bayh D IN 1 -1 1 1 1 1 -1 Bennett R UT 1 1 1 1 1 1 1 Biden D DE -1 -1 1 1 1 1 -1 Bingaman D NM 1 0 1 1 1 1 -1 Bond R MO 1 1 1 1 1 1 1 Your first task is to implement our cosine function. get_vote_cosine_similarity(senator1, senator2) returns the cosine of the angle between the vote vectors of get_votes(senator1) and get_votes(senator2) Note: The R command length(vec) returns the number of entries in the vector vec. You can get the magnitude (Euclidean length) of vector vec using the pracma function Norm(vec). To get you started, we have written the helper function get_votes(senator). This function returns the vote vector for the senator. These are the values in columns 3 to 49. Turn In: Your implementation of the function. # returns the vote vector for the senator get_votes &lt;- function(senator) { #unlist makes this dataframe row into a regular vector return (unlist(senator[,4:49])) } # Implement this function! # Return the cosine of the angle between the vote vectors # of senator1 and senator2 get_vote_cosine_similarity &lt;- function(senator1, senator2) { votes1 = get_votes(senator1) votes2 = get_votes(senator2) # your code goes here! # find the cosine of the angle between votes1 and votes2 cosine = 1 return (cosine) } Find the cosine of the angles between every pair of the following senators of note: clinton: Hilary Clinton (D, NY), presidential candidate 2016 mccain: John McCain (R, AZ), presidential candidate 2008 obama: Barack Obama (D, IL), president 2008-2016 collins: Susan Collins (R, ME), moderate Republican Does the cosine similarity pick up on the fact that Senator Collins is a “moderate Republican”? Turn In: Your code and the output. Comment on the similarities between all pairs of senators. The senate majority leader of the 109th Congress was Bill Frist (R, TN). The senate minority leader was Harry Reid (D, NV). Create a function classify_senator(senator) that returns “R” or “D” depending on the cosine similarity of senator to frist and to reid. You will have to write an “if … else statement” (here is the syntax). Then run the classify_all() function that we have written for you. You will see how many senators are misclassified, meaning that their votes are more similar to the leader of the opposing party. Note: Your misclassified list will include Jim Jeffords (I, VT), a Republican who became and Independent in 2001 and then caucused with the Democrats. Your classifer correctly aligns the Independent Jeffords with the Democrats. Turn In: Your code and the output. Comment on how many senators are misclassified, as well as their party affiliations. # Implement this function! # return &quot;R&quot; if senator is closer to frist # return &quot;D&quot; if senator is closer to reid classify_senator &lt;- function(senator) { # your code goes here! # use the get_vote_cosine_similarity() method to compare senator with frist and reid val = &quot;R&quot; return (val) } # returns a dataframe containing all misclassified senator names # and their party affiliations classify_all &lt;- function() { mismatch = vector() for (i in 1:99) { senator = record[i,] party = classify_senator(senator) if (party != record[i,2]) { mismatch = c(mismatch, i) } } record[mismatch,1:2] } # Uncomment the next line to classify all senators! # classify_all() "],["important-definitions.html", "Vector 9 Important Definitions 9.1 Vector Spaces 9.2 Matrices 9.3 Orthogonality 9.4 Spectral Decompostion", " Vector 9 Important Definitions 9.1 Vector Spaces span A set of vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\) span a vector space \\(V\\) if for every \\(\\mathsf{v} \\in V\\) there exist a set of scalars (weights) \\(c_1, c_2, \\ldots, c_n \\in \\mathbb{R}\\) such that \\[ \\mathsf{v} = c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + \\cdots + c_n \\mathsf{v}_n. \\] Connection to Matrices: If \\(A = [\\mathsf{v}_1 \\mathsf{v}_2 \\cdots \\mathsf{v}_n]\\) is the matrix with these vectors in the columns, then this is the same as saying that \\(\\mathsf{x} = [c_1, \\ldots, c_n]^{\\top}\\) is a solution to \\(A x = \\mathsf{v}\\). linear independence A set of vectors \\(\\mathsf{v}_1, \\mathsf{v}_2,\\ldots, \\mathsf{v}_n\\) are linearly independent if the only way to write \\[ \\mathsf{0} = c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + \\cdots + c_n \\mathsf{v}_n \\] is with \\(c_1 = c_2 = \\cdots = c_n = 0\\). Connection to Matrices: If \\(A = [\\mathsf{v}_1 \\mathsf{v}_2 \\cdots \\mathsf{v}_n]\\) is the matrix with these vectors in the columns, then this is the same as saying that \\(A x = \\mathsf{0}\\) has only the trivial solution. linear dependence Conversely, a set of vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\) are linearly dependent if there exist scalars \\(c_1, c_2,\\ldots, c_n \\in \\mathbb{R}\\) that are not all equal to 0 such that \\[ \\mathsf{0} = c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + \\cdots + c_n \\mathsf{v}_n \\] This is called a dependence relation among the vectors. Connection to Matrices: If \\(A = [\\mathsf{v}_1 \\mathsf{v}_2 \\cdots \\mathsf{v}_n]\\) is the matrix with these vectors in the columns, then this is the same as saying that \\(\\mathsf{x} = [c_1, c_2, \\ldots, c_n]^{\\top}\\) is a nontrivial solution to \\(A \\mathsf{x} = \\mathsf{0}\\). linear transformation A function \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is a linear transformation when: \\(T(\\mathsf{u} + \\mathsf{v}) = T(\\mathsf{u}) + T(\\mathsf{v})\\) for all \\(\\mathsf{u}, \\mathsf{v} \\in \\mathbb{R}^n\\) (preserves addition) \\(T(c \\mathsf{u} ) = c T(\\mathsf{u})\\) for all \\(\\mathsf{u} \\in \\mathbb{R}^n\\) and \\(c \\in \\mathbb{R}\\) (preserves scalar multiplication). It follows from these that also \\(T(\\mathsf{0}) = \\mathsf{0}\\). one-to-one A function \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is a one-to-one when: for all \\(\\mathsf{y} \\in \\mathbb{R}^m\\) there is at most one \\(\\mathsf{x} \\in \\mathbb{R}^n\\) such that \\(T(\\mathsf{x}) = \\mathsf{y}\\). onto A function \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is a onto when: for all \\(\\mathsf{y} \\in \\mathbb{R}^m\\) there is at least one \\(\\mathsf{x} \\in \\mathbb{R}^n\\) such that \\(T(\\mathsf{x}) = \\mathsf{y}\\). subspace A subset \\(S \\subseteq \\mathbb{R}^n\\) is a subspace when: \\(\\mathsf{u} + \\mathsf{v} \\in S\\) for all \\(\\mathsf{u}, \\mathsf{v} \\in S\\) (closed under addition) \\(c \\mathsf{u} \\in S\\) for all \\(\\mathsf{u}\\in S\\) and \\(c \\in \\mathbb{R}\\) (closed under scalar multiplication) It follows from these that also \\(\\mathsf{0} \\in S\\). basis A basis of a vector space (or subspace) \\(V\\) is a set of vectors \\(\\mathcal{B} = \\{\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\}\\) in \\(V\\) such that \\(\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\) span \\(V\\) \\(\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\) are linearly independent Equivalently, one can say that \\(\\mathcal{B} = \\{\\mathsf{v}_1, \\mathsf{v}_2, \\ldots, \\mathsf{v}_n\\}\\) is a basis of \\(V\\) if for every vector \\(\\mathsf{v} \\in V\\) there is a unique set of scalars \\(c_1, \\ldots, c_n\\) such that \\[ \\mathsf{v} = c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + \\cdots + c_n \\mathsf{v}_n. \\] (The fact that there is a set of vectors comes from the span; the fact that they are unique comes from linear independence). dimension The dimension of a subspace \\(W\\) is the number of vectors in any basis of \\(W\\). This is also the fewest number of vectors required to span the subspace. 9.2 Matrices invertible The square \\(n \\times n\\) matrix \\(A\\) is invertible when there exists an \\(n \\times n\\) matrix \\(A^{-1}\\) such that \\(A A^{-1} = I = A^{-1} A\\). The Invertible Matrix Theorem collects over two dozen equivalent conditions, each of which guarantees that \\(A\\) is invertible. null space The null space \\(\\mbox{Nul}(A) \\subset \\mathbb{R}^n\\) of the \\(m \\times n\\) matrix \\(A\\) is the set of solutions to the homogeneous equation \\(A \\mathsf{x} = \\mathbf{0}\\)&gt; We also write this as \\[ \\mbox{Nul}(A) = \\{ \\mathsf{x} \\in \\mathbb{R}^n : A \\mathsf{x} = \\mathbf{0} \\} \\] Connection to Linear Transformations: If \\(T(\\mathsf{x}) = A \\mathsf{x}\\), then the kernel of \\(T\\) is the null space of matrix \\(A\\). column space The column space \\(\\mbox{Col}(A) \\subset \\mathbb{R}^m\\) of the \\(m \\times n\\) matrix \\(A\\) is the set of all linear combinations of the columns of \\(A\\). For \\(A = \\begin{bmatrix} \\mathsf{a}_1 &amp; \\mathsf{a}_2 &amp; \\cdots &amp; \\mathsf{a}_n \\end{bmatrix}\\), we have \\[ \\mbox{Col}(A) = \\mbox{span} ( \\mathsf{a}_1, \\mathsf{a}_2, \\ldots , \\mathsf{a}_n ) \\] We also write this as \\[ \\mbox{Col}(A) = \\{ \\mathsf{b} \\in \\mathbb{R}^m : \\mathsf{b} = A \\mathsf{x} \\mbox{ for some } \\mathsf{x} \\in \\mathbb{R}^n \\}. \\] Connection to Linear Transformations: If \\(T(\\mathsf{x}) = A \\mathsf{x}\\), then the range (also called the image) of \\(T\\) is the column space of matrix \\(A\\). rank The rank of the \\(m \\times n\\) matrix \\(A\\) is the dimension of the column space of \\(A\\). This is also the number of pivot columns of the matrix. eigenvalue and eigenvector For a square \\(n \\times n\\) matrix \\(A\\), the scalar \\(\\lambda \\in \\mathbb{R}\\) is an eigenvalue for \\(A\\) when there exists a nonzero vector \\(\\mathsf{x} \\in \\mathbb{R}^n\\) such that \\(A \\mathsf{x} = \\lambda \\mathsf{x}\\). The nonzero vector \\(\\mathsf{x}\\) is the eigenvector for eigenvalue \\(\\lambda\\). The collection of all of these eigenvalues and eigenvectors is called the eigensystem of A. diagonalization A square \\(n \\times n\\) matrix is diagonalizable when \\(A = P D P^{-1}\\) where \\(D\\) is a diagonal matrix and \\(P\\) is an invertible matrix. In this case, the eigenvalues of \\(A\\) are the diagonal entries of \\(D\\) and their corresponding eigenvectors are the columns of \\(P\\). dominant eigenvalue The eigenvalue \\(\\lambda\\) of the square matrix \\(A\\) is the dominant eigenvalue when \\(| \\lambda | &gt; | \\mu |\\) where \\(\\mu\\) is any other eigenvalue of \\(A\\). The dominant eigenvalue determines the long-term behavior of \\(A^t\\) as \\(t \\rightarrow \\infty\\). 9.3 Orthogonality length The length of a vector \\(\\mathsf{v}\\) is \\[ \\| \\mathsf{v} \\| = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2}. \\] distance and angle The distance between vectors \\(\\mathsf{u}\\) and \\(\\mathsf{v}\\) is \\[ \\mbox{dist}(\\mathsf{u},\\mathsf{v}) = \\| \\mathsf{u} - \\mathsf{v} \\|. \\] The angle \\(\\theta\\) between these vectors is determined by \\[ \\cos \\theta = \\frac{\\mathsf{u} \\cdot \\mathsf{v}}{ \\| \\mathsf{u} \\| \\, \\| \\mathsf{v} \\|}. \\] orthogonal The vectors \\(\\mathsf{u}\\) and \\(\\mathsf{v}\\) are orthogonal when \\(\\mathsf{u} \\cdot \\mathsf{v} = 0\\). This means that either one of them is the zero vector, or they are perpendicular to one another. orthogonal complement If \\(W \\subset \\mathbb{R}^n\\) is a subspace, then its orthogonal complement \\(W^{\\perp}\\) is the set of all vectors in \\(\\mathsf{R}^n\\) that are orthogonal to \\(W\\). We also write \\[ W^{\\perp} = \\{ \\mathsf{v} \\in \\mathbb{R}^n : \\mathsf{v} \\cdot \\mathsf{w} \\mbox{ for all } \\mathsf{w} \\in W \\}. \\] orthonormal set A collection of vectors \\(\\mathsf{u}_1, \\mathsf{u}_2, \\ldots, \\mathsf{u}_k\\) are an orthonormal set when every vector has length 1 and the vectors are pairwise orthogonal. orthogonal matrix orthogonal matrix A square \\(n \\times n\\) matrix \\(P\\) is an orthogonal matrix when its columns are an orthonormal set. As a result, we have \\(P^{-1} = P^{\\top}\\). projection and residual The orthogonal projection of vector \\(\\mathsf{y}\\) into a subspace \\(W\\) is the unique vector \\(\\hat{\\mathsf{y}} \\in W\\) such that \\(\\mathsf{z} = \\mathsf{y} - \\hat{\\mathsf{y}} \\in W^{\\perp}\\). The vector \\(\\mathsf{z}\\) is called the residual vector for the projection. 9.4 Spectral Decompostion orthogonal diagonalization Every symmetric \\(n \\times n\\) matrix is orthogonally diagonalizable, meaning that we have \\(A = P D P^{\\top}\\) where \\(D\\) is a diagonal matrix and \\(P\\) is an orthogonal matrix. The diagonal entries of \\(D\\) are the eigenvalues of \\(A\\) and the columns of \\(P\\) are the corresponding orthonormal eigenvectors. Furthermore, the eigenvalues of \\(A\\) are nonnegative. spectral decomposition A symmetric matrix \\(A\\) can be written as a linear combination of rank 1 matrices derived from the orthonormal eigensystem of \\(A\\). In particular, we have \\[ A = \\lambda_1 \\mathsf{u}_1 \\mathsf{u}_1^{\\top} + \\lambda_2 \\mathsf{u}_2 \\mathsf{u}_2^{\\top} + \\cdots + \\lambda_n \\mathsf{u}_n \\mathsf{u}_n^{\\top}. \\] This linear combination of rank 1 vectors is called the spectral decomposition of \\(A\\). singular value decomposition (SVD) Any \\(m \\times n\\) matrix \\(A\\) of rank \\(r\\) can be factored into its singular value decomposition \\(U \\Sigma V^{\\top}\\) where \\(U\\) is an \\(m \\times m\\) orthogonal matrix, \\(\\Sigma\\) is a matrix whose nonzero entries are the positive numbers \\(\\sigma_1, \\ldots , \\sigma_r\\), which appear in decreasing order on the diagonal, and \\(V\\) is an \\(n \\times n\\) orthogonal matrix. The nonzero entries of \\(\\Sigma\\) are called the singular values of \\(A\\). The columns of \\(U\\) are the left singular vectors and the rows of \\(V^{\\top}\\) are the right singular vectors. SVD spectral decomposition Any \\(m \\times n\\) matrix \\(A\\) of rank \\(r\\) can be written as a linear combination of rank 1 matrices derived from the singular value decomposition of \\(A\\). In particular, we have \\[ A = \\sigma_1 \\mathsf{u}_1 \\mathsf{v}_1^{\\top} + \\sigma_2 \\mathsf{u}_2 \\mathsf{v}_2^{\\top} + \\cdots + \\sigma_r \\mathsf{u}_r \\mathsf{v}_r^{\\top}. \\] This linear combination of rank 1 vectors is called the (SVD) spectral decomposition of \\(A\\). "],["week-1-learning-goals.html", "Vector 10 Week 1 Learning Goals 10.1 Solving Linear Equations 10.2 RStudio 10.3 Vocabulary 10.4 Conceptual Thinking", " Vector 10 Week 1 Learning Goals Here are the knowledge and skills you should master by the end of this first, shorter week. 10.1 Solving Linear Equations I should be able to do the following tasks: Identify linear systems from nonlinear systems Create a linear system to solve a variety of applied scenarios Convert between a linear system and an augmented matrix Row reduce an augmented matrix into Row Echelon Form (REF) and Reduced Row Echelon Form (RREF) Use REF to determine whether a linear system is consistent or inconsistent Use REF to determine whether a consistent system has a unique solution or an infinite number of solutions Use RREF to find explicit equations for the solution set of a consistent system 10.2 RStudio I should be able to do the following tasks: Log in to Macalester’s RStudio server Upload R Markdown files to RStudio Knit R Markdown to produce HTML Use RStudio to create vectors and matrices Use the rref command from pracma to solve a linear system 10.3 Vocabulary I should know and be able to use and explain the following terms: elementary row operation (and be able to state them) augmented matrix REF and RREF pivot position basic variable (pivot variable) free variable consistent system and inconsistent system 10.4 Conceptual Thinking I should understand and be able to perform the following conceptual tasks: Model 2-dimensional linear systems as the intersections of lines Model 3-dimensional linear systems as the intersections of planes "],["week-2-learning-goals.html", "Vector 11 Week 2 Learning Goals 11.1 Solution Sets, Span and Linear Independence 11.2 Vocabulary 11.3 Conceptual Thinking", " Vector 11 Week 2 Learning Goals Here are the knowledge and skills you should master by the end the second week. 11.1 Solution Sets, Span and Linear Independence I should be able to do the following tasks: Go back and forth between (i) systems of equations, (ii) vector equations, and (iii) the matrix equation \\(Ax = b\\). Compute and understand the matrix-vector product \\(A x\\) both as a linear combination of the columns of A and as the dot product of \\(x\\) with the rows of \\(A\\). Write the solution set to \\(Ax=b\\) as a parametric vector equation. Determine whether a set of vectors is linearly dependent or independent Find a dependence relation among a set of vectors Decide if a set of vectors span \\(\\mathbb{R}^n\\) 11.2 Vocabulary I should know and be able to use and explain the following terms or properties. \\(A(x + y) = Ax + Ay\\) and \\(A(c x) = c A x\\) homogeneous and nonhomogeneous equations parametric vector equations linear independence and linear dependence 11.3 Conceptual Thinking I should understand and be able to explain the following concepts: Theorem 4 in Section 1.4 which says that the following are equivalent (they are all true or are all false) for an \\(m \\times n\\) matrix \\(A\\) For each \\(b \\in \\mathbb{R}^m\\), the system \\(A x = b\\) has at least one solution Each \\(b \\in \\mathbb{R}^m\\) is a linear combination of the columns of \\(A\\) The columns of \\(A\\) span \\(\\mathbb{R}^m\\) \\(A\\) has a pivot in every row. Understand the relation between homogeneous solutions and nonhomogeneous solutions. Linear independence Span More than \\(n\\) vectors in \\(\\mathbb{R}^n\\) must be linearly dependent. "],["week-3-learning-goals.html", "Vector 12 Week 3 Learning Goals 12.1 Linear Transformations and Matrix Inverses 12.2 Vocabulary 12.3 Conceptual Thinking", " Vector 12 Week 3 Learning Goals Here are the knowledge and skills you should master by the end the third week. 12.1 Linear Transformations and Matrix Inverses I should be able to do the following tasks: Determine whether a mapping from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^n\\) is a linear transformation. Use the RREF of the corresponding matrix to determine whether \\(T(\\mathsf{x})\\) is one-to-one and/or onto. Describe 2D linear transformations as a mixture of geometric operations, including expansion, contraction, reflection, rotation, shearing and dimension reduction. Perform a 2D translation using 3D homogeneous coordinates. Multiply an \\(m \\times n\\) matrix with an \\(n \\times p\\) matrix to get an \\(m \\times p\\) matrix. Determine whether a \\(2 \\times 2\\) matrix is invertible. Find the inverse of a \\(2 \\times 2\\) matrix by hand. Use RStudio to check for invertiblity and to find the inverse of an \\(n \\times n\\) square matrix. Explain the connection between Gaussian Elimination, elementary matrices, and the matrix inverse. 12.2 Vocabulary I should know and be able to use and explain the following terms or properties. linear transformation: \\(T(a \\mathsf{u} + b \\mathsf{v}) = a T(\\mathsf{u}) + b T(\\mathsf{v})\\) domain, codomain (aka target) and range (aka image) \\(T\\) maps vector \\(\\mathsf{x}\\) to its image \\(T(\\mathsf{x})\\) one-to-one onto standard matrix for a linear transformation homogeneous coordinates transpose of a matix invertible matrix elementary matrices 12.3 Conceptual Thinking I should understand and be able to explain the following concepts: A linear transformation \\(T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) corresponds to multiplication by an \\(m \\times n\\) matrix \\(A\\). \\(T(\\mathsf{x})=\\mathsf{A} \\mathsf{x}\\) is a one-to-one linear transformations if and only \\(\\mathsf{A}\\) has linearly independent columns \\(T(\\mathsf{x})=\\mathsf{A} \\mathsf{x}\\) is an onto linear transformations if and only if the columns of \\(\\mathsf{A}\\) span \\(\\mathbb{R}^m\\). The Invertible Matrix Theorem (Section 2.3, Theorem 8, page 112) is one of the highlights of the course! It gives 12 different conditions that all equivalent! You should think deeply about why everything comes together like this for square matrices. "],["week-4-learning-goals.html", "Vector 13 Week 4 Learning Goals 13.1 Vector Spaces and the Determinant 13.2 Vocabulary 13.3 Conceptual Thinking", " Vector 13 Week 4 Learning Goals Here are the knowledge and skills you should master by the end of the fourth week. 13.1 Vector Spaces and the Determinant I should be able to do the following tasks: Prove/disprove that a subset of a vector space is a subspace. Prove/disprove that a set of vectors is linearly dependent. Prove/disprove that a set of vectors span a vector space (or a subspace). Find the kernel and image of \\(T(\\mathsf{x}) = Ax\\). Determine whether a set of vectors is a basis. Find a basis for \\(\\mathrm{Nul}(A)\\) and a basis for \\(\\mathrm{Col}(A)\\). Find the change-of-coordinate matrix \\(P_{\\mathcal{B}}\\) from basis \\({\\mathcal{B}}\\) to the standard basis \\(\\mathcal{S}\\). Use matrix inverses (and RStudio) to find the change-of-coordinate matrix \\(P_{\\mathcal{B}}^{-1}\\) from basis \\({\\mathcal{S}}\\) to the standard basis \\(\\mathcal{B}\\). Find the coordinate vector with respect to a given basis. Find the dimension of a vector space (or subspace) by finding or verifying a basis. Find the determinant of a \\(2 \\times 2\\) matrix by hand. Find the determinant of a \\(3 \\times 3\\) matrix by using row operations/cofactor expansion/permutation method. Use RStudio to calculate the determinant of a square matrix. Use \\(\\det(A)\\) to decide whether the square matrix \\(A\\) is invertible. 13.2 Vocabulary I should know and be able to use and explain the following terms or properties. every one of these Important Definitions subspace null space and column space of a matrix kernel and image of a linear transformation basis coordinate vector with respect to a basis change-of-coordinates matrix the coordinate vector with respect to a basis the dimension of a vector space (or a subspace) determinant 13.3 Conceptual Thinking I should understand and be able to explain the following concepts: A vector space consists of a collection of vectors and all of their linear combinations. A subspace is a subset of a vector space that is also a vector space by itself (closed under linear combinations). The solutions to \\(A \\mathsf{x} = \\mathbb{0}\\) form a subspace. The span of the columns of \\(A\\) form a subspace. How the kernel and image of \\(T(\\mathsf{x}) = Ax\\) correspond to the nullspace and columnspace of \\(A\\). Every basis of a given vector space (or subspace) contains the same number of vectors. Why every vector in a vector space has a unique representation as a linear combination of a given basis \\({\\mathcal{B}}\\). How dimension relates to span and linear independence. Interpret \\(\\det(A)\\) as a measure the expansion/contraction of “volumes” in \\(\\mathbb{R}^n\\) under the linear transformation \\(T(\\mathsf{x})=A\\mathsf{x}\\). "],["week-5-6-learning-goals.html", "Vector 14 Week 5-6 Learning Goals 14.1 Eigensystems 14.2 Vocabulary 14.3 Conceptual Thinking", " Vector 14 Week 5-6 Learning Goals Here are the knowledge and skills you should master by the end of the fifth and sixth weeks. 14.1 Eigensystems I should be able to do the following tasks: Check whether a given vector \\(\\mathsf{v}\\) is an eigenvector for square matrix \\(A\\). Find the eigenvalues of a matrix \\(2 \\times 2\\) matrix by hand, using the characteristic equation Find the eigenvalues of a triangular matix by inspection. Given the eigenvalues of matrix \\(A\\), find the eigenvectors by solving \\((A \\lambda I) = \\mathbf{0}\\). Find the eigenvalues and (“human readable”) eigenvectors of an \\(n \\times n\\) matrix \\(A\\) using eigen(A) on RStudio. Determine whether a matrix is diagonalizable. Factor a diagonalizable \\(n \\times n\\) matrix as \\(A = PDP^{-1}\\) where \\(D\\) is a diagonal matrix of eigenvalues and \\(P\\) is the matrix whose columns are the corresponding eignvectors. Use RStudio to find complex eigenvalues and (“human readable”) eigenvectors of a square matrix. Factor a \\(2 \\times 2\\) scaling-rotation matrix as \\(A = P C P^{-1}\\) where \\(C\\) is a scaling-rotation matrix \\(\\begin{bmatrix} a &amp; -b \\\\ b &amp; a \\end{bmatrix}\\) and \\(P = [ \\mathsf{w}, \\mathsf{u}]\\) where \\(\\mathsf{v} = \\mathsf{u} + i \\mathsf{w}\\) is the eigenvector for \\(\\lambda = a + b i\\). Use RStudio to find the Gould Index of a network Use RStudio to create a 2D plot of pairs of eigenvalues of a square matrix Use the dominant eigenvalue and dominant eigenvector to determine the long-term behavior of a dynamical system Create a trajectory of a \\(2 \\times 2\\) dynamical system (either using RStudio or by using a given vector field plot) and then relate the trajectory to the eigenvectors and eigenvalues Interpret the constants in the \\(2 \\times 2\\) matrix for two interacting populations (competition, predator-prey, mutualism, etc) Use RStudio to investigate the animal population modeled with a Leslie matrix. 14.2 Vocabulary I should know and be able to use and explain the following terms or properties. eigenvalue, eigenvector and eigenspace characteristic equation diagonalizable matrix similar matrices algebraic multiplicity of an eigenvalue geometric multiplicity of an eigenvalue scaling-rotation matrix Gould Index discrete dynamical system trajectory dominant eigenvalue and dominant eigenvector population model Leslie matrix 14.3 Conceptual Thinking I should understand and be able to explain the following concepts: An eigenspace of \\(A\\) is a subspace that is fixed under the linear transformation \\(T(\\mathsf{x}) = A \\mathsf{x}\\). An eigenvalue \\(\\lambda\\) with \\(1 &lt;| \\lambda |\\) corresponds to expansion. An eigenvalue \\(\\lambda\\) with \\(0 &lt; | \\lambda | &lt; 1\\) corresponds to contraction. A complex eigenvalue corresponds to a rotation in a 2D subspace. The eigenspace for \\(\\lambda\\) is the subspace \\(\\mathrm{Nul}(A - \\lambda I)\\). A matrix is not diagonalizable when it has complex eigenvalues. A matrix is not diagonalizable when it has an eigenvalue whose algebraic mutiplicity is strictly larger than its geometrix multiplicity. The Gould Index measures the centrality of a vertex in the network. The eigenvalues of a matrix “encode some of the patterns” found in the matrix. Larger magnitude eigenvalues indicate more important patterns. The long-term behavior of a dynamical system is determined by its dominent eigenvalue and eigenvector. Any population model predicts one of: long term growth, extinction, convergence to a stable population, or convergence to a stable orbit of populations. "],["week-7-8-learning-goals.html", "Vector 15 Week 7-8 Learning Goals 15.1 Orthogonality and SVD 15.2 Vocabulary 15.3 Conceptual Thinking", " Vector 15 Week 7-8 Learning Goals Here are the knowledge and skills you should master by the end of the seventh and eighth weeks. 15.1 Orthogonality and SVD I should be able to do the following tasks: Find the length of a vector Find the distance between two vectors Normalize a vector Find the cosine of the angle between two vectors Find the orthogonal projection of one vector onto another Find the orthogonal projection of one vector onto a subspace (using an orthogonal basis) Find the orthogonal complement of a subspace Use the Gram-Schmidt process to create an orthonormal basis (starting from a given basis) Find the least squares approximation for an inconsistent system Formulate a curve fitting problem as an inconsistent linear system \\(A \\mathsf{x} = \\mathsf{b}\\) Orthogonally diagonalize a symmetric matrix as \\(A=PDP^{\\top}\\). Find the spectral decomposition \\(A = \\lambda_1 \\mathsf{v}_1 \\mathsf{v}_1^{\\top} + \\lambda_2 \\mathsf{v}_2 \\mathsf{v}_2^{\\top} + \\cdots + \\lambda_n \\mathsf{v}_n \\mathsf{v}_n^{\\top}\\) of a symmetric matrix \\(A\\) Use an orthogonal diagonalization to find the best rank \\(k\\) approximation of symmetric matrix \\(A\\) Find the singular value decomposition \\(A = U \\Sigma V^{\\top}\\) of a rectangular matrix \\(A\\) Use the singular value decomposition \\(A = U \\Sigma V^{\\top}\\) to find orthonormal bases for \\(\\mbox{Row}(A), \\mbox{Nul}(A), \\mbox{Col}(A), \\mbox{Nul}(A^{\\top})\\) Find the SVD spectral decomposition \\(A = \\sigma_1 \\mathsf{u}_1 \\mathsf{v}_1^{\\top} + \\sigma_2 \\mathsf{u}_2 \\mathsf{v}_2^{\\top} + \\cdots + \\lambda_n \\mathsf{u}_r \\mathsf{v}_r^{\\top}\\) of a rank \\(r\\) matrix \\(A\\) Use SVD to find the best rank \\(k\\) approximation of a matrix \\(A\\) 15.2 Vocabulary I should know and be able to use and explain the following terms or properties. dot product of two vectors \\(\\mathsf{v} \\cdot \\mathsf{w} = \\mathsf{v}^{\\top} \\mathsf{w}\\) (aka scalar product, inner product) length (magnitude) of a vector angle between vectors normalize unit vector orthogonal vectors orthogonal complement of a subspace orthogonal projection orthogonal basis orthonormal basis Gram-Schmidt process normal equations for a least squares approximation least squares solution residual vector symmetric matrix orthogonally diagonalizable outer product of two vectors \\(\\mathsf{v} \\, \\mathsf{w}^{\\top}\\) spectral decomposition of a symmetric matrix singular value decomposition singular value, left singular vector, right singular vector SVD spectral decomposition of a rectangular matrix image compression 15.3 Conceptual Thinking I should understand and be able to explain the following concepts: The dot product gives an algebraic encoding of the geometry (lengths and angles) of \\(\\mathbb{R}^n\\) If two vectors are orthogonal, then they are perpendicular, or one of them is the zero vector An orthogonal projection is a linear transformation The row space of a matrix is orthogonal to its nullspace The inverse of orthogonal matrix \\(A\\) is the transpose \\(A^{\\top}\\) Cosine similarity is a useful way to compare vectors, especially in high-dimensional vector spaces. The residual vector measures the quality of fit of a least squares solution The outer product \\(\\mathsf{v}\\, \\mathsf{w}^{\\top}\\) is a square matrix with rank 1 Singular value decomposition is a generalization of orthogonal diagonalization The singular values of \\(A\\) are the square roots of the eigenvalues of \\(A^{\\top}A\\). The spectral decomposition of a matrix allows us to approximate a matrix with a linear combination of rank 1 matrices. The relative magnitudes of the eigenvectors/singular values indicate the quality of the spectral decomposition approximation. "],["linear-systems-in-r.html", "Vector 16 Linear Systems in R 16.1 Building Vectors and Matrices 16.2 Solving a Linear System 16.3 Solving another Linear System 16.4 Appendix: Dimensionless Vectors in R", " Vector 16 Linear Systems in R Let’s learn how to use R to solve systems of linear equations! Download this Rmd file. First, we will create vectors and matrices Then we will see how to create an augmented matrix and then apply Gaussian Elimination to obtain is reduced row echelon form. Gaussian elimination is performed by the rref() command. However, this command is not loaded into R by default. So we have have to tell RStudio to use the practical math package, which is known as pracma. So we need to run the following command once at the beginning of our session. require(pracma) 16.1 Building Vectors and Matrices A vector in R is a list of data. The simplest way to create a vector is to use the c() command. The letter ‘c’ is short for ‘combine these values into a vector.’ For example, we can make a vector v for the numbers 1,2,3 as follows: v=c(1,2,3) v ## [1] 1 2 3 Note that we had to ask R to display the value of v. This is because the assignment of v doesn’t echo the value to the console. But can see the value of v in the Environment tab in the upper right panel of RStudio. For example, run this command and then check to see that the value of v gets updated in the environment. v=c(1,2,3,4,5,6) It is interesting to note that c() returns a dimensionless vector. So you can treat a vector c() as either a row or a column when you construct a matrix. For example, suppose that we want to make the matrix \\[ A = \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 2 &amp; 4 &amp; 8 \\\\ 3 &amp; 9 &amp; 27 \\end{bmatrix}. \\] We could create this matrix by binding three row vectors: A = rbind(c(1,1,1), c(2,4,8), c(3,9,27)) A ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 2 4 8 ## [3,] 3 9 27 or we could bind three column vectors: A = cbind(c(1,2,3), c(1,4,9), c(1,8,27)) A ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 2 4 8 ## [3,] 3 9 27 16.2 Solving a Linear System Suppose that we want to solve the linear system \\[\\begin{aligned} x + y + z &amp;= 7 \\\\ 2x + 4y + 8z &amp;= 6 \\\\ 3x +9y+27z &amp;=12 \\end{aligned}\\] which has coefficient matrix \\[ A = \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 2 &amp; 4 &amp; 8 \\\\ 3 &amp; 9 &amp; 27 \\end{bmatrix}. \\] and target (column) vector \\[ b = \\begin{bmatrix} 4 \\\\ 6 \\\\ 12 \\end{bmatrix}. \\] This is the same matrix A we defined above. Let’s define a vector b and use cbind() to create an augmented matrix which we will name Ab. (We could have just made the full augmented matrix from the start, but using cbind to add a column to a matrix is a skill we will use later in the course!) A ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 2 4 8 ## [3,] 3 9 27 b = c(4,6,12) Ab = cbind(A,b) Ab ## b ## [1,] 1 1 1 4 ## [2,] 2 4 8 6 ## [3,] 3 9 27 12 Now we use the rref() command to apply Gaussian Elimination to produce the reduced row echelon form. (And remember: we had to load this function into R by using the require(pracma) command above.) rref(Ab) ## b ## [1,] 1 0 0 7 ## [2,] 0 1 0 -4 ## [3,] 0 0 1 1 We conclude that this is a consistent system no free variables. The unique solution is \\[\\begin{align} x&amp;=7\\\\ y&amp;=-4\\\\ z&amp;=1 \\end{align}\\] We can verify that our answer works by multiplying \\(A\\) by one of the solutions above. Matrix multiplication uses the funny operation %*%. A %*% c(7,-4,1) ## [,1] ## [1,] 4 ## [2,] 6 ## [3,] 12 #A %*% c(1,2,3,0,0) Which matches our target \\[ b = \\begin{bmatrix} 4 \\\\ 6 \\\\ 12 \\end{bmatrix} \\] just as we had hoped. 16.3 Solving another Linear System Now let’s find the solution set for the linear system \\[ \\begin{array}{rrrrrcr} x_1 &amp; &amp; -x_3 &amp; -x_4 &amp; -x_5 &amp; = &amp; -2 \\\\ 2x_1 &amp; +x_2 &amp; +2x_3 &amp; -x_4 &amp; -x_5 &amp; = &amp; 4 \\\\ -x_1 &amp; +x_2 &amp; +x_3 &amp; &amp; &amp; = &amp; 10 \\\\ x_1 &amp; &amp; -x_3 &amp; -x_4 &amp; -x_5 &amp; = &amp; -2 \\\\ \\end{array} \\] which corresponds to augmented matrix \\[ \\left[ \\begin{array}{rrrrr|r} 1 &amp; &amp; -1 &amp; -1 &amp; -1 &amp; -2 \\\\ 2 &amp; +1 &amp; +2 &amp; -1 &amp; -1 &amp; 4 \\\\ -1 &amp; +1 &amp; +1 &amp; &amp; &amp; 10 \\\\ 1 &amp; &amp; -1 &amp; -1 &amp; -1 &amp; -2 \\\\ \\end{array} \\right] \\] This time, let’s just construct the augmented matrix direclty. Then we define the coefficient matrix \\(A\\). Here we use cbind to combine the vectors into the columns of a matrix named \\(A\\). You can use rbind if you want to combine the vectors into the rows of a matrix. Ab = cbind(c(1,2,-1,1),c(0,1,1,0),c(-1,2,1,-1),c(-1,1,0,-1),c(-1,5,0,-1),c(-2,10,4,-2)) Ab ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 0 -1 -1 -1 -2 ## [2,] 2 1 2 1 5 10 ## [3,] -1 1 1 0 0 4 ## [4,] 1 0 -1 -1 -1 -2 And now let’s row reduce to get RREF. rref(Ab) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 0 0 0 1 1 ## [2,] 0 1 0 -1 -1 2 ## [3,] 0 0 1 1 2 3 ## [4,] 0 0 0 0 0 0 So the set of solutions in parametric form is \\[ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 0 \\\\ 0 \\end{bmatrix} + s \\begin{bmatrix} 0 \\\\ 1 \\\\ -1 \\\\ 1 \\\\ 0 \\end{bmatrix} + t \\begin{bmatrix} -1 \\\\ 1 \\\\ -2 \\\\ 0 \\\\ 1 \\end{bmatrix} \\] and this is a “plane” in \\(\\mathbb{R}^5\\). It is in \\(\\mathbb{R}^5\\) because these vectors have 5 coordinates. It is a plane because it is spanned by two vectors that are not on the same line. 16.4 Appendix: Dimensionless Vectors in R Let’s revisit the vector constructed by cbind. Above we called this a “dimensionless” vector because it can be used as a column vector or a row vector. In general, R will do its best to make sense of a dimensionless vector. In other words, it will promote c() to make an expression valid. For example, let \\(A\\) be an \\(n \\times n\\) matrix, and let \\(b\\) be a vector. The expression \\(Av\\) is only defined when \\(v\\) is a \\(n \\times 1\\) column vector and that \\(wA\\) is only defined when \\(w\\) is a \\(1 \\times n\\) ** row vector**. But let’s look at what happens when we use a dimensionless vector instead. A = cbind(c(1,1,1),c(-1,0,1), c(0,1,-1)) A ## [,1] [,2] [,3] ## [1,] 1 -1 0 ## [2,] 1 0 1 ## [3,] 1 1 -1 b = c(2,5,11) b ## [1] 2 5 11 # A times b A %*% b ## [,1] ## [1,] -3 ## [2,] 13 ## [3,] -4 # b times A b %*% A ## [,1] [,2] [,3] ## [1,] 18 9 -6 Both of these multiplications worked! So R treated b as a column vector for the multiplicationA %*% b. And then R treated b as a row vector for the multiplication b %b% A. So how do you make a true column vector or a true row vector? The answer is to use cbind and rbind! Here are some examples: # dimensionless b = c(1,2,3,4) b ## [1] 1 2 3 4 # column vector b.col = cbind(b) b.col ## b ## [1,] 1 ## [2,] 2 ## [3,] 3 ## [4,] 4 # row vector b.row = rbind(b) b.row ## [,1] [,2] [,3] [,4] ## b 1 2 3 4 "],["d-linear-transformations.html", "Vector 17 2D Linear Transformations 17.1 Functions to plot our vectors 17.2 Your Turn 17.3 Exploration 17.4 Even More Exploration", " Vector 17 2D Linear Transformations Let’s explore linear transformations of the plane! Download this Rmd file. We know that a linear transformation \\(T\\) satisfies \\[ T(c \\mathsf{u} + d \\mathsf{v}) = c\\, T( \\mathsf{u}) +d \\, T( \\mathsf{v}). \\] But what do linear transformations look like? Let’s start to answer this question by considering linear transformations of the plane. We will look at mappings \\(T: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2\\) where \\(T(\\mathsf{x}) = \\mathsf{A} \\mathsf{x}\\) for a \\(2 \\times 2\\) matrix \\(\\mathsf{A}\\). \\[ T \\left( \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\right) = \\begin{bmatrix} a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22} \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix}. \\] The linear transformation \\(T(\\mathsf{x})\\) maps the plane to itself. This nicely allows us to compare vectors before the mapping with their images after the mapping. We can then describe the effect of the mapping on the plane. 17.1 Functions to plot our vectors Here are a couple of helper functions to plot our “before” and “after” vectors as arrows in the plane. plot_four_vectors &lt;- function(before1, before2, after1, after2){ a0 = c(0,0, before1[1], before2[1]) b0 = c(0,0, before1[2], before2[2]) a1 = c(before1[1],before2[1],before1[1]+before2[1], before1[1]+before2[1]) b1 = c(before1[2],before2[2],before1[2]+before2[2], before1[2]+before2[2]) c0 = c(0,0, after1[1], after2[1]) d0 = c(0,0, after1[2], after2[2]) c1 = c(after1[1],after2[1],after1[1]+after2[1], after1[1]+after2[1]) d1 = c(after1[2],after2[2],after1[2]+after2[2], after1[2]+after2[2]) max_val = max(a0,b0,a1,b1,c0,d0,c1,d1) min_val = min(a0,b0,a1,b1,c0,d0,c1,d1) plot(NA, xlim=c(min_val*1.5,max_val*1.5), ylim=c(min_val,max_val), xlab=&quot;X&quot;, ylab=&quot;Y&quot;, ) abline(v=0, col=&quot;gray&quot;) abline(h=0, col=&quot;gray&quot;) vecs &lt;- data.frame(vname=c(&quot;a&quot;,&quot;b&quot;,&quot;a+b&quot;, &quot;transb&quot;), x0=a0, y0=b0, x1=a1 ,y1=b1, col=1:4) with( vecs, mapply(&quot;arrows&quot;, x0, y0, x1,y1,col=col, lty=2) ) vecs &lt;- data.frame(vname=c(&quot;a&quot;,&quot;b&quot;,&quot;a+b&quot;, &quot;transb&quot;), x0=c0, y0=d0, x1=c1, y1=d1, col=1:4) with( vecs, mapply(&quot;arrows&quot;, x0, y0, x1,y1,col=col) ) } This function has four parameters. The parameters before1 and before2 are two vectors before the mapping, and after1 and after2 are their images after the mapping. The function creates a dashed plot of the parallelogram with corners (0,0), before1, before2 and before1+before2. It also creates a solid plot of the image of this parallelogram. These are plotted on the same plane so taht we can easily compare “before” and “after.” Here is an example. Let’s start with vectors \\(\\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix}\\) and \\(\\begin{bmatrix} 0 \\\\ 2 \\end{bmatrix}\\). It’s nice to start with vectors of different lengths. Let’s consider the mapping correspoding to multiplication by the matrix \\[ A = \\begin{bmatrix} 2 &amp; 3 \\\\ 4 &amp; -1 \\end{bmatrix}. \\] Here is some example code showing how to make our plot. Note: A %*% x is the syntax for matrix multiplication in R. vec1 = c(3,0) vec2 = c(0,2) A = cbind(c(2,1), c(4,-1)) newvec1 = A %*% vec1 newvec2 = A %*% vec2 plot_four_vectors(vec1, vec2, newvec1, newvec2) Figure 17.1: An example 2D linear transformation This visualization uses different colors for the vectors so that you can match up the original vector with its image. There is a lot going on in this mapping, so let’s start making some observations. The original rectangle mapped to a parallelogram. So the shape is ``squished’’ a bit. Both the black vector and the red vector have gotten larger. But we can see that the red vector has grown much more than the black one. So there is expansion, but it is not uniform. The black vector and the red vector have flipped! This means that there is some sort of reflection happening. There isn’t a simple description for what’s happening here. It’s a combination of effects, so that the image of the rectangle is a warped version of the original. 17.2 Your Turn Now it’s your turn. Investigate the effect of each of the following families of mappings. Using the previous code snippet as a guide, create a “before and after plot” for the black vector \\(\\begin{bmatrix} 3 \\\\ 0 \\end{bmatrix}\\) and the red vector \\(\\begin{bmatrix} 0 \\\\ 2 \\end{bmatrix}\\) Describe the effect of the mapping as best you can. Be sure to look at the different effect on the black vector and the red vector. Use words like, expansion, contraction, rotation, reflection and shear. Once you have looked at the effect of the family, look back at the form of the matrix \\(A\\). Can you explain why it leads to the outcome you see? 17.2.1 Family 1 Explore matrices of the form \\[A=\\displaystyle{ \\begin{bmatrix} a &amp; 0 \\\\ 0 &amp; b \\end{bmatrix}}.\\] Once again, try various combinations of positive, negative, small and large numbers. Here is some sample code that would create such a matrix for \\(a=1\\) and \\(b=1\\). a = 1 b = 1 A = cbind(c(a,0), c(0,b)) 17.2.2 Family 2 Explore matrices of the form \\[A=\\displaystyle{ \\begin{bmatrix} 0 &amp; b \\\\ c &amp; 0 \\end{bmatrix}}.\\] Try various combinations of positive and negative numbers. Also try numbers of small magnitude (less than 1) and large magnitude. 17.2.3 Family 3 Explore matrices of the form \\[A=\\displaystyle{ \\begin{bmatrix} \\cos(t) &amp; -\\sin(t) \\\\ \\sin(t) &amp; \\cos(t) \\end{bmatrix}}\\] where \\(t\\) is in radians. Here is a function that will create such a matrix. create_angle_matrix &lt;- function(t) { A = cbind(c(cos(t), sin(t)), c(-sin(t), cos(t))) return(A) } A = create_angle_matrix(pi/2) A ## [,1] [,2] ## [1,] 6.123234e-17 -1.000000e+00 ## [2,] 1.000000e+00 6.123234e-17 Note: R is numerical software. You’ll note that sin(pi/2) returns a value of 6.123234e-17 or something similar. This value is given in scientific notation, and is \\(\\approx 6.12 \\times 10^{-17}\\). You should treat this tiny number as equal to 0. Be sure to keep an eye out for return values like this that are ``numerically equivalent to 0.’’ 17.2.4 Family 4 Now explore matrices of the form \\[A=\\displaystyle{ \\begin{bmatrix} a &amp; -b \\\\ b &amp; a \\end{bmatrix}}\\] Once again, consider a wide variety of such matrices. 17.2.5 Some Other Matrices Now try these matrices. For each one, try to make other matrices that have a similar effect. (What is the relationship between the entries that leads to this particular kind of image?) \\[ A=\\displaystyle{ \\begin{bmatrix} 1 &amp; -3 \\\\ 2 &amp; -6 \\end{bmatrix}}, \\qquad B=\\displaystyle{ \\begin{bmatrix} 1 &amp; 1 \\\\ 0 &amp; 1 \\end{bmatrix}}, \\qquad C=\\displaystyle{ \\begin{bmatrix} 0 &amp; 1 \\\\ 0 &amp; 0 \\end{bmatrix}}, \\qquad D=\\displaystyle{ \\begin{bmatrix} 3 &amp; 0 \\\\ 3 &amp; 2 \\end{bmatrix}}, \\qquad \\] 17.3 Exploration Now it’s time to explore. Try one or more of the following: Can you find a mapping that will turn the rectangle with corners (0,0), (3,0), (3,2), (0,2) into a square? Look at some different starting vectors. Try some other matrices. What is the relationship between the rank of the matrix and the image of the transformation? 17.4 Even More Exploration For a more interactive experinece, head over to Section 2.6.2 of Understanding Linear Algbra. This short section on the geometry of \\(2 \\times 2\\) matrix transformations has an interactive activity where you can change the entries using slider controls. It is illuminating to see the progressive impact of your choices! "],["linear-transformations-of-a-house.html", "Vector 18 Linear Transformations of a House 18.1 Rotations 18.2 Expansion and contraction 18.3 Reflection over the line \\(y=x\\) 18.4 Shear Transformations 18.5 Dimension Reduction 18.6 A Complicated Transformation", " Vector 18 Linear Transformations of a House Download this Rmd file from GitHub Here is a plot of my house. You will need to run this chunk of code each time you re-start R to get the house back in memory. house = cbind(c(0,0), c(0,3/4), c(1/2,3/4), c(1/2,0), c(1,0), c(1,1), c(5/4,1), c(0,2), c(-5/4,1), c(-1,1), c(-1,0), c(0,0)); plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-5,5),ylim=c(-5,5),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) Today we will explore various families of linear transformations on the plane by looking at their effect on my house. We give a series of examples of 2D linear transformations. After each example, it’s your turn to play with variations from the same family of transformations. Using the previous code snippets as a guide, create a single plot with the “before and after” houses. Discuss the results with your group and share cool examples! Describe the effect of the mapping as best you can. Be sure to look at the different effect on the “ground” and the “walls” of the course. Use words like, expansion, contraction, rotation, reflection and shear. Once you have looked at the effect of the family, look back at the form of the matrix \\(A\\). Can you explain why it leads to the outcome you see? 18.1 Rotations First we will rotate my house by pi/3 radians. A 2D rotation matrix by \\(t\\) radians is given by \\[A=\\displaystyle{ \\begin{bmatrix} \\cos(t) &amp; -\\sin(t) \\\\ \\sin(t) &amp; \\cos(t) \\end{bmatrix}}.\\] Here is the code to display this transformation. Observe that I apply the matrix A to the house, call it house2 and plot both the original house and the new house in the same plot. # create a plot that we will add more layers to plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-5,5),ylim=c(-5,5),xlab=&quot;x&quot;,ylab=&quot;y&quot;) # add the grid lines abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) # define the matrix A A = cbind(c(cos(pi/3),sin(pi/3)),c(-sin(pi/3),cos(pi/3))) # display A A ## [,1] [,2] ## [1,] 0.5000000 -0.8660254 ## [2,] 0.8660254 0.5000000 # transform the house using matrix multiplication house2 = A %*% house # plot the original house polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) # plot the transformed house polygon(house2[1,], house2[2,], col = &quot;pink&quot;, border = &quot;black&quot;) And now I rotate it back to its original position. The inverse in this case is rotation by -pi/3.I apply this mapping to house2 to get house3 and then plot those houses together. plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-5,5),ylim=c(-5,5),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) Ainv = cbind(c(cos(-pi/3),sin(-pi/3)),c(-sin(-pi/3),cos(-pi/3))) # A inverse Ainv # display A inverse ## [,1] [,2] ## [1,] 0.5000000 0.8660254 ## [2,] -0.8660254 0.5000000 house3 = Ainv %*% house2 polygon(house2[1,], house2[2,], col = &quot;pink&quot;, border = &quot;black&quot;) polygon(house3[1,], house3[2,], col = &quot;green&quot;, border = &quot;black&quot;) 18.1.1 Your Turn Explore matrices of the form \\[A=\\displaystyle{ \\begin{bmatrix} \\cos(t) &amp; -\\sin(t) \\\\ \\sin(t) &amp; \\cos(t) \\end{bmatrix}}\\] where \\(t\\) is in radians. Here is a function that will create such a matrix (so that you only have to enter the angle once). create_angle_matrix &lt;- function(t) { A = cbind(c(cos(t), sin(t)), c(-sin(t), cos(t))) return(A) } A = create_angle_matrix(pi/2) A ## [,1] [,2] ## [1,] 6.123234e-17 -1.000000e+00 ## [2,] 1.000000e+00 6.123234e-17 Note #1: R is numerical software. The example code rotates the couse by \\(pi/2\\). You’ll note that sin(pi/2) returns a value of 6.123234e-17 or something similar. This value is given in scientific notation, and is \\(\\approx 6.12 \\times 10^{-17}\\). You should treat this tiny number as equal to 0. Be sure to keep an eye out for return values like this that are “numerically equivalent to 0.” Note #2: By default, an R plot stretches the horizontal scale compared to the vertical scale. In order to create a square plot, your R chunk should start with {r, fig.height=4,fig.width=4} rather than {r}. Here is an example of what that looks like: 18.2 Expansion and contraction Next, let’s perform the transformation that scales the house by 2 in the \\(x\\)-direction and by 3 in the \\(y\\)-direction. (A = cbind(c(2,0),c(0,3))) ## [,1] [,2] ## [1,] 2 0 ## [2,] 0 3 plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-5,5),ylim=c(-5,5),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) house2 = A %*% house polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) polygon(house2[1,], house2[2,], col = &quot;pink&quot;, border = &quot;black&quot;) 18.2.1 Your Turn Explore matrices of the form \\[A=\\displaystyle{ \\begin{bmatrix} a &amp; 0 \\\\ 0 &amp; b \\end{bmatrix}}.\\] Here is some sample code that would create such a matrix for \\(a=1\\) and \\(b=1\\). a = 1 b = 1 A = cbind(c(a,0), c(0,b)) Try various combinations of \\(a\\) and \\(b\\). Here are some guiding questions: How would you expand along the \\(x\\)-axis while contracting along the \\(y\\)-axis? What happens when \\(a\\) is negative? when \\(b\\) is negative? when both are negative? 18.3 Reflection over the line \\(y=x\\) Now let’s try reflecting the house over the line \\(y = x\\). Note that by putting the assignment of A into parentheses it prints out the value of A while you are assigning it (A = cbind(c(0,1),c(1,0))) ## [,1] [,2] ## [1,] 0 1 ## [2,] 1 0 plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-5,5),ylim=c(-5,5),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) house2 = A %*% house polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) polygon(house2[1,], house2[2,], col = &quot;pink&quot;, border = &quot;black&quot;) 18.3.1 Your Turn Explore matrices of the form \\[A=\\displaystyle{ \\begin{bmatrix} 0 &amp; a \\\\ b &amp; 0 \\end{bmatrix}}.\\] Try various combinations of \\(a\\) and \\(b\\), using both positive and negative values. 18.4 Shear Transformations A shear transformation leaves the direction of one axis fixed. Here is an example. (A = cbind(c(1,0),c(1,1))) ## [,1] [,2] ## [1,] 1 1 ## [2,] 0 1 plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-5,5),ylim=c(-5,5),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) house2 = A %*% house polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) polygon(house2[1,], house2[2,], col = &quot;pink&quot;, border = &quot;black&quot;) 18.4.1 Your Turn Now explore matrices of the form \\[ A=\\displaystyle{ \\begin{bmatrix} a &amp; b \\\\ 0 &amp; c \\end{bmatrix}} \\quad \\mbox{and} \\quad A=\\displaystyle{ \\begin{bmatrix} a &amp; 0 \\\\ b &amp; c \\end{bmatrix}} \\] Start with \\(a=b=c=1\\). Then consider a wider variety of such matrices as with the families above. 18.5 Dimension Reduction Here we perform the transformation that sends \\(\\mathsf{e}_1\\) to \\((-1,1/2)\\) and \\(\\mathsf{e}_2\\) to \\((2,-1)\\). Notice that they are the same line and the transformation projects the house onto this line. (A = cbind(c(-1,1/2),c(2,-1))) ## [,1] [,2] ## [1,] -1.0 2 ## [2,] 0.5 -1 plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-5,5),ylim=c(-5,5),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) house2 = A %*% house polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) polygon(house2[1,], house2[2,], col = &quot;pink&quot;, border = &quot;black&quot;) 18.5.1 Your Turn Explore matrices of the form \\[ A=\\displaystyle{ \\begin{bmatrix} a &amp; ca \\\\ b &amp; cb \\end{bmatrix}} \\quad \\mbox{and} \\quad A=\\displaystyle{ \\begin{bmatrix} a &amp; b \\\\ ca &amp; cb \\end{bmatrix}} \\] Once again, consider a wide variety of such matrices using various values for \\(a,b,c\\). 18.6 A Complicated Transformation Finally, suppose that we wish to perform the transformation that reflects over the \\(y\\)-axis, then rotates by pi/2, and finally scales the \\(x\\)-direction by 2 and the y-direction by 3 in that order. You can do this as three matrices that you multiply together. Note: The order of these matrices goes from right to left. The rightmost matrix is the one that is closest to the vector. So that transformation happens first! The matrix A: scale = cbind(c(2,0),c(0,3)) rot = cbind(c(cos(pi/2),sin(pi/2)),c(-sin(pi/2),cos(pi/2))) reflect = cbind(c(-1,0),c(0,1)) A = scale %*% rot %*% reflect plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-5,5),ylim=c(-5,5),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) house2 = A %*% house polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) polygon(house2[1,], house2[2,], col = &quot;pink&quot;, border = &quot;black&quot;) 18.6.1 Your Turn Using the previous example as a guide, create a complicated mapping of your own that combines some of the linear transformations above. "],["d-translations-with-homogeneous-coordinates.html", "Vector 19 2D Translations with Homogeneous Coordinates 19.1 Translation 19.2 Translation and then Rotation 19.3 Rotation and then Translation 19.4 Your Turn", " Vector 19 2D Translations with Homogeneous Coordinates Download this Rmd file A translation of the plane shifts every vector by a constant vector. For example, the mapping \\[ S \\left( \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\right) = \\begin{bmatrix} x \\\\ y \\end{bmatrix} + \\begin{bmatrix} 3 \\\\ -4 \\end{bmatrix} = \\begin{bmatrix} x +3 \\\\ y - 4 \\end{bmatrix} \\] translates every vector in the plane by \\(\\begin{bmatrix} 3 \\\\ -4~ \\end{bmatrix}\\). The bad news: This is a simple and natural mapping, but it is not a linear transformation! We know that a linear transformation must map \\(\\mathbb{0}\\) to \\(\\mathbb{0}\\), and that is certainly not the case when we translate! This restriction is rather limiting for computer graphics: we can never move our image away from the origin. The good news: We can work around this problem by creating a 3D linear transformation \\(T: \\mathbb{R}^3 \\rightarrow \\mathbb{R}^3\\) and then retricting our attention to a plane in this larger space. As discussed in the Homogeneous Coordinates video, we do the following: Embed the \\(xy\\)-plane \\(\\mathbb{R}^2\\) into the plane \\(z = 1\\) in \\(\\mathbb{R}^3\\). Translate in \\(\\mathbb{R}^3\\) using a mapping \\(T\\) that maps this horizontal plane to itself. That is: \\[ T \\left( \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\right) = \\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix}. \\] When we create our plot, we use only the first two coordinates and ignore the third coordinate (which is still 1). In summary, during our calculations, we replace the vector \\(\\begin{bmatrix} x \\\\ y \\end{bmatrix}\\) in \\(\\mathbb{R}^2\\) with the homogeneous coordinate vector \\(\\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}\\) in \\(\\mathbb{R}^3\\). 19.1 Translation Here is my house once again. Note that we have add \\(z=1\\) as the third coordinate to each point. However, when plotting, we only use the first two coordinates. # the third entry always = 1 house = cbind(c(0,0,1), c(0,3/4,1), c(1/2,3/4,1), c(1/2,0,1), c(1,0,1), c(1,1,1), c(5/4,1,1), c(0,2,1), c(-5/4,1,1), c(-1,1,1), c(-1,0,1), c(0,0,1)); # only plot the first two coordinates plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-6,6),ylim=c(-6,6),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) Next, we translate by \\(\\begin{bmatrix} 3 \\\\ - 4 \\end{bmatrix}\\) by using the linear transformation \\[ T \\left( \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} \\right) = \\begin{bmatrix} 1 &amp; 0 &amp; 3 \\\\ 0 &amp; 1 &amp; -4 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}. \\] Let’s check that this has the desired effect on a homogeneous coordinate vector: \\[ T \\left( \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} \\right) = \\begin{bmatrix} 1 &amp; 0 &amp; 3 \\\\ 0 &amp; 1 &amp; -4 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} x+3 \\\\ y - 4 \\\\ 1 \\end{bmatrix}. \\] It worked! Note that this linear transformation maps every horizontal plane to itself. For the plane \\(z=1\\) (and only for this plane!) we get the exact translation that we desire. So it is crucial that \\(z=1\\). That’s the magic of homogeneous coordinates. Let’s do this calculation in R and plot the first two coordiantes: A = cbind(c(1,0,0),c(0,1,0),c(3,-4,1)) house2 = A %*% house # only plot the first two coordinates plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-6,6),ylim=c(-6,6),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) polygon(house2[1,], house2[2,], col = &quot;pink&quot;, border = &quot;black&quot;) 19.2 Translation and then Rotation We know plenty 2D linear transformation, including rotation, reflection and shear mappings. We can turn any of them into a 3D transformation by appending a row and a column with a 1 in the lower right corner and zero everywhere else. For example, the 2D rotation \\[ \\begin{bmatrix} \\cos \\theta &amp; -\\sin \\theta~ \\\\ \\sin \\theta &amp; \\cos \\theta \\end{bmatrix} \\] becomes the 3D transformation \\[ \\begin{bmatrix} \\cos\\theta &amp; -\\sin\\theta~ &amp; 0 \\\\ \\sin\\theta &amp; \\cos\\theta &amp;0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}. \\] This mapping rotates 3D space around the \\(z\\)-axis. So let’s combine two operations: a translation and a rotation. First, let’s translate by \\(\\begin{bmatrix} -2 \\\\ 1 \\end{bmatrix}\\) and then rotate counterclockwise by \\(2 \\pi/3\\). And remember: the matrix closest to the vector acts first. So if we want to translate first, the translation matrix needs to be to the right of the rotation matrix: \\[ T \\left( \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} \\right) = \\begin{bmatrix} \\cos \\frac{2\\pi}{3} &amp; -\\sin\\frac{2\\pi}{3}~ &amp; 0 \\\\ \\sin\\frac{2\\pi}{3} &amp; \\cos\\frac{2\\pi}{3} &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 0 &amp; -2 \\\\ 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}. \\] translate = cbind(c(1,0,0),c(0,1,0),c(-2,1,1)) translate ## [,1] [,2] [,3] ## [1,] 1 0 -2 ## [2,] 0 1 1 ## [3,] 0 0 1 t = 2*pi/3 rot = cbind(c(cos(t),sin(t),0),c(-sin(t),cos(t),0),c(0,0,1)) rot ## [,1] [,2] [,3] ## [1,] -0.5000000 -0.8660254 0 ## [2,] 0.8660254 -0.5000000 0 ## [3,] 0.0000000 0.0000000 1 A = rot %*% translate A # display A ## [,1] [,2] [,3] ## [1,] -0.5000000 -0.8660254 0.1339746 ## [2,] 0.8660254 -0.5000000 -2.2320508 ## [3,] 0.0000000 0.0000000 1.0000000 house3 = A %*% house plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-6,6),ylim=c(-6,6),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) polygon(house3[1,], house3[2,], col = &quot;green&quot;, border = &quot;black&quot;) 19.3 Rotation and then Translation Let’s reverse the order of these matrices and see that we get a different transformation. ## [,1] [,2] [,3] ## [1,] -0.5000000 -0.8660254 -2 ## [2,] 0.8660254 -0.5000000 1 ## [3,] 0.0000000 0.0000000 1 Indeed, these two transformations are different! So the order matters. 19.4 Your Turn Here are a couple of plots that you should try to reproduce using homogeneous coordinates. 19.4.1 House of Orange Here is a picture of a gray house and a larger, upside-down orange house. Work as a group to reproduce this image using homogeneous coordinates. You will have to use a combination of translation, rotation, and expansion. You will do this by multiplying three matrices. Think carefully and experiment. Remember that the order of your matrices matters, and the rightmost one happens first. ############################# # your code defining the 3x3 matrices A1 and A2 A1 = cbind(c(1,0,0), c(0,1,0), c(0,0,1)) A2 = cbind(c(1,0,0), c(0,1,0), c(0,0,1)) A3 = cbind(c(1,0,0), c(0,1,0), c(0,0,1)) A = A3 %*% A2 %*% A1 ############################# # you do not need to change this code plot(house[1,],house[2,],type=&quot;n&quot;,xlim=c(-6,6),ylim=c(-6,6),xlab=&quot;x&quot;,ylab=&quot;y&quot;) house2 = A %*% house abline(h=-6:6, v=-6:6, col=&quot;gray&quot;, lty=&quot;dotted&quot;) polygon(house2[1,], house2[2,], col = &quot;orange&quot;, border = &quot;green&quot;) polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) 19.4.2 House Party Here is a plot of the grey house and four other houses, colored cyan, red, gold and green. Reproduce this image using homogeneous coordinates. Work as a group! You can collaborate, or divide and conquer. Be ready to help one another out! ############# # your code for 3x3 matrices that create the transformed houses goes here A.red = cbind(c(1,0,0), c(0,1,0), c(0,0,1)) A.purple = cbind(c(1,0,0), c(0,1,0), c(0,0,1)) A.gold = cbind(c(1,0,0), c(0,1,0), c(0,0,1)) A.cyan = cbind(c(1,0,0), c(0,1,0), c(0,0,1)) #################### # you do not need to change this code house = cbind(c(0,0,1), c(0,3/4,1), c(2/4,3/4,1), c(2/4,0,1), c(4/4,0,1), c(4/4,4/4,1), c(5/4,4/4,1), c(0,8/4,1), c(-5/4,4/4,1), c(-4/4,4/4,1), c(-4/4,0,1), c(0,0,1)); plot(house[1,], house[2,], type = &quot;n&quot;, xlim=c(-2.5,2.5),ylim=c(-2.0,3.0),,xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-4:4, v=-4:4, col=&quot;gray&quot;, lty=&quot;dotted&quot;) house.gold = A.gold %*% house polygon(house.gold[1,], house.gold[2,], col = &quot;gold&quot;, border = &quot;blue&quot;) house.cyan = A.cyan %*% house polygon(house.cyan[1,], house.cyan[2,], col = &quot;cyan&quot;, border = &quot;blue&quot;) house.red = A.red %*% house polygon(house.red[1,], house.red[2,], col = &quot;red&quot;, border = &quot;blue&quot;) house.purple= A.purple %*% house polygon(house.purple[1,], house.purple[2,], col = &quot;purple&quot;, border = &quot;blue&quot;) polygon(house[1,], house[2,], col = &quot;gray&quot;, border = &quot;blue&quot;) 19.4.3 test checkin "],["fractals.html", "Vector 20 Fractals 20.1 Transforming My House. 20.2 Watching a series of points. 20.3 Here are some from http://paulbourke.net/fractals/ifs/", " Vector 20 Fractals You’ve probably heard of fractals: mathematical sets with a self-similar structure. Each smaller part is a miniature copy of the whole structure. Today, we will see how to use linear transformations and homogeneous coordiantes to generate a fractal. We will generate a fractal by repeatedly applying the following 3D linear transformations. We pick one of these four transformations using some randomness. Most of the time we apply matrix A1. Occasionally, we apply matrix A2 or A3. And rarely we apply A4. So let’s generate a fractal! 20.1 Transforming My House. So what is happening? Let’s look at the effect of each of these linear transformations on my house ## [,1] [,2] [,3] ## [1,] 0.20 -0.25 0.0 ## [2,] 0.21 0.23 1.5 ## [3,] 0.00 0.00 1.0 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] 0.00 0.046875 0.066875 0.02000 0.0400 0.1025 0.112500 ## [2,] 2.25 2.289675 2.311725 2.27205 2.2941 2.3470 2.358025 ## [3,] 1.00 1.000000 1.000000 1.00000 1.0000 1.0000 1.000000 ## [,8] [,9] [,10] [,11] [,12] ## [1,] 0.1250 0.012500 0.0225 -0.0400 0.00 ## [2,] 2.3558 2.247775 2.2588 2.2059 2.25 ## [3,] 1.0000 1.000000 1.0000 1.0000 1.00 20.2 Watching a series of points. Let’s see what happens when we repeatedly apply the same mapping. ## Your Turn Make some small adjustments to one of these matrices. Explore the impact of the fractal. Can you make it spikier? Bushier? 20.3 Here are some from http://paulbourke.net/fractals/ifs/ a 0.2020 0.1380 b -0.8050 0.6650 c -0.6890 -0.5020 d -0.3420 -0.2220 e -0.3730 0.6600 f -0.6530 -0.2770 ## [,1] [,2] ## [1,] 0.824074 0.088272 ## [2,] 0.281428 0.520988 ## [3,] -0.212346 -0.463889 ## [4,] 0.864198 -0.377778 ## [5,] -1.882290 0.785360 ## [6,] -0.110607 8.095795 ## [[1]] ## [,1] [,2] [,3] ## [1,] 0.787879 -0.424242 1.758647 ## [2,] 0.242424 0.859848 1.408065 ## [3,] 0.000000 0.000000 1.000000 ## ## [[2]] ## [,1] [,2] [,3] ## [1,] -0.121212 0.257576 -6.721654 ## [2,] 0.151515 0.053030 1.377236 ## [3,] 0.000000 0.000000 1.000000 ## ## [[3]] ## [,1] [,2] [,3] ## [1,] 0.181818 -0.136364 6.086107 ## [2,] 0.090909 0.181818 1.568035 ## [3,] 0.000000 0.000000 1.000000 "],["dynamical-systems-in-2d.html", "Vector 21 Dynamical Systems in 2D 21.1 Helper Function to Plot Dynamical Systems 21.2 Our first example 21.3 Your Turn", " Vector 21 Dynamical Systems in 2D Download this Rmd file Let \\(A\\) be a square \\(n \\times n\\) matrix and let \\(\\mathsf{x}_0 \\in \\mathbb{R}^n\\). A dynamical system is a sequence of vectors \\(\\mathsf{x}_0,\\mathsf{x}_1,\\mathsf{x}_2, \\ldots, \\mathsf{x}_t, \\ldots\\) where \\[ \\mathsf{x}_{t} = A \\mathsf{x}_{t-1} = A^t \\mathsf{x}_0 \\quad \\mbox{for} \\quad t \\geq 1. \\] The sequence \\(\\mathsf{x}_0,\\mathsf{x}_1,\\mathsf{x}_2, \\ldots, \\mathsf{x}_t, \\ldots\\) is called the trajectory for initial vector \\(\\mathsf{x}_0\\). A dynamical system evolves over time. The long-term behavior is governed by the eigenvalues of matrix \\(A\\) We will look at some \\(2 \\times 2\\) dynamical systems to develop some intuition about eigensystems. 21.1 Helper Function to Plot Dynamical Systems I have written some code that makes some helpful plots. You should run this code chunk before the others. get_traj &lt;- function(mat, x0, num) { traj = cbind(x0) num for (i in 1:num) { traj = cbind(traj, mat %*% traj[,dim(traj)[2]]) traj } return(traj) } plot_traj &lt;- function(mat, x0, num) { traj = get_traj(mat,x0,num) points(traj[1,],traj[2,], pch=20, col=rainbow(length(traj))) } trajectory_plot &lt;- function(mat, t=20, datamax=5, plotmax=10, numpoints=10, showEigenspaces=TRUE) { # initialize plot par(pty = &quot;s&quot;) plot(c(0),c(0),type=&quot;n&quot;, xlim=c(-plotmax,plotmax),ylim=c(-plotmax,plotmax), xlab=&#39;x&#39;, ylab=&#39;y&#39;) abline(h=-plotmax:plotmax, v=-plotmax:plotmax, col=&quot;gray&quot;) mygrid &lt;- expand.grid(x=seq(from = -datamax, by = 2*datamax/numpoints, l = numpoints+1), y=seq(from = -datamax, by = 2*datamax/numpoints, l = numpoints+1)) for (t in 1:dim(mygrid)[1]) { plot_traj(A,c(mygrid[t,1],mygrid[t,2]),t) } if (showEigenspaces) { eigen = eigen(A) #mylabel = cat(&#39;lambda=&#39;, eigen$values[1], &#39;and lambda=&#39;, eigen$values[2]) #title(xlab=mylabel) v1 = zapsmall(eigen$vectors[,1]) v2 = zapsmall(eigen$vectors[,2]) if (! class(v1[1]) == &quot;complex&quot;) { if (v1[1] == 0) { abline(v=0) } else { abline(a=0,b=v1[2]/v1[1], col=&quot;blue&quot;) } if (v2[1] == 0) { abline(v=0) } else { abline(a=0,b=v2[2]/v2[1], col=&quot;blue&quot;) } } } } 21.2 Our first example Let’s start by looking at the example from the video \\[ A = \\frac{1}{30} \\begin{bmatrix} 31 &amp; 4 \\\\ 2 &amp; 29 \\end{bmatrix}. \\] Here is some code that creates a single trajectory for \\(0 \\leq t \\leq 30\\) starting at \\(\\mathsf{x}_0 = \\begin{bmatrix} -0.5 &amp; 1 \\end{bmatrix}.\\) The colors of the points follow the rainbow ordering at \\(t\\) increases. A = 1/30 * cbind(c(31,2),c(4,29)) x0 = c(-0.5,1) # initialize the plot plot(c(0),c(0),type=&quot;n&quot;,xlim=c(-5,5),ylim=c(-5,5),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-5:5, v=-5:5, col=&quot;gray&quot;) plot_traj(A,x0,30) We get a more complete picture when we plot multiple trajectories at once. So I have written a helper function to plot the trajectories of a grid of points. It also plots the eigenspaces for the matrix. You can specify the matrix A the number of iterations the size of the square where the initial points lie the size of the plot the number of points along the side of the grid A = 1/30 * cbind(c(31,2),c(4,29)) trajectory_plot(A, t=30, datamax=5, plotmax=15, numpoints=5) Perhaps this system is best understood by comparing what we see with the eigenvector and eigenvalues. eigen(A) ## eigen() decomposition ## $values ## [1] 1.1 0.9 ## ## $vectors ## [,1] [,2] ## [1,] 0.8944272 -0.7071068 ## [2,] 0.4472136 0.7071068 We can see that we have slight expansion along \\([ 2, 1]^{\\top}\\) and slight contraction along \\([-1,1]\\). The long term behavior is an expansion in the direction of \\([2, 1]^{\\top}\\). 21.3 Your Turn Now it’s your turn to explore some dynamical systems. Create trajectory plots for each of these dynamical systems. Characterize the long-term behavior. What direction to vectors converge to? Do magnitudes increase? decrease? stabilize? Calculate the eigenvectors and eigenvalues and compare them to your plot. The eigensystem should tell the same story as your plot. If your original plot is confusing, try changing the parameters (initial square size, plot size, number of grid points). Here is some code for you to adapt for the examples. &#39;```{r, echo=TRUE} A = cbind(c(1,0),c(0,1)) trajectory_plot(A, t=30, datamax=5, plotmax=10, numpoints=10) eigen(A) 21.3.1 Example 1 \\[ A = \\frac{1}{60} \\begin{bmatrix} 55&amp; -8 \\\\ -1 &amp; 53 \\end{bmatrix} \\] 21.3.2 Example 2 \\[ A = \\frac{1}{20} \\begin{bmatrix} 24&amp; -6 \\\\ 1 &amp; 19 \\end{bmatrix} \\] 21.3.3 Example 3 \\[ A = \\frac{1}{110} \\begin{bmatrix} 106&amp; 12 \\\\ 6 &amp; 92 \\end{bmatrix} \\] 21.3.4 Example 4 \\[ A = \\frac{1}{16} \\begin{bmatrix} 17&amp; -15 \\\\ 15 &amp; 17 \\end{bmatrix} \\] "],["network-centralities.html", "Vector 22 Network Centralities 22.1 Graphs and Networks 22.2 Degree Centrality 22.3 Gould’s Index 22.4 Your Turn: The Rise of Moscow", " Vector 22 Network Centralities Download this Rmd file In this example, we will use a package called igraph. To install it, you need to go to the packages window (bottom right), choose install, and search for and install igraph from the packages window. library(igraph) The igraph R package isn’t all that well documented. Here are some places to look for documentation if you want to learn about other features. Let me know if you find any other good references: http://kateto.net/netscix2016 http://igraph.org/r/doc/aaa-igraph- 22.1 Graphs and Networks Graphs consists of vertices and the edges between them. These edges are used to model connections in a wide array of applications, including but not limited to, physical, biological, social, and information networks. To emphasize the application to real-world systems, the term Network Science is sometimes used. So we will use the terms graph and network interchangeably. In this application question, we will see that linear algebra is an important tool in the study of graphs. 22.1.1 Adjacency Matrices Matrices are used to represent graphs and networks in a very direct way: we place a 1 in position \\((i,j)\\) of the adjacency matrix \\(A\\) of the graph \\(G\\), if there is an edge from vertex \\(i\\) to vertex \\(j\\) in \\(G\\). Here is the adjacency matrix we will use today. As you can see it is a \\(12 \\times 12\\) matrix. A = rbind( c(0,1,0,1,0,0,0,0,1,0,0,0), c(1,0,1,1,1,0,1,0,0,0,0,0), c(0,1,0,0,1,0,0,0,0,0,0,0), c(1,1,0,0,0,1,0,1,0,0,0,0), c(0,1,1,0,0,0,1,1,0,0,0,1), c(0,0,0,1,0,0,1,0,0,0,0,0), c(0,1,0,0,1,1,0,1,0,0,0,0), c(0,0,0,1,1,0,1,0,0,1,1,0), c(1,0,0,0,0,0,0,0,0,0,0,0), c(0,0,0,0,0,0,0,1,0,0,0,0), c(0,0,0,0,0,0,0,1,0,0,0,0), c(0,0,0,0,1,0,0,0,0,0,0,0)) A ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] ## [1,] 0 1 0 1 0 0 0 0 1 0 0 ## [2,] 1 0 1 1 1 0 1 0 0 0 0 ## [3,] 0 1 0 0 1 0 0 0 0 0 0 ## [4,] 1 1 0 0 0 1 0 1 0 0 0 ## [5,] 0 1 1 0 0 0 1 1 0 0 0 ## [6,] 0 0 0 1 0 0 1 0 0 0 0 ## [7,] 0 1 0 0 1 1 0 1 0 0 0 ## [8,] 0 0 0 1 1 0 1 0 0 1 1 ## [9,] 1 0 0 0 0 0 0 0 0 0 0 ## [10,] 0 0 0 0 0 0 0 1 0 0 0 ## [11,] 0 0 0 0 0 0 0 1 0 0 0 ## [12,] 0 0 0 0 1 0 0 0 0 0 0 ## [,12] ## [1,] 0 ## [2,] 0 ## [3,] 0 ## [4,] 0 ## [5,] 1 ## [6,] 0 ## [7,] 0 ## [8,] 0 ## [9,] 0 ## [10,] 0 ## [11,] 0 ## [12,] 0 dim(A) ## [1] 12 12 Here is how to make the graph from your adjacency matrix g=graph_from_adjacency_matrix(A,mode=&#39;undirected&#39;) plot(g, vertex.color=&#39;tan1&#39;, vertex.frame.color=&quot;dodgerblue&quot;) Observe that there is an edge from vertex \\(i\\) to vertex \\(j\\) if and only if there is a 1 in position \\((i,j)\\) in the matrix. Let’s assume that this network is the route map of a small airline. Let’s add vertex labels and change the vertex size: airports = c(&quot;ATL&quot;,&quot;LAX&quot;,&quot;ORD&quot;,&quot;MSP&quot;,&quot;DEN&quot;,&quot;JFK&quot;,&quot;SFO&quot;,&quot;SEA&quot;,&quot;PHL&quot;,&quot;PDX&quot;,&quot;MDW&quot;,&quot;LGA&quot;) V(g)$label = airports plot(g,vertex.size=30, vertex.color=&#39;tan1&#39;, vertex.frame.color=&quot;dodgerblue&quot;) 22.1.2 Graph Layouts There are a variety of graph layout algorithms which place the vertices in the plane. You can find many algorithms in the igraph documentation. For example, here is a layout on a circle coords = layout_in_circle(g) plot(g, layout=coords, vertex.size = 30,vertex.label.cex=0.65, vertex.color=&#39;tan1&#39;, vertex.frame.color=&quot;dodgerblue&quot;) The Fruchterman-Reingold algorithm is one of the most popular graph vertex layout algorithms. It is a force-directed layout that tries to get a nice-looking graph where edges are similar in length and cross each other as little as possible. The algorithm simulates the graph as a physical system. Vertices are electrically charged particles that repulse each other when they get too close. The edges act as springs that attract connected vertices closer together. As a result, vertices are evenly distributed through the chart area. The resulting layout is intuitive: vertices which share more connections are closer to each other. coords = layout_with_fr(g) plot(g, layout=coords, vertex.size = 30, vertex.label.cex=0.65, vertex.color=&#39;tan1&#39;, vertex.frame.color=&quot;dodgerblue&quot;) We can also choose to layout vertices by hand: locations = rbind( c(20,0),c(-10,0),c(11,7),c(10,15),c(3,12),c(25,10), c(-10,10),c(-12,15),c(20,6),c(-15,12),c(12,4),c(25,13) ) plot(g,vertex.size=20, layout=locations, vertex.label.cex=0.65, vertex.color=&#39;tan1&#39;, vertex.frame.color=&quot;dodgerblue&quot;) 22.2 Degree Centrality If we are considering placing an office in one of our airport locations, we may want to chose the most central hub for that office. It turns out that there are many interesting centrality measures for networks. We will talk about two of them today. The simplest measure centrality is the degree of the vertex, or the number of edges connected to that vertex. We calculate the degree centralities from the adjacency matrix as follows: First make a vector \\(\\mathsf{v}\\) of all 1’s; then multiply \\(\\mathsf{d} = A\\mathsf{v}\\) to get the degree proportions; and we also divide vector \\(\\mathsf{d}\\) by the sum of its entries. The result is a normalized vector \\(\\mathsf{u}\\) whose entries sum to 1. Each entry of vector \\(\\mathsf{u}\\) represents to proportion of edges incident with the corresponding vertex. v=rep(1,nrow(A)) # all 1s vector d = A %*% v # degrees u=d/sum(d) # proportion of degrees cbind(d,u) # show d and u together side-by-side in a matrix ## [,1] [,2] ## [1,] 3 0.08823529 ## [2,] 5 0.14705882 ## [3,] 2 0.05882353 ## [4,] 4 0.11764706 ## [5,] 5 0.14705882 ## [6,] 2 0.05882353 ## [7,] 4 0.11764706 ## [8,] 5 0.14705882 ## [9,] 1 0.02941176 ## [10,] 1 0.02941176 ## [11,] 1 0.02941176 ## [12,] 1 0.02941176 Now let’s create a data visualization. We plot the network and size each vertex according to the vector \\(u\\). The larger vertices have more edges connected to them. This conveys information to the viewer about the relative importance of the vertices. plot(g, layout=locations, vertex.size=250*u,vertex.label.cex=0.65, vertex.color=&#39;tan1&#39;, vertex.frame.color=&quot;dodgerblue&quot;) We can also sort the vertices by degree. To include the vertex names, we do it in a data frame. df = data.frame(d) # Convert vector to a data frame so that we can add row names rownames(df)=airports ii=order(d,decreasing=TRUE) df2 = data.frame(df[ii,]) rownames(df2) = airports[ii] df2 ## df.ii... ## LAX 5 ## DEN 5 ## SEA 5 ## MSP 4 ## SFO 4 ## ATL 3 ## ORD 2 ## JFK 2 ## PHL 1 ## PDX 1 ## MDW 1 ## LGA 1 22.3 Gould’s Index Gould’s Index is a measure of centrality that uses the dominant eigenvector of a matrix. It was introduced by geographer P. R. Gould in 1967 to analyze the geographical features on maps. We will build up Gould’s Index step-by-step so that we can understand what it measures. 22.3.1 Step 1 The first step is typically to add the identity matrix to the adjancency matrix \\(A\\) to get a new matrix \\[B = A + I.\\] The \\(n \\times n\\) identity matrix in R is obtained by using diag(n). Adding the identity gives a connection from a vertex to itself. This loop edge corresponds to staying at the current city during a layover. (B = A + diag(nrow(A))) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] ## [1,] 1 1 0 1 0 0 0 0 1 0 0 ## [2,] 1 1 1 1 1 0 1 0 0 0 0 ## [3,] 0 1 1 0 1 0 0 0 0 0 0 ## [4,] 1 1 0 1 0 1 0 1 0 0 0 ## [5,] 0 1 1 0 1 0 1 1 0 0 0 ## [6,] 0 0 0 1 0 1 1 0 0 0 0 ## [7,] 0 1 0 0 1 1 1 1 0 0 0 ## [8,] 0 0 0 1 1 0 1 1 0 1 1 ## [9,] 1 0 0 0 0 0 0 0 1 0 0 ## [10,] 0 0 0 0 0 0 0 1 0 1 0 ## [11,] 0 0 0 0 0 0 0 1 0 0 1 ## [12,] 0 0 0 0 1 0 0 0 0 0 0 ## [,12] ## [1,] 0 ## [2,] 0 ## [3,] 0 ## [4,] 0 ## [5,] 1 ## [6,] 0 ## [7,] 0 ## [8,] 0 ## [9,] 0 ## [10,] 0 ## [11,] 0 ## [12,] 1 Here is what the corresponding network (with layovers) looks like. You can see why we refer to these additional edges as “loops.” However, we usually do not draw the network with these added loops to keep the figure less cluttered. g2=graph_from_adjacency_matrix(B,mode=&#39;undirected&#39;) airports = c(&quot;ATL&quot;,&quot;LAX&quot;,&quot;ORD&quot;,&quot;MSP&quot;,&quot;DEN&quot;,&quot;JFK&quot;,&quot;SFO&quot;,&quot;SEA&quot;,&quot;PHL&quot;,&quot;PDX&quot;,&quot;MDW&quot;,&quot;LGA&quot;) V(g2)$label = airports coords = layout_with_fr(g2) plot(g2, layout=coords, vertex.color=&#39;tan1&#39;, vertex.frame.color=&quot;dodgerblue&quot;) 22.3.2 Step 2 Start with the all 1’s vector \\(\\mathsf{v}_0 = [1, 1, \\ldots ,1 ]^{\\top}\\). We iterate the mapping \\(T(\\mathsf{x}) = B\\mathsf{x}\\) for a few steps to get \\(\\mathsf{v}_1 = B \\mathsf{v}_0\\) and \\(\\mathsf{v}_2 = B \\mathsf{v}_1 = B^2 \\mathsf{v}_0\\) and \\(\\mathsf{v}_3 = B \\mathsf{v}_2 = B^3 \\mathsf{v}_0\\). v0 = rep(1,nrow(B)) v1 = B %*% v0 v2 = B %*% v1 v3 = B %*% v2 df = data.frame(cbind(v0,v1,v2,v3)) #Convert vector to a data frame in order to add row names rownames(df)=airports colnames(df)=c(0,1,2,3) df ## 0 1 2 3 ## ATL 1 4 17 76 ## LAX 1 6 29 139 ## ORD 1 3 15 72 ## MSP 1 5 24 109 ## DEN 1 6 28 132 ## JFK 1 3 13 63 ## SFO 1 5 26 122 ## SEA 1 6 26 120 ## PHL 1 2 6 23 ## PDX 1 2 8 34 ## MDW 1 2 8 34 ## LGA 1 2 8 36 22.3.2.1 Your Turn Discuss with your group: Each of the entries of vector \\(\\mathsf{v}_{t}\\) corresponds to “a trip of length \\(t\\).” What kinds of trips does the \\(i\\)th entry of \\(\\mathsf{v}_{t}\\) count? Here is how you can figure this out: Compare the table of vectors with the picture of the network with layovers. Start by looking at the \\(t=1\\) column. The \\(i\\)th entry has something to do with the \\(i\\)th city. Next, look at the \\(t=2\\) column. And so on. Once you have noticed the connection between the network and data, explain why the rule \\(\\mathsf{v}_t = A \\mathsf{v}_{t-1}\\) leads to this result. 22.3.3 Step 3 We can calculate \\(\\mathsf{v}_1, \\ldots, \\mathsf{v}_{10}\\) using a loop: N = 10 X = matrix(0,nrow=nrow(B),ncol=N+1) X[,1] = rep(1,nrow(B)) for (i in 2:(N+1)) { X[,i] = B %*% X[,i-1] } X ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 1 4 17 76 347 1603 7442 34638 161411 752642 ## [2,] 1 6 29 139 650 3044 14211 66352 309652 1445058 ## [3,] 1 3 15 72 343 1614 7567 35389 165336 771972 ## [4,] 1 5 24 109 507 2349 10936 50930 237450 1107376 ## [5,] 1 6 28 132 621 2909 13611 63595 296984 1386347 ## [6,] 1 3 13 63 294 1377 6418 29939 139617 651292 ## [7,] 1 5 26 122 576 2692 12585 58748 274225 1279724 ## [8,] 1 6 26 120 551 2563 11923 55591 259246 1209469 ## [9,] 1 2 6 23 99 446 2049 9491 44129 205540 ## [10,] 1 2 8 34 154 705 3268 15191 70782 330028 ## [11,] 1 2 8 34 154 705 3268 15191 70782 330028 ## [12,] 1 2 8 36 168 789 3698 17309 80904 377888 ## [,11] ## [1,] 3510616 ## [2,] 6743119 ## [3,] 3603377 ## [4,] 5165837 ## [5,] 6470458 ## [6,] 3038392 ## [7,] 5971890 ## [8,] 5642972 ## [9,] 958182 ## [10,] 1539497 ## [11,] 1539497 ## [12,] 1764235 df = data.frame(X) rownames(df)=airports colnames(df)=c(0:10) df ## 0 1 2 3 4 5 6 7 8 9 10 ## ATL 1 4 17 76 347 1603 7442 34638 161411 752642 3510616 ## LAX 1 6 29 139 650 3044 14211 66352 309652 1445058 6743119 ## ORD 1 3 15 72 343 1614 7567 35389 165336 771972 3603377 ## MSP 1 5 24 109 507 2349 10936 50930 237450 1107376 5165837 ## DEN 1 6 28 132 621 2909 13611 63595 296984 1386347 6470458 ## JFK 1 3 13 63 294 1377 6418 29939 139617 651292 3038392 ## SFO 1 5 26 122 576 2692 12585 58748 274225 1279724 5971890 ## SEA 1 6 26 120 551 2563 11923 55591 259246 1209469 5642972 ## PHL 1 2 6 23 99 446 2049 9491 44129 205540 958182 ## PDX 1 2 8 34 154 705 3268 15191 70782 330028 1539497 ## MDW 1 2 8 34 154 705 3268 15191 70782 330028 1539497 ## LGA 1 2 8 36 168 789 3698 17309 80904 377888 1764235 Wow, these numbers get big fast! Let’s normalize by dividing by the sum each time. N = 10 X = matrix(0,nrow=nrow(B),ncol=N+1) X[,1] = rep(1,nrow(B)) for (i in 2:(N+1)) { X[,i] = B %*% X[,i-1] X[,i] = X[,i]/sum(X[,i]) } df = data.frame(X) rownames(df)=airports colnames(df)=c(0:10) df ## 0 1 2 3 4 5 ## ATL 1 0.08695652 0.08173077 0.07916667 0.07773297 0.07708213 ## LAX 1 0.13043478 0.13942308 0.14479167 0.14560932 0.14637430 ## ORD 1 0.06521739 0.07211538 0.07500000 0.07683692 0.07761108 ## MSP 1 0.10869565 0.11538462 0.11354167 0.11357527 0.11295441 ## DEN 1 0.13043478 0.13461538 0.13750000 0.13911290 0.13988267 ## JFK 1 0.06521739 0.06250000 0.06562500 0.06586022 0.06621466 ## SFO 1 0.10869565 0.12500000 0.12708333 0.12903226 0.12944797 ## SEA 1 0.13043478 0.12500000 0.12500000 0.12343190 0.12324485 ## PHL 1 0.04347826 0.02884615 0.02395833 0.02217742 0.02144643 ## PDX 1 0.04347826 0.03846154 0.03541667 0.03449821 0.03390075 ## MDW 1 0.04347826 0.03846154 0.03541667 0.03449821 0.03390075 ## LGA 1 0.04347826 0.03846154 0.03750000 0.03763441 0.03793999 ## 6 7 8 9 10 ## ATL 0.07674064 0.07657108 0.07647933 0.07643081 0.07640399 ## LAX 0.14654141 0.14667834 0.14671848 0.14674567 0.14675521 ## ORD 0.07802962 0.07823125 0.07833906 0.07839377 0.07842281 ## MSP 0.11277017 0.11258632 0.11250792 0.11245405 0.11242772 ## DEN 0.14035431 0.14058369 0.14071617 0.14078356 0.14082110 ## JFK 0.06618132 0.06618343 0.06615295 0.06613871 0.06612665 ## SFO 0.12977438 0.12986887 0.12993256 0.12995600 0.12997042 ## SEA 0.12294795 0.12288997 0.12283525 0.12282160 0.12281194 ## PHL 0.02112894 0.02098089 0.02090908 0.02087259 0.02085358 ## PDX 0.03369906 0.03358136 0.03353774 0.03351435 0.03350515 ## MDW 0.03369906 0.03358136 0.03353774 0.03351435 0.03350515 ## LGA 0.03813315 0.03826343 0.03833372 0.03837453 0.03839628 22.3.3.1 Your Turn Discuss with your group: What do we learn from this table of normalized vectors? Interpret the vector for \\(t=10\\). Since we have normalized our vector, each entry is a proportion, rather than a raw count. Look at the entries as \\(t\\) increases. What do you observe? Iterative multiplication \\(\\mathsf{v}_t = A \\mathsf{v}_{t-1}\\) is a dynamical system! We talked about dynamical systems last week. There is a relationship between the eigenvectors of \\(A\\) and the “long term behavior” of the dynamical system. Share what you remember about this topic with your group. 22.3.4 Step 4 You have observed that the vectors are converging to a common direction. We get this final direction without saving all of the values along the way. Here we iterate 1000 steps and save the final value. This limiting vector, whose entries are scaled to sum to one, is called Gould’s Index. N = 1000 w = rep(1,nrow(B)) for (i in 2:(N+1)) { w = B %*% w w = w/sum(w) } df = data.frame(w) rownames(df)=airports colnames(df)=c(&quot;Gould Index&quot;) df ## Gould Index ## ATL 0.07637062 ## LAX 0.14676474 ## ORD 0.07845446 ## MSP 0.11239329 ## DEN 0.14086410 ## JFK 0.06611172 ## SFO 0.12998472 ## SEA 0.12280792 ## PHL 0.02083107 ## PDX 0.03349744 ## MDW 0.03349744 ## LGA 0.03842249 We saw last week that this direction must be the dominant eigenvector of \\(B\\). This dominant eigenvector is the directon that corresponds to the eigenvalue of largest magnitude. We can confirm that this is the dominant eigenvector as follows. First compute the eigenvectors. eigen(B) ## eigen() decomposition ## $values ## [1] 4.66618847 2.64207538 2.41909839 1.80037113 1.27260439 ## [6] 1.00000000 1.00000000 0.49835918 0.02718633 -0.67732874 ## [11] -0.92557189 -1.72298263 ## ## $vectors ## [,1] [,2] [,3] [,4] ## [1,] -0.23401334 0.57249329 -0.03677166 0.289569576 ## [2,] -0.44971357 0.23508235 0.28327135 -0.009053428 ## [3,] -0.24039858 -0.04730698 0.44304618 0.102332378 ## [4,] -0.34439328 0.35635470 -0.30954196 -0.120977575 ## [5,] -0.43163295 -0.31276398 0.34545477 0.090957309 ## [6,] -0.20257820 0.10572025 -0.24645181 -0.612347635 ## [7,] -0.39829657 -0.18275409 -0.04019741 -0.369127791 ## [8,] -0.37630557 -0.32813461 -0.43931838 0.235004528 ## [9,] -0.06383014 0.34864008 -0.02591199 0.361794131 ## [10,] -0.10264218 -0.19982920 -0.30957570 0.293619448 ## [11,] -0.10264218 -0.19982920 -0.30957570 0.293619448 ## [12,] -0.11773343 -0.19046871 0.24343257 0.113643916 ## [,5] [,6] [,7] [,8] ## [1,] -0.11347970 0.000000e+00 0.000000e+00 0.17344569 ## [2,] 0.29280642 2.626816e-16 1.203957e-16 0.27532162 ## [3,] 0.42192015 -1.576610e-16 -4.434216e-17 -0.59911594 ## [4,] 0.09253829 4.851644e-01 1.208947e-01 -0.01657232 ## [5,] -0.17778914 1.198948e-16 1.395385e-16 0.02521939 ## [6,] -0.18646435 -5.005332e-18 -9.291441e-17 -0.52296040 ## [7,] -0.14336929 -4.851644e-01 -1.208947e-01 0.27891061 ## [8,] 0.03236396 -1.021498e-16 -3.393381e-17 0.08250644 ## [9,] -0.41627977 -4.851644e-01 -1.208947e-01 -0.34575674 ## [10,] 0.11872135 -1.709709e-01 6.861261e-01 -0.16447314 ## [11,] 0.11872135 1.709709e-01 -6.861261e-01 -0.16447314 ## [12,] -0.65218735 4.851644e-01 1.208947e-01 -0.05027381 ## [,9] [,10] [,11] [,12] ## [1,] 0.19570619 6.653425e-01 0.08742351 0.026476499 ## [2,] 0.33415547 -4.571765e-01 -0.14917762 0.397124122 ## [3,] -0.08232149 1.379670e-01 0.40635541 -0.072909426 ## [4,] -0.32336571 -2.621537e-01 0.02623868 -0.459495824 ## [5,] -0.25407199 2.257605e-01 -0.63328894 -0.198593023 ## [6,] 0.18953651 1.563407e-01 -0.22162883 0.307139725 ## [7,] 0.13898200 -8.104161e-05 0.40052356 -0.376840313 ## [8,] -0.40482358 7.521122e-02 0.23285847 0.520458800 ## [9,] -0.20117541 -3.966679e-01 -0.04540132 -0.009723345 ## [10,] 0.41613681 -4.483988e-02 -0.12092951 -0.191135557 ## [11,] 0.41613681 -4.483988e-02 -0.12092951 -0.191135557 ## [12,] 0.26117231 -1.345953e-01 0.32888356 0.072932167 Now, we will extract the dominant eigenvector \\(\\mathsf{v}_1\\) rescale this vector to sum to 1, and display it next to the vector \\(w\\) we got above by iteration. vecs = eigen(B)$vectors gould = vecs[,1] gould = gould/sum(gould) cbind(w,gould) ## gould ## [1,] 0.07637062 0.07637062 ## [2,] 0.14676474 0.14676474 ## [3,] 0.07845446 0.07845446 ## [4,] 0.11239329 0.11239329 ## [5,] 0.14086410 0.14086410 ## [6,] 0.06611172 0.06611172 ## [7,] 0.12998472 0.12998472 ## [8,] 0.12280792 0.12280792 ## [9,] 0.02083107 0.02083107 ## [10,] 0.03349744 0.03349744 ## [11,] 0.03349744 0.03349744 ## [12,] 0.03842249 0.03842249 22.3.5 Step 5 Now let’s plot the network with: the vertices sized by Gould’s Index the labels sized by degree centrality plot(g, layout=locations, vertex.size=250*gould,vertex.label.cex=7*u, vertex.color=&#39;tan1&#39;, vertex.frame.color=&quot;dodgerblue&quot; ) And we can create a data frame containing both the Gould’s Index and the Degree Centrality. We order the data using the Gould Index and then compare the two. df = data.frame(gould, u) #Convert vector to a data frame in order to add row names rownames(df)=airports colnames(df)=c(&#39;Gould&#39;, &#39;Degree&#39;) ii=order(gould,decreasing=TRUE) df2 = data.frame(df[ii,]) rownames(df2) = airports[ii] df2 ## Gould Degree ## LAX 0.14676474 0.14705882 ## DEN 0.14086410 0.14705882 ## SFO 0.12998472 0.11764706 ## SEA 0.12280792 0.14705882 ## MSP 0.11239329 0.11764706 ## ORD 0.07845446 0.05882353 ## ATL 0.07637062 0.08823529 ## JFK 0.06611172 0.05882353 ## LGA 0.03842249 0.02941176 ## PDX 0.03349744 0.02941176 ## MDW 0.03349744 0.02941176 ## PHL 0.02083107 0.02941176 22.3.5.1 Your Turn Discuss with your group: Degree centrality and Gould’s Index give different rankings. Look at the table and observe that: LAX, DEN and SEA have the same degree centrality. However LAX and DEN have higher Gould Index than SEA. SFO has lower degree centrality than SEA, but higher Gould centrality! So these two centralities give different rankings. Remind yourselves about the intuitive idea behind Gould’s Index. What does Gould Index measure? Why does the Gould Index value SFO more than SEA? Find another pair of cities where the rankings of degree centrality and Gould’s Index differ. Look at the plot of the network and explain why this is the case. 22.3.6 Gould Index Summary Now that we understand what Gould’s Index means, let’s summarize how to find the Gould Index values for an adjacency matrix \\(A\\). Create the matrix \\(B = A+I\\). Find the dominant eigenvector \\(\\mathbf{v}\\) of \\(B\\). Normalize the values of \\(\\mathbf{v}\\) so that the entries sum to 1. 22.4 Your Turn: The Rise of Moscow Russian historians often attribute the dominance and rise to power of Moscow to its strategic position on medieval trade routes (see Figure 1). Others argue that sociological and political factors aided Moscow’s rise to power, and thus Moscow did not rise to power strictly because of its strategic location on the trade routes. The figure below shows the major cities and trade routes of medieval Russia. Let’s use Gould’s Index to form a geographer’s opinion about this debate. Either: Moscow’s location was the primary reason for its rise to power, or Other forces must have come into play. Here is the adjacency matrix for this transportation network into an adjacency matrix and a plot of the network. RusCity = c(&quot;Novgorod&quot;, &quot;Vitebsk&quot;, &quot;Smolensk&quot;, &quot;Kiev&quot;, &quot;Chernikov&quot;, &quot;Novgorod Severskij&quot;, &quot;Kursk&quot;, &quot;Bryansk&quot;, &quot;Karachev&quot;, &quot;Kozelsk&quot;, &quot;Dorogobusch&quot;, &quot;Vyazma&quot;, &quot;A&quot;, &quot;Tver&quot;, &quot;Vishnij Totochek&quot;, &quot;Ksyatyn&quot;, &quot;Uglich&quot;, &quot;Yaroslavl&quot;, &quot;Rostov&quot;, &quot;B&quot;, &quot;C&quot;, &quot;Suzdal&quot;, &quot;Vladimir&quot;, &quot;Nizhnij Novgorod&quot;, &quot;Bolgar&quot;, &quot;Isad&#39;-Ryazan&quot;, &quot;Pronsk&quot;, &quot;Dubok&quot;, &quot;Elets&quot;, &quot;Mtsensk&quot;, &quot;Tula&quot;, &quot;Dedoslavl&quot;, &quot;Pereslavl&quot;, &quot;Kolomna&quot;, &quot;Moscow&quot;, &quot;Mozhaysk&quot;, &quot;Dmitrov&quot;, &quot;Volok Lamskij&quot;, &quot;Murom&quot;) A = rbind(c(0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0), c(0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0), c(1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0), c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)) g=graph_from_adjacency_matrix(A,mode=&#39;undirected&#39;) V(g)$label = RusCity # Plot network plot(g) 22.4.1 Find the Degree Centrality values. Create a vector containing the normalized Degree Centralities. See Section 22.2 for help. 22.4.2 Find the Gould Index values Create a vector containing the Gould Index values. See Section 22.3.6 for help. 22.4.3 Analyze the Data Plot the network where the size of the vertices is determined by Gould’s Index and the size of the label is determined by degree centrality. Create a data frame that contains Gould’s Index and Degree Centralities. The rows should be labeled with the city names and the columns should be named by the centrality measures. Sort according to Gould’s Index. Use Gould’s Index to decide whether Moscow’s dominance was solely due to its geographic location. Compare the Gould’s Index and Degree Centrality rankings and note any interesting findings. See Section 22.3.5 for help. "],["voting-patterns-in-the-us-senate.html", "Vector 23 Voting Patterns in the US Senate 23.1 The Data 23.2 The 88th Congressional Session (1964) 23.3 Using the Eigenbasis 23.4 Your Turn", " Vector 23 Voting Patterns in the US Senate Download this Rmd file In 2014, the Pew Research Center published a report about the increasing polarization of US politics. They wrote: Republicans and Democrats are more divided along ideological lines – and partisan antipathy is deeper and more extensive – than at any point in the last two decades. These trends manifest themselves in myriad ways, both in politics and in everyday life. Is this really true, or is this just hype? Let’s see what linear algebra can tell us about the evolution of voting patterns in the US Senate from 1964 to 2012. 23.1 The Data We will analyze datasets corresponding to US Senate votes during a 2-year Congressional Session. Our data sets are 12 years apart, and were chosen to coincide with US election years. Table 23.1: Congressional Sessions of the US Senate Session Years US Election # Floor Votes 84 1963-1965 Lyndon Johnson vs Richard Nixon 534 94 1975-1977 Jimmy Carter vs Gerald Ford 1311 100 1987-1989 George H. W. Bush vs Michael Dukakis 799 106 1999-2001 George W. Bush vs Al Gore 672 112 2011-2013 Barack Obama vs John McCain 486 Here is a list our our data files, which we will load from Github. The original data can be found at voteview.com. There are two files for each year. A csv file containing a matrix whose \\((i,j)\\) entry counts the number of times Senator \\(i\\) and Senator \\(j\\) cast the same vote on an issue. There are actually nine different possibilities, including Yea, Nay, Present, Abstention and “Not a member of the chamber when the vote was taken.” A csv file containing senator information: name, state and party affiliation. senate.1964.files = c(&#39;https://raw.github.com/mathbeveridge/math236_f20/main/data/Senate088matrix.csv&#39;, &#39;https://raw.github.com/mathbeveridge/math236_f20//main/data/Senate088senators.csv&#39;) senate.1976.files = c(&#39;https://raw.github.com/mathbeveridge/math236_f20/main/data/Senate094matrix.csv&#39;, &#39;https://raw.github.com/mathbeveridge/math236_f20//main/data/Senate094senators.csv&#39;) senate.1988.files = c(&#39;https://raw.github.com/mathbeveridge/math236_f20/main/data/Senate100matrix.csv&#39;, &#39;https://raw.github.com/mathbeveridge/math236_f20//main/data/Senate100senators.csv&#39;) senate.2000.files = c(&#39;https://raw.github.com/mathbeveridge/math236_f20/main/data/Senate106matrix.csv&#39;, &#39;https://raw.github.com/mathbeveridge/math236_f20//main/data/Senate106senators.csv&#39;) senate.2012.files = c(&#39;https://raw.github.com/mathbeveridge/math236_f20/main/data/Senate112matrix.csv&#39;, &#39;https://raw.github.com/mathbeveridge/math236_f20//main/data/Senate112senators.csv&#39;) 23.2 The 88th Congressional Session (1964) Let’s load in our data. # pick the data set that we want to look at senate.files = senate.1964.files # First we load in the information about the senators. # We will use these names as our labels. # We also set up the colors we will use for our data points. senators &lt;- read.csv(senate.files[2], header=FALSE) sen.name = senators[,2] sen.party = senators[,4] sen.color=rep(&quot;goldenrod&quot;, length(senators)) sen.color[sen.party==&#39;D&#39;]=&quot;cornflowerblue&quot; sen.color[sen.party==&#39;R&#39;]=&quot;firebrick&quot; # Next we load in the square matrix that measures how often senators voted together. # We add names for the columns and rows. votes &lt;- read.csv(senate.files[1], header=FALSE) names(votes) &lt;- sen.name row.names(votes) &lt;- sen.name knitr::kable( head(votes), booktabs = TRUE, caption = &#39;Congressional Sessions of the US Senate&#39; ) Table 23.2: Congressional Sessions of the US Senate AIKEN George David ALLOTT Gordon Llewellyn ANDERSON Clinton Presba BARTLETT Edward Lewis (Bob) BEALL James Glenn BENNETT Wallace Foster BIBLE Alan Harvey BOGGS James Caleb BREWSTER Daniel Baugh BURDICK Quentin Northrup BYRD Harry Flood BYRD Robert Carlyle CANNON Howard Walter CARLSON Frank CASE Clifford Philip CHURCH Frank Forrester CLARK Joseph Sill COOPER John Sherman COTTON Norris H. CURTIS Carl Thomas DIRKSEN Everett McKinley DODD Thomas Joseph DOUGLAS Paul Howard EASTLAND James Oliver ELLENDER Allen Joseph ENGLE Clair ERVIN Samuel James Jr. FONG Hiram Leong FULBRIGHT James William GOLDWATER Barry Morris GORE Albert Arnold GRUENING Ernest Henry HART Philip Aloysius HARTKE Rupert Vance HAYDEN Carl Trumbull HICKENLOOPER Bourke Blakemore HILL Joseph Lister HOLLAND Spessard Lindsey HRUSKA Roman Lee HUMPHREY Hubert Horatio Jr. INOUYE Daniel Ken JACKSON Henry Martin (Scoop) JAVITS Jacob Koppel JOHNSTON Olin DeWitt Talmadge JORDAN Benjamin Everett KEATING Kenneth Barnard KEFAUVER Carey Estes KUCHEL Thomas Henry LAUSCHE Frank John LONG Edward Vaughn LONG Russell Billiu MAGNUSON Warren Grant MANSFIELD Michael Joseph (Mike) McCARTHY Eugene Joseph McCLELLAN John Little McGEE Gale William McGOVERN George Stanley McNAMARA Patrick Vincent METCALF Lee Warren MONRONEY Almer Stillwell Mike MORSE Wayne Lyman MORTON Thruston Ballard MOSS Frank Edward (Ted) MUNDT Karl Earl MUSKIE Edmund Sixtus NEUBERGER Maurine Brown PASTORE John Orlando PROUTY Winston Lewis PROXMIRE William RANDOLPH Jennings RIBICOFF Abraham Alexander ROBERTSON Absalom Willis RUSSELL Richard Brevard Jr. SALTONSTALL Leverett SCOTT Hugh Doggett Jr. SMATHERS George Armistead SMITH Margaret Chase SPARKMAN John Jackson STENNIS John Cornelius SYMINGTON William Stuart (Stuart) TALMADGE Herman Eugene WILLIAMS Harrison Arlington Jr. WILLIAMS John James YARBOROUGH Ralph Webster YOUNG Milton Ruben YOUNG Stephen Marvin DOMINICK Peter Hoyt BAYH Birch Evans EDMONDSON James Howard JORDAN Leonard Beck (Len) KENNEDY Edward Moore (Ted) McINTYRE Thomas James MECHEM Edwin Leard MILLER Jack Richard NELSON Gaylord Anton PEARSON James Blackwood PELL Claiborne de Borda SIMPSON Milward Lee TOWER John Goodwin THURMOND James Strom WALTERS Herbert Sanford SALINGER Pierre Emil George AIKEN George David 0 321 252 302 336 287 282 384 286 297 155 246 245 318 366 294 254 316 259 248 302 301 322 172 175 58 201 377 183 128 215 248 292 259 185 308 198 218 243 310 326 302 339 183 199 379 33 394 287 275 155 262 258 278 201 288 298 285 289 301 265 266 280 306 316 256 320 425 334 292 318 173 181 326 321 160 416 217 187 292 190 305 296 219 290 296 286 287 227 323 221 331 215 328 316 297 306 248 187 182 124 26 ALLOTT Gordon Llewellyn 321 0 221 251 293 326 266 344 237 250 171 196 234 322 289 237 172 296 260 286 286 255 239 163 193 50 191 326 137 139 157 218 219 200 147 287 179 227 269 235 257 271 272 169 205 296 14 310 285 237 141 237 203 222 211 235 230 204 227 253 230 285 235 323 246 178 239 340 278 240 247 204 177 284 292 127 323 183 199 244 196 232 288 190 294 228 339 221 225 344 159 263 244 320 236 327 234 321 240 208 129 17 ANDERSON Clinton Presba 252 221 0 304 258 175 302 248 322 303 106 254 298 215 290 287 279 227 170 142 187 286 279 138 161 82 173 256 193 114 213 256 308 284 213 180 186 179 132 303 338 287 276 203 190 280 40 284 190 265 178 278 255 296 155 276 308 305 303 310 255 191 298 201 321 253 301 235 271 283 303 129 136 223 248 186 295 200 165 289 182 299 166 238 219 263 182 302 290 203 280 332 135 215 299 214 289 144 111 114 137 21 BARTLETT Edward Lewis (Bob) 302 251 304 0 288 201 363 291 349 386 133 318 323 258 335 359 335 264 182 162 213 351 355 150 187 83 196 313 237 75 264 340 370 328 252 216 245 221 154 395 419 349 332 263 230 332 48 317 205 328 209 335 325 361 196 362 377 375 374 389 328 208 334 230 401 327 377 284 336 349 379 162 175 249 286 215 334 250 185 333 206 378 193 285 256 326 200 348 291 220 292 400 141 233 379 215 377 159 110 144 150 37 BEALL James Glenn 336 293 258 288 0 274 290 357 279 290 170 227 249 307 330 250 243 278 268 249 254 315 306 152 156 57 191 355 168 149 194 256 286 270 174 271 183 194 255 280 310 292 327 201 207 367 24 352 248 264 158 256 223 260 188 262 276 266 258 275 275 255 247 295 284 241 292 349 326 286 310 175 182 287 347 145 368 179 170 307 198 295 278 216 267 293 292 266 236 318 219 315 238 304 295 306 289 249 225 189 124 32 BENNETT Wallace Foster 287 326 175 201 274 0 249 337 178 183 238 198 212 296 233 185 135 270 317 343 290 199 189 212 226 37 244 284 162 220 162 200 167 156 124 336 189 238 328 176 205 215 223 203 231 251 14 263 282 188 191 178 151 163 241 189 176 161 173 208 203 267 181 360 185 151 188 305 246 198 189 239 244 261 264 148 274 204 253 207 245 184 349 177 286 184 329 171 198 393 109 223 307 321 181 300 195 370 303 290 143 17 Looking at this table, we see that Aiken voted with Allott 321 times. Aiken voted with Anderson 252 times. Allot voted with Anderson 221 times. And so on. We also should note that ** each “coordinate” represents a senator. Coordinate 1 is “Aiken.” Coordinate 2 is “Allot,” etc. This interpretation will be important later! 23.3 Using the Eigenbasis As the table above notes, there are 102 Senators in our data set. So each Senator is represented by a vector in \\(\\mathbb{R}^{102}\\). This vector represents how similar the Senator is to each colleague. But just because our data lives in \\(\\mathbb{R}^{102}\\) doesn’t mean that it is inherently 102-dimensional! We can draw a line in \\(\\mathbb{R}^3\\). That line is 1-dimensional, even though it lives in a higher dimensional space. Maybe if we find a good basis for \\(\\mathbb{R}^{102}\\) then we can detect some interesting features of the data set. So what happens if we use an eigenbasis? We know that this basis is custom-made for our matrix. Here is what we will observe: The basis of eigenvector of our matrix will pick up some patterns in our data. The eigenvectors corresponding to the largest eigenvalues are the most important ones when modeling our column space. This is where the patterns are! The eigenvectors for the small eigenvalues don’t really matter. They just pick up “noise” in the data. In other words: Our data set will be “essentially low dimensional” (maybe only 3D or 4D) even though it lives in a high dimensional space. Moreover, the structure of the eigenvectors (for the large eigenvalues) will reflect the prevalent patterns in the data. 23.3.1 Using the Dominant Eigenvectors. First we will find the eigenvalues and eigenvectors. mat = data.matrix(votes) eigsys = eigen(mat) eigsys$values ## [1] 24198.14970 4385.33792 2523.42898 413.59778 ## [5] 274.12291 219.84394 93.86570 -19.59596 ## [9] -39.68442 -73.58725 -84.08451 -124.18257 ## [13] -137.88542 -155.95645 -162.94701 -176.71449 ## [17] -183.14311 -197.74427 -203.16191 -219.90337 ## [21] -222.98559 -225.77864 -236.01870 -240.90787 ## [25] -249.06305 -257.49697 -263.81555 -268.10860 ## [29] -272.63265 -275.30214 -279.92416 -280.34948 ## [33] -287.69592 -292.00408 -296.93484 -299.17667 ## [37] -303.67128 -309.89778 -314.33373 -318.90633 ## [41] -320.25895 -321.98658 -326.58688 -327.83596 ## [45] -332.06144 -336.46538 -338.76991 -343.48455 ## [49] -346.38591 -348.16746 -349.23190 -350.47588 ## [53] -358.25692 -359.24338 -360.90037 -363.21807 ## [57] -365.17417 -370.46006 -371.56674 -374.12517 ## [61] -379.23090 -380.38760 -382.65978 -384.71680 ## [65] -388.62093 -389.78271 -391.81075 -394.10770 ## [69] -396.11524 -400.65534 -401.80357 -405.81092 ## [73] -407.44165 -410.46333 -411.66607 -412.99086 ## [77] -416.39535 -418.09114 -418.76583 -422.02379 ## [81] -424.59269 -426.38701 -428.84359 -430.30353 ## [85] -431.59921 -435.24536 -435.91447 -437.63064 ## [89] -439.58808 -443.56104 -445.20242 -447.99489 ## [93] -451.15747 -453.68351 -454.57543 -458.33775 ## [97] -461.38347 -463.97462 -467.11710 -469.27240 ## [101] -474.43223 -475.75931 What do we see? The largest eigenvalue \\(\\approx 24200\\) is huge compared to the others! There is a huge gap after that. So this eigenspace is the most important (by far)! The second and third eigenvectors are still pretty big \\(\\approx 4400\\) and \\(\\approx 2500\\). But then we have another big gap: the rest have magnitudes below 500. So it seems like this data set is “roughly” 3-dimensional. Of course, this isn’t technically correct because our matrix is invertible (all the eigenvalues are nonzero). But you can think of this data set as a “cloud of points” around a 3D subspace of \\(\\mathbb{R}^{102}\\). 23.3.2 Patterns in the First Two Eigenvectors Let’s create a plot of the first two eigenvectors. Each point represents a Senator. # let&#39;s give simple names to our top eigenvectors v1 = eigsys$vectors[,1] v2 = eigsys$vectors[,2] v3 = eigsys$vectors[,3] v4 = eigsys$vectors[,4] v5 = eigsys$vectors[,5] # Plot use v1 for the x-axis and v2 for the y-axis # We color the points by the party of the Senator xdir = v1 ydir = v2 plot(xdir, ydir, pch=20,cex=1, col=sen.color) During class, we will talk about how to think about this picture. We will also compare this plot to plots made using the other eigenvectors. 23.3.3 Creating a Table Sorted by an Eigenvector When we get to more contemporary data, it will be fun later on to look at the names of the Senators who get large (positive or negative) eigenvector scores. Here is some code to help with that. myorder= order(v1) eigendata = data.frame(cbind(v1,v2, sen.party)) rownames(eigendata)=sen.name myorder= order(v1) knitr::kable( head(eigendata[myorder,]), booktabs = TRUE, caption = &#39;One Extreme&#39; ) Table 23.3: One Extreme v1 v2 sen.party INOUYE Daniel Ken -0.124510990388193 0.0960716902237526 D McINTYRE Thomas James -0.123097587465213 0.0793272186071726 D SMITH Margaret Chase -0.120654354853401 0.0290668793843522 R MUSKIE Edmund Sixtus -0.118947766632202 0.0997692334878442 D MONRONEY Almer Stillwell Mike -0.118641982145802 0.0596868115454118 D HUMPHREY Hubert Horatio Jr. -0.118613491347109 0.112820684546629 D myrevorder = order(-v1) knitr::kable( head(eigendata[myrevorder,]), booktabs = TRUE, caption = &#39;The Other Extreme&#39; ) Table 23.3: The Other Extreme v1 v2 sen.party SALINGER Pierre Emil George -0.011895821132766 0.00841073422406016 D KEFAUVER Carey Estes -0.0154423731600536 0.0140931934686823 D ENGLE Clair -0.0307343177099913 0.0102249015658184 D GOLDWATER Barry Morris -0.0482357085359829 -0.118817158736983 R WALTERS Herbert Sanford -0.0605618365871763 -0.0767520789280585 D BYRD Harry Flood -0.0665699568843238 -0.172435813910847 D 23.4 Your Turn The data for the 88th Congressional Session (1964) does not seem very partisan. Democrats and Republicans are finding plenty of common ground. When we look at the “extremes” we find some recognizable names, including Hubert Humphrey (D) and Barry Goldwater (R). But there is no clear “left” or “right” pattern to the party affiliation. What about the remaining 5 data sets? It’s your turn to explore and discuss. The code above has been written for ease of reuse. Go back to the top and change the following line: # pick the data set that we want to look at senate.files = senate.1964.files to one of the other options: senate.1976.files, senate.1988.files, senate.2000.files, senate.2012.files Do a similar analysis as described above, as well as the things we tried in class together. Find the eigenvalues. Where are the gaps? What is the rough “dimension” of each data set? Plotting the two dominant eigenvectors. Do you see evidence of political divission? Then try plotting other pairs of eigenvectors. Which ones are just noise? Create tables of the two extreme for the dominant eigenvectors. Do you see any names that you recognize? Finally, you have some evidence to help you to answer the orignal question: “Is US Politics more polarized than ever before?” "],["modeling-of-ecological-systems.html", "Vector 24 Modeling of Ecological Systems 24.1 Helper Functions to Plot Dynamical Systems 24.2 Northern Spotted Owl Population 24.3 Northern Spotted Owls Revisited 24.4 Brook Trout in Hunt Creek, MI", " Vector 24 Modeling of Ecological Systems Download this Rmd file In this exploration, we will explore the dynamics of a female animal population modeled by a Leslie matrix \\(L\\) which has the form \\[L = \\begin{bmatrix} F_1 &amp; F_2 &amp; F_3 &amp; \\cdots &amp; F_{n-1} &amp; F_n \\\\ S_1 &amp; 0 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 \\\\ 0&amp; S_2 &amp; 0 &amp; \\cdots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\ddots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\ 0&amp; 0 &amp; 0 &amp; S_{n-2} &amp; 0 &amp; 0 \\\\ 0&amp; 0 &amp; 0 &amp; 0 &amp; S_{n-1} &amp; S_n \\\\ \\end{bmatrix} \\] The female population is grouped in \\(n\\) age classes. The survival rate \\(S_i\\) is probability that an animal in age class \\(i\\) survives and enters age class \\(i+1\\). The fecundity rate \\(F_i\\) is the reproduction rate of animals in age class \\(i\\). Given an initial population \\(\\mathsf{x}_0 \\in \\mathbb{R}^n\\), the population \\(\\mathsf{x}_{t+1}\\) at time \\(t+1\\) is determined by \\[ \\mathsf{x}_{t+1} = A \\mathsf{x}_t. \\] 24.1 Helper Functions to Plot Dynamical Systems I have written some code that generates a trajectory for a dynamical system and then plots it. You should run this code chunk before the others. # run this command to load some practical math functions require(pracma) # Creates a trajectory for the dynamical system # A = the matrix # start = the initial vector # N = number of iterations get_trajectory &lt;- function(A, start, N) { ### this code follows the populations for N steps m = dim(A)[1] # m is the number of rows of L X = matrix(0, nrow=m, ncol=N) # Store the results in a (m x N) matrix called X X[,1] = start # put start in the first column of X # loop N times and put your results in X for (i in 2:N) { X[,i] = A %*% X[,i-1] } return(X) } # Plots a trajectory along a time axis # X = the trajectory # title = the title for the plot # types = a vector of the names for each of the entries of the vector. plot_trajectory &lt;- function(X, title, types) { m = dim(X)[1] N = dim(X)[2] t = seq(1,N) # time print(dim(X)) print(dim(t)) # Expand right side of clipping rect to make room for the legend par(xpd=T, mar=par()$mar+c(0,0,0,10)) ymin = min(0, 1.1 * min(X)) ymax = max(0, 1.1 * max(X)) # Plot graph plot(t, X[1,], type=&#39;l&#39;, col=1, ylim=c(ymin,ymax), ylab=&quot;amount&quot;, xlab=&quot;time&quot;, main=title) for (i in 1:m) { lines(t, X[i,], col=i) points(t,X[i,], col=i, pch=20, cex=.8) } # Plot legend where you want legend(N *1.1, ymax * .85, types, col=1:m, lty = 1) # Restore default clipping rect par(mar=c(5, 4, 4, 2) + 0.1) } 24.2 Northern Spotted Owl Population We model the population dynamics of the Northern Spotted Owls using a Leslie Matrix. The age classes for the spotted owls are: juvenile (less than 1 year old) subadult (1 to 2 years old) adult (over 2 years old) and the Leslie Matrix is: \\[L=\\begin{bmatrix} 0&amp;0&amp;0.33 \\\\ 0.18 &amp; 0 &amp; 0 \\\\ 0 &amp; 0.71 &amp; 0.94 \\end{bmatrix}.\\] 24.2.1 Population forecast Suppose that we started out with 200 juveniles, 100 subadults and 500 adults. What happens to this population over time? L = cbind(c(0,.18,0),c(0,0,.71),c(.33,0,.94)) # the Leslie Matrix L ## [,1] [,2] [,3] ## [1,] 0.00 0.00 0.33 ## [2,] 0.18 0.00 0.00 ## [3,] 0.00 0.71 0.94 start = c(200,100,500) # the starting distribution N = 50 # N is the number of iterations X = get_trajectory(L, start, N) plot_trajectory(X, &quot;Owls in Age Group&quot;, c(&quot;(0-1)&quot;, &quot;(1-2)&quot;, &quot;(over 2)&quot;)) ## [1] 3 50 ## NULL 24.2.2 The population at \\(t=200\\) Change the code above to run for \\(N=200\\). Can you now make a stronger statement about the long-term population? N = 200 # N is the number of iterations X = get_trajectory(L, start, N) plot_trajectory(X, &quot;Owls in Age Group&quot;, c(&quot;(0-1)&quot;, &quot;(1-2)&quot;, &quot;(over 2)&quot;)) ## [1] 3 200 ## NULL Yes, we can now see that populations are dying out. 24.2.3 Eigenvectors and eigenvalues of \\(L\\) Use eigenvalues and eigenvectors to explain the asymptotic behavior of the system. You can use the function eigen(A) to compute the eigenvalues and eigenvectors of a matrix. # your code goes here eigen(L) ## eigen() decomposition ## $values ## [1] 0.9835927+0.0000000i -0.0217964+0.2059185i ## [3] -0.0217964-0.2059185i ## ## $vectors ## [,1] [,2] [,3] ## [1,] 0.31754239+0i 0.6820937+0.0000000i 0.6820937+0.0000000i ## [2,] 0.05811107+0i -0.0624124-0.5896338i -0.0624124+0.5896338i ## [3,] 0.94646180+0i -0.0450520+0.4256233i -0.0450520-0.4256233i The largest eigenvalue of less than one. This explains why the owls are dying: when the dominant eigenvalue is less than one, we tend to the zero vector as time increases. 24.3 Northern Spotted Owls Revisited More recent spotted owl data give the following entries for the Leslie Matrix of the spotted owl population: Juvenile Survival 0.22 Subadult Survival 0.85 Adult Survival 0.94 Subadult Fecundity 0.00 Adult Fecundity 0.33 24.3.1 Find the long-range populations Using this new Leslie matrix \\(L\\) and the same initial populations \\(\\mathsf{x}_0 = [200,100, 500]\\) as above, create a plot of the owl populations for \\(N=100\\). # Uncomment the next line and create this matrix # L = L = cbind(c(0,.22,0),c(.15,0,.85),c(.33,0,.94)) # the Leslie Matrix L ## [,1] [,2] [,3] ## [1,] 0.00 0.15 0.33 ## [2,] 0.22 0.00 0.00 ## [3,] 0.00 0.85 0.94 start = c(200,100,500) N = 100 X = get_trajectory(L, start, N) plot_trajectory(X, &quot;Owls in Age Group&quot;, c(&quot;(0-1)&quot;, &quot;(1-2)&quot;, &quot;(over 2)&quot;)) ## [1] 3 100 ## NULL 24.3.2 The populations at \\(t=100\\) versus the dominant eigenvector The following code compares your \\(t=100\\) population ratios to the dominant eigenvector ratios. What do you notice? Why does this make sense? finalpop = X[,N] finalpop = finalpop/sum(finalpop) vec = as.numeric(eigen(L)$vectors[,1]) vec = vec/sum(vec) tabledata = cbind(finalpop,vec) tableframe = data.frame(tabledata) names(tableframe) = c(&#39;population&#39;, &#39;eigenvector&#39;) rownames(tableframe) = c(&#39;juveniles&#39;,&#39;subadults&#39;,&#39;adults&#39;) knitr::kable( tableframe, booktabs = TRUE, caption = &#39;Ratios of vector entries&#39; ) Table 24.1: Ratios of vector entries population eigenvector juveniles 0.2403776 0.2403776 subadults 0.0527053 0.0527053 adults 0.7069171 0.7069171 These ratios are the same. This is as expected, since in the limit, the vectors should coverge to the direction of the dominant eigenvalue. 24.3.3 The long-range prospects What are the long-range prospects for the Northern Spotted Owl? Is it better or worse than the original data given above? They are much better. The population is increasing because the dominant eigenvalue 1.003 is larger than 1. ## [1] 1.003373+0i 24.4 Brook Trout in Hunt Creek, MI Here is the Leslie Matrix of a population of brook trout in Hunt Creek in Michigan. The population is categorized into 5 age categories: fingerlings (0,1), yearlings (1-2), young adults (2-3), adults (3-4), and adults (4-5). Right now the population is seen to be dying off. 24.4.1 The Leslie Matrix Explain, in words, the meaning of each of the non-zero entries of the matrix \\(L\\). L = cbind(c(0,.05,0,0,0),c(0,0,.28,0,0),c(37,0,0,.16,0),c(64,0,0,0,.08),c(82,0,0,0,0.00)) L ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.00 0.00 37.00 64.00 82 ## [2,] 0.05 0.00 0.00 0.00 0 ## [3,] 0.00 0.28 0.00 0.00 0 ## [4,] 0.00 0.00 0.16 0.00 0 ## [5,] 0.00 0.00 0.00 0.08 0 The survival rates are: fingerlings to yearlings: 0.05 yearlings to young adults: 0.28 young adults to adults (3-4): 0.16 adults (3-4) to adults (4-5): 0.08 adults (4-5) to adults (4-5): 0 The nonzero fecundity rates are: young adults: 37 adults (3-4): 64 adults (4-5): 82 24.4.2 Find the long-range populations Here are the population dynamics. Calculate the eigenvalues for \\(L\\) and match what you see to the plot of the populations for \\(t=50\\). What do you conclude about the long-range population? # your code goes here eigen(L) ## eigen() decomposition ## $values ## [1] 0.8871688+0.0000000i -0.3053909+0.7049316i ## [3] -0.3053909-0.7049316i -0.1381935+0.0946928i ## [5] -0.1381935-0.0946928i ## ## $vectors ## [,1] [,2] ## [1,] 0.9982530311+0i 9.975971e-01+0.000000e+00i ## [2,] 0.0562606057+0i -2.580999e-02-5.957701e-02i ## [3,] 0.0177564522+0i -1.618518e-02+1.726352e-02i ## [4,] 0.0032023584+0i 4.639142e-03+1.663816e-03i ## [5,] 0.0002887711+0i -3.305712e-05-5.121576e-04i ## [,3] [,4] ## [1,] 9.975971e-01+0.000000e+00i 0.7864719+0.0000000i ## [2,] -2.580999e-02+5.957701e-02i -0.1936371-0.1326839i ## [3,] -1.618518e-02-1.726352e-02i 0.1416272+0.3658824i ## [4,] 4.639142e-03-1.663816e-03i 0.0859429-0.3647277i ## [5,] -3.305712e-05+5.121576e-04i -0.1323078+0.1204805i ## [,5] ## [1,] 0.7864719+0.0000000i ## [2,] -0.1936371+0.1326839i ## [3,] 0.1416272-0.3658824i ## [4,] 0.0859429+0.3647277i ## [5,] -0.1323078-0.1204805i The largest eigenvalue 0.887 is less than 1. So the population is going to die out. start = c(100,100,100,100,100) # the starting distribution N = 50 # N is the number of iterations X = get_trajectory(L, start, N) plot_trajectory(X, &quot;Trout in Age Group&quot;, c(&quot;Fingerlings (0-1)&quot;, &quot;Yearlings (1-2)&quot;, &quot;Young Adults (2-3)&quot;,&quot;Adults (3-4)&quot;,&quot;Adults (4-5)&quot;)) ## [1] 5 50 ## NULL 24.4.3 River Clean Up A river clean up effort is being conducted with the hope of increasing the rate of survival from fingerlings to yearlings. To what level does that survival rate need to be increased to in order for the population to reach a steady state? Find your answer (accurate up to 3 decimal places) by trial and error. (You only need to change a single entry of \\(L\\). Which one?) What are the population percentages in the stable population? # update the fingerling to yearling survival rate L = cbind(c(0,.074,0,0,0),c(0,0,.28,0,0),c(37,0,0,.16,0),c(64,0,0,0,.08),c(82,0,0,0,0.00)) L ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.000 0.00 37.00 64.00 82 ## [2,] 0.074 0.00 0.00 0.00 0 ## [3,] 0.000 0.28 0.00 0.00 0 ## [4,] 0.000 0.00 0.16 0.00 0 ## [5,] 0.000 0.00 0.00 0.08 0 ## do your eigenvalue/eigenvector analysis here eigensys = eigen(L) val = eigensys$values[1] vec = eigensys$vectors[,1] val ## [1] 1.000172+0i vec/sum(vec) ## [1] 0.9105157386+0i 0.0673665682+0i 0.0188593926+0i ## [4] 0.0030169835+0i 0.0002413171+0i Changing the fingerling survival rate to 0.074 results in a dominant eigenvector of 1 (approximately). The long-range population percentages will match the dominant eigenvector calculated above: \\((0.9105, 0.0674, 0.0189, 0.0030, 0.0002)\\). This code shows the population dynamics. You shouldn’t need to edit this. start = c(100,100,100,100,100) # the starting distribution N = 50 # N is the number of iterations X = get_trajectory(L, start, N) plot_trajectory(X, &quot;Trout in Age Group&quot;, c(&quot;Fingerlings (0-1)&quot;, &quot;Yearlings (1-2)&quot;, &quot;Young Adults (2-3)&quot;,&quot;Adults (3-4)&quot;,&quot;Adults (4-5)&quot;)) ## [1] 5 50 ## NULL "],["digit-recognition.html", "Vector 25 Digit Recognition 25.1 US Post Office Zip Code Data 25.2 Magnitude, Distance and Angle 25.3 Machine Learning Algorithms", " Vector 25 Digit Recognition Download this Rmd file Machine Learning has become a big part of our daily experience. When my phone suggests that it’s time for bed, or volunteers how long my today’s commute will be, I am benefiting from machine learning. My phone has learned my patterns and is predicting what might be useful for me today based on past data. Rather than being “told” the answer up front, a machine learning algorithm learns what to do by analyzing training data. The algorithm then uses patterns to decide what to do with new data. In other words, these algorithms learn from experience, just like humans do. That’s why machine learning falls under the umbrella of artificial intelligence. And guess what sits at the heart of machine learning? That’s right: linear algebra. Today we will create a rudimentary machine learning algorithm using the dot product in a high-dimensional space. 25.1 US Post Office Zip Code Data We will look at a classic machine learning problem: identifying hand-written digits. Our data comes from the U.S. Postal Service: each digit comes zip codes automatically scanned from envelopes. I found this data at this Machine Learning course page at Stanford, but it originates from the neural network group at AT&amp;T research labs. I’ve made a copy of the data, and pre-processed the training data (to speed things up for us). Let’s load the data and print out the first 10 rows and columns of the test data. library(readr) require(pracma) digit.test.file = &#39;https://raw.github.com/mathbeveridge/math236_f20//main/data/digit-test.csv&#39; digit.centroids.file = &#39;https://raw.github.com/mathbeveridge/math236_f20//main/data/digit-centroid.csv&#39; test &lt;- data.matrix(read_csv(digit.test.file, col_names = FALSE)) centroids &lt;- data.matrix(read_csv(digit.centroids.file, col_names = FALSE)) # take the transpose so that we can deal with column vectors (which is what we are used to) test=t(test) centroids=t(centroids) dim(test) ## [1] 257 2007 test[1:10,1:10] ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## X1 9.000 6 3.000 6 6.000 0.000 0.000 0.000 6.000 ## X2 -1.000 -1 -1.000 -1 -1.000 -1.000 -1.000 -1.000 -1.000 ## X3 -1.000 -1 -1.000 -1 -1.000 -1.000 -1.000 -1.000 -1.000 ## X4 -1.000 -1 -1.000 -1 -1.000 -1.000 -1.000 -1.000 -1.000 ## X5 -1.000 -1 -0.593 -1 -1.000 -1.000 -1.000 -1.000 -1.000 ## X6 -1.000 -1 0.700 -1 -1.000 -1.000 -1.000 -1.000 -1.000 ## X7 -0.948 -1 1.000 -1 -1.000 -1.000 -0.785 -0.914 -1.000 ## X8 -0.561 -1 1.000 -1 -1.000 0.195 0.775 -0.688 -1.000 ## X9 0.148 -1 1.000 -1 -0.858 1.000 0.268 -0.736 -0.761 ## X10 0.384 -1 1.000 -1 -0.106 0.054 -1.000 0.956 0.438 ## [,10] ## X1 9.000 ## X2 -1.000 ## X3 -1.000 ## X4 -1.000 ## X5 -1.000 ## X6 -1.000 ## X7 -0.904 ## X8 -0.060 ## X9 0.638 ## X10 0.678 The data set has 257 rows and 2007 columns. Each column of the data corresponds to a hand-written digit. So the test data has 2007 hand-written digits. 25.1.1 Visualizing the Data Here is how the data is formatted. The first value in each column tells you the correct digit. Looking at the data above, we see that the first ten hand-written digits are \\(9,6,3,6,6,0,0,0,6,9\\). The remaining 256 values in each column correspond to a grayscale image of the digit. We must reshape this \\(256 \\times 1\\) column vector into a \\(16 \\times 16\\) matrix in order to display it. The following code will display the handwritten image for a given column. (Note: You don’t have to understand this code.) # Takes in a vector of length 257 and creates an image form entries 2:257 # digitcol: a 257 x 1 vector. The first entry is the digit classification. The remaining 256 entries are the 16 x 16 image. plot_digit &lt;- function(digitcol) { img = digitcol[2:257] dim(img) &lt;- c(16, 16) img = img[,ncol(img):1 ] image(img, axes = FALSE, col = grey(seq(0, 1, length = 256))) } Now let’s display a few random digits from the test dataset. Feel free to re-run this block a few times. You’ll see different examples each time! &#39;```{r digitsamples, out.width = &quot;150px&quot;, out.height = &quot;150px&quot;, echo=TRUE} samp = sample(1:dim(test)[2], 6, replace=FALSE) for (i in 1:6) { print(test[1,samp[i]]) plot_digit(test[,samp[i]]) } Next, let’s talk about our training data. The original training data has 7291 different digits. Different machine learning techniques will use this data in different ways. But each of them follows the same idea: Define a measure of “closenesss” to the training data of known digits. Then compare the new (unknown) digits and categorize it according to the best fit to the training data. Our goal today is to highlight the role of linear algebra, not come up with a high-performance algorithm. So we will simplify our training set. I have pre-processed the 7291 training digits and found the “average” vector for each digit. We will refer to these 10 training vectors as centroids since we are finding the vector in the center of all the training digits of each type. Since these are averages, when we plot them, they will look like blurry versions of the actual digit. Let’s look. &#39;```{r digitcentroid, out.width = &quot;150px&quot;, out.height = &quot;150px&quot;, echo=TRUE} library(grid) images = vector() for (i in 0:9) { dimg = centroids[,i+1] plot_digit(dimg) } 25.1.2 Algorithm Overview Here is our simple machine learning algorithm. For each test digit: Find the centroid that is closest to that digit. Classify the test digit as the same type as that centroid. We will consider two definitions of “closeness.” Both are calculated using the dot product! The distance between the vectors in \\(\\mathbb{R}^{256}\\). This Euclidean distance or Pythagorean distance is probably what you were thinking of when I started talking about distance in high dimensions. The angle between the vectors in \\(\\mathbb{R}^{256}\\). Interesting! Does this surprise you? Why is this a reasonable way to measure distance? Well, span is one of the two fundamental characteristics of linear algebra (the other is linear independence). By measuring the angle between the vectors, we have a notion of distance between the span of each vector. In other words, the angle measures the distance between a pair of one-dimensional subspaces. This measure is known as cosine similarity. So let’s get started. Here is the good news: I’ve written the code to process all 2007 of the digits and then compare the classification to the truth. The results are then presented a matrix whose \\((i,j)\\)-th entry counts the number of time digit \\(i\\) was classified as digit \\(j\\). (So we hope that the diagonal entries will be big!) Here’s the even better news: I’ve left three functions for you to implement using the dot product. 25.2 Magnitude, Distance and Angle Here are the three methods that you must implement: get_magnitude(colvec) get_distance(colvec1, colvec2) get_angle(colvec1, colvec2) Implement these methods according to the definitions: \\[ \\| \\mathsf{v} \\| = \\sqrt{ \\mathsf{v} \\cdot \\mathsf{v}}, \\qquad d(\\mathsf{v}, \\mathsf{w}) = \\| \\mathsf{v} - \\mathsf{w} \\|, \\qquad \\theta = \\cos^{-1} \\left( \\frac{\\mathsf{v} \\cdot \\mathsf{w}}{\\| \\mathsf{v} \\| \\, \\| \\mathsf{w} \\|} \\right). \\] Note: The R command length(vec) returns the number of entries in the vector vec. To avoid confusion, we use “magnitude” to refer to the geometric length of a vector. Here are some R functions that you will need. The dot product of two column vectors colvec1 and colvec2 is calculated by t(colvec1) %*% colvec2. Here we are tranposing colvec1 to turn it into a row vector. The square root of y is calculated by sqrt(y). The inverse cosine of x is calculated by acos(x). get_magnitude &lt;- function(colvec) { # replace with your implementation len = sqrt(t(colvec) %*% colvec) return (len) } get_distance &lt;- function(colvec1, colvec2) { v = colvec1 - colvec2 dist = get_magnitude(v) return (dist) } get_angle &lt;- function(colvec1, colvec2) { dotprod = t(colvec1) %*% colvec2 len1 = get_magnitude(colvec1) len2 = get_magnitude(colvec2) val = dotprod / (len1 * len2) angle = acos(val) return (angle) } Here is some test code you can use to confirm that your implementations are correct. check_equal &lt;- function(val1, val2) { diff = zapsmall(val1 - val2) if (diff^1 &gt; 0.001) { sprintf(&#39;fail %f %f&#39;, val1, val2) } else { print(&#39;pass&#39;) } } print(&#39;get_magnitude&#39;) ## [1] &quot;get_magnitude&quot; check_equal(get_magnitude(c(3,4)),5) ## [1] &quot;pass&quot; check_equal(get_magnitude(c(1,2,-2,1)),sqrt(10)) ## [1] &quot;pass&quot; check_equal(get_magnitude(c(0,0,0)),0) ## [1] &quot;pass&quot; print(&#39;get_dist&#39;) ## [1] &quot;get_dist&quot; check_equal(get_distance(c(3,4,5),c(2,-1,-3)), sqrt(90)) ## [1] &quot;pass&quot; check_equal(get_distance(c(1,2,-2,1),c(1,2,-2,1)), 0) ## [1] &quot;pass&quot; check_equal(get_distance(c(2,1.5),c(-2,-1.5)), 5) ## [1] &quot;pass&quot; print(&#39;get_angle&#39;) ## [1] &quot;get_angle&quot; check_equal(get_angle(c(1,2,3,1),c(-3,3,-1,0)), pi/2) ## [1] &quot;pass&quot; check_equal(get_angle(c(1,0),c(1,1)), pi/4) ## [1] &quot;pass&quot; check_equal(get_angle(c(3,0),c(1,0)), 0) ## [1] &quot;pass&quot; 25.3 Machine Learning Algorithms Once your magnitude, distance and angle code is working, you are ready to try out our two rudimentary machine learning algorithms! You will find that each of them does okay (but not great) for this data set. Both get around \\(80\\%\\) of the digits correct. (A good machine learning algorithm can get \\(98\\%\\) correct.) Run the code below and look at the output. Discuss the following questions Which digits are most commonly mistaken for one another? Which algorithm does better: closest by distance or closest by angle? We turned the training data into a single centroid and then used that to measure distances. A better algorithm would make use of all of the training points during the classification. Discuss what improvements you would try instead of using the centroids. 25.3.1 Get Closest by Distance # Finds closest centroid to the given data # data - a column vector of length 257. the first entry is the actual digit. the remaining 256 are the data # centroids - a 257 x 10 matrix. Column j is the centroid of digit j-1. get_closest_by_distance &lt;- function(data, centroids) { idx = 0 min_dist = 1024 for (i in 1:10) { colvec1 = centroids[2:257,i] colvec2 = data[2:257] dist = get_distance(colvec1, colvec2) if (dist &lt; min_dist) { idx = i min_dist = dist } } return (idx) } results = matrix(0, nrow=10, ncol=10) # try to match each column of the test data for (i in 1:dim(test)[2]) { true_digit = data.matrix(test[1, i]) + 1 digit_data = data.matrix(test[,i]) matched_digit = get_closest_by_distance(digit_data, centroids) results[true_digit,matched_digit] = results[true_digit,matched_digit]+1 } labels=c(0:9) colnames(results) &lt;- labels rownames(results) &lt;- labels results ## 0 1 2 3 4 5 6 7 8 9 ## 0 297 0 2 3 4 2 39 1 10 1 ## 1 0 259 0 1 2 0 2 0 0 0 ## 2 6 0 145 8 17 3 2 2 15 0 ## 3 6 0 4 131 1 15 0 0 7 2 ## 4 1 6 5 0 150 1 5 2 2 28 ## 5 10 0 0 8 6 123 0 0 6 7 ## 6 14 0 4 0 4 4 143 0 1 0 ## 7 0 2 2 0 8 0 0 117 2 16 ## 8 4 2 3 11 7 6 0 1 128 4 ## 9 0 4 0 0 16 1 0 11 4 141 sprintf(&#39;matched: %f percent&#39; , sum(diag(results))/sum(results) * 100) ## [1] &quot;matched: 81.415047 percent&quot; 25.3.2 Get Closest by Angle get_closest_angle &lt;- function(data, centroids) { idx = 0 minangle = pi for (i in 1:10) { angle = get_angle(data[2:257,1], centroids[2:257,i]) if (angle &lt; minangle) { idx = i minangle = angle } } return (idx) } results = matrix(0, nrow=10, ncol=10) for (i in 1:dim(test)[2]) { true_digit = data.matrix(test[1, i]) + 1 digit_data = data.matrix(test[, i]) matched_digit = get_closest_angle(digit_data, centroids) results[true_digit,matched_digit] = results[true_digit,matched_digit]+1 } labels=c(0:9) colnames(results) &lt;- labels rownames(results) &lt;- labels results ## 0 1 2 3 4 5 6 7 8 9 ## 0 298 0 1 3 4 1 39 1 11 1 ## 1 0 260 0 1 2 0 1 0 0 0 ## 2 6 7 135 9 16 3 2 5 15 0 ## 3 6 1 3 129 0 15 0 3 7 2 ## 4 1 13 4 0 147 1 4 2 1 27 ## 5 10 2 0 11 6 120 0 0 4 7 ## 6 12 0 2 0 5 3 147 0 1 0 ## 7 0 3 2 0 7 0 0 118 1 16 ## 8 4 3 1 12 7 5 0 2 127 5 ## 9 0 9 0 0 14 1 0 12 3 138 sprintf(&#39;matched: %f percent&#39; , sum(diag(results))/sum(results) * 100) ## [1] &quot;matched: 80.667663 percent&quot; "],["least-squares-approximation.html", "Vector 26 Least Squares Approximation 26.1 Introduction 26.2 Fitting for a Linear Function 26.3 Fitting a Quadratic Function 26.4 A Linear Model for Production at a Hosiery Mill 26.5 Global Fossil Fuel Emissions", " Vector 26 Least Squares Approximation Download this Rmd file 26.1 Introduction Let’s start with a summary of Least Squares Approximation. The Why: Given a matrix \\(A\\) and a vector \\(\\mathsf{b}\\) that is not in \\(W = \\mathrm{Col}(A)\\), we want to find the “best approximate solution” \\(\\hat{\\mathsf{b}} \\in W\\). In other words, we want to pick the best possible \\(\\hat{\\mathsf{b}} \\approx \\mathsf{b}\\) that lies in the column space of \\(A\\). The What: The answer is to use projections. This “best approximation” is the projection \\(\\hat{\\mathsf{b}} = \\mbox{proj}_W \\mathsf{b}\\). The residual vector vector \\(\\mathsf{z} = \\mathsf{b} - \\hat{\\mathbf{b}}\\) is in \\(W^{\\perp}\\). The length \\(\\| \\mathsf{z} \\|\\) of the residual vector measures the closeness the approximation. The approximate soltuion to our original problem is the vector \\(\\hat{\\mathsf{x}}\\) such that \\(A \\hat{\\mathsf{x}} = \\hat{\\mathsf{b}}\\). The How: A clever way to solve this is to use the normal equations. The best choice for \\(\\hat{\\mathsf{x}}\\) satisfies \\[ A^{\\top} A \\hat{\\mathsf{x}} = A^{\\top} \\mathsf{b}. \\] Here is a template for the R code that will find the least squares solution \\(\\hat{\\mathsf{x}}\\), the projection vector \\(\\hat{\\mathsf{b}} = \\mbox{proj}_W \\mathsf{b}\\) and the residual vector \\(\\mathsf{z} = \\mathsf{b} - \\hat{\\mathsf{b}}\\). You should read this code line-by-line and make sure that you understand what it is doing! # Given: the matrix A # Given: the target vector b #solve the normal equation (xhat = solve(t(A) %*% A, t(A) %*% b)) # find the projection (bhat = A %*% xhat) # find the residual vector (z = b - bhat) # check that z is orthogonal to Col(A) t(A) %*% z # measure the distance between bhat and b sqrt( t(z) %*% z) 26.2 Fitting for a Linear Function Here are some points that we’d like to fit to a linear function \\(y = cx+d\\). Note: Here we use y instead of b because we like to write linear equations as “\\(y = cx + d\\).” So the expression “\\(b = cx +d\\)” looks funny to us. So we will talk about y and yhat instead of b and bhat. x = c(1, 2, 3, 4) y = c(1, 2.4, 3.6, 4) plot(x,y,pch=19,ylim=c(0,5),) grid() To find a perfect fit line \\(y = d + cx\\) to the data, we need to solve the following \\(Ax = y\\) problem: \\[ \\begin{bmatrix} 1 &amp; 1 \\\\ 1 &amp; 2 \\\\ 1 &amp; 3 \\\\ 1 &amp; 4 \\\\ \\end{bmatrix} \\begin{bmatrix} d \\\\ c \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2.4 \\\\ 3.6 \\\\ 4 \\end{bmatrix} \\] These equations are inconsistent, so we solve the normal equations \\(A^T A x = A^T y\\) and find an approximate solution instead. Pro Tip: a clever way to create the desired matrix \\(A\\) is to use the fact that \\(x^0=1\\) for any number \\(x\\). (A = cbind(x^0,x)) ## x ## [1,] 1 1 ## [2,] 1 2 ## [3,] 1 3 ## [4,] 1 4 26.2.1 Your Turn Use the template code above to find xhat. This is the “least squares solution” that approximates the coefficients \\(c, d\\). # Replace with your code goes here. xhat = c(0,0) 26.2.2 Plotting your least squares solution The following code gives you a visualization of the approximate solution. The black points: the original data points cbind(x,y). This represents the entries of the desired target vector y. The blue curve: the fitted curve, created from the approximate solution xhat. The orange points: the approximations cbind(x,yhat) of the data points cbind(x,y). This represents entries of the projection yhat. The red line segments: the distances between the original data points (block dots) and their approximations (orange dots). The lengths of these red segments are the entries of the residual vector z plot(x,y,pch=19,ylim=c(0,5), main=&#39;the best-fit linear function&#39;) yhat = A %*% xhat tt = seq(1,4,len=100) lines(tt,xhat[1]+xhat[2]*tt,col=&#39;blue&#39;) for (i in 1:length(x)) { lines(c(x[i],x[i]),c(y[i],yhat[i]), col=&#39;red&#39;) } points(x,yhat,pch=19,col=&#39;orange&#39;) points(x,y,pch=19,col=&quot;black&quot;) 26.2.3 Your turn If you have not already done so Find the residual vector \\(z\\). Find the length of \\(z\\). This is a measure of the quality of the approximation: the smaller, the better! Confirm that \\(z\\) is orthogonal to \\(\\mbox{Col}(A)\\). (You may want to use zapsmall().) # your code goes here 26.3 Fitting a Quadratic Function It might surprise you at first that we can use linear algebra to fit a quadratic function. But remember: we are fitting the coefficients. Once we plug in our empirical data, we have a linear combination of the unknown coefficients! So let’s fit a quadratic function \\(y = a_0 + a_1 x + a_2 x^2\\) to the following data: x = c(1,2,3,4,5,6) y = c(7,2,1,3,7,7) plot(x,y,pch=19,ylim=c(0,10)) grid() In this case, the linear model that we’d like to solve is: \\[ \\begin{bmatrix} 1 &amp; 1 &amp; 1\\\\ 1 &amp; 2 &amp; 4 \\\\ 1 &amp; 3 &amp; 9 \\\\ 1 &amp; 4 &amp; 16 \\\\ 1 &amp; 5 &amp; 25 \\\\ 1 &amp; 6 &amp; 36 \\\\ \\end{bmatrix} \\begin{bmatrix} a_0 \\\\ a_1 \\\\ a_2 \\end{bmatrix} = \\begin{bmatrix} 7 \\\\ 2 \\\\ 1 \\\\ 3 \\\\ 7 \\\\ 7 \\end{bmatrix}. \\] Again, this is inconsistent (go ahead and check if you want to). So we need to find the best approximate solution. I will make the \\(A\\) matrix for you one more time. (You’ll have to construct it in the examples below.) A = cbind(x^0, x, x^2) 26.3.1 Your Turn Use the template code above to find xhat. This is the “least squares solution” that approximations of the coefficients \\(a_0, a_1, a_2\\). # Replace with your code goes here. xhat = c(0,0,0) 26.3.2 Plotting your least squares solution Note: this code assumes that you have defined xhat yhat = A %*% xhat # plotting the solution plot(x,y,pch=19,ylim=c(0,10), main=&#39;the best-fit quadratic function&#39;) tt = seq(0,7,len=100) lines(tt,xhat[1] + xhat[2]*tt + xhat[3]*tt^2,col=&#39;blue&#39;) grid() # adding the residual lines for (i in 1:length(x)) { lines(c(x[i],x[i]),c(y[i],yhat[i]), col=&#39;red&#39;) } points(x,yhat,pch=19,col=&#39;orange&#39;) points(x,y,pch=19,col=&quot;black&quot;) 26.3.3 Your turn If you have not already done so, you should Find the residual vector \\(z\\). Compare the entries (positive, negative) with the red line segments in the plot above. Find the length of \\(z\\). This is a measure of the quality of the approximation: the smaller, the better! Confirm that \\(z\\) is orthogonal to \\(\\mbox{Col}(A)\\). (You may want to use zapsmall().) # your code goes here 26.4 A Linear Model for Production at a Hosiery Mill Here is a data set from a 1940’s hosiery mill. It measures monthly production output and total production cost. The data come from the article “Statistical Cost Functions of a Hosiery Mill”, by Joel Dean in Studies in Business Administration, vol. 14, no. 3, 1941. units.produced =c(561.0, 506.16, 502.32, 519.48, 505.44, 501.36, 497.64, 506.52, 492.36, 478.08, 469.8, 470.4, 474.24, 456.6, 469.92, 463.08, 438.48, 444.36, 439.2, 450.96, 437.76, 459., 447.12, 463.08, 490.68, 451.92, 465.48, 465.36, 440.4, 421.2, 405., 411.48, 387.12, 371.64, 338.4, 294.96, 243., 205.08, 172.2, 157.32, 114., 116.88, 112.08, 90.12, 100.2, 75., 65.4, 45.48) production.cost = c(92.64, 88.81, 86.44, 88.8, 86.38, 89.87, 88.53, 91.11, 81.22, 83.72, 84.54, 85.66, 85.87, 85.23, 87.75, 92.62, 91.56, 84.12, 81.22, 83.35, 82.29, 80.92, 76.92, 78.35, 74.57, 71.6, 65.64, 62.09, 61.66, 77.14, 75.47, 70.37, 66.71, 64.37, 56.09, 50.25, 43.65, 38.01, 31.4, 29.45, 29.02, 19.05, 20.36, 17.68, 19.23, 14.92, 11.44, 12.69) Fit a linear model between the units produced and the production cost. \\[p=\\beta_0+\\beta_1 u,\\] where \\(u\\) is the number of units produced in a given month (in thousands), \\(p\\) is the monthly production cost (in $1000s). The constant \\(\\beta_0\\) represents the fixed costs for running the hosiery mill and \\(\\beta_1\\) represents the variable cost per production of 1000 units. Use least squares approximation to find the best fitting linear function for this data. Look at the previous two examples for how to create your matrix \\(A\\). Look at your least squared solution. What are the monthly fixed costs? What are the variable costs per 1000 units? Plot the data along with your linear function. Optional: Compute the length of the residual vector Optional: Confirm that the residual vector is orthogonal to both columns of the coefficient matrix. # your least squares code goes here x = units.produced y = production.cost A = NULL # you must define A and xhat **Note: this code assumes that you have defined xhat and x and y and A. if (! is.null(A)) { plot(x,y,pch=19, main=&#39;hosiery mill production&#39;, xlab=&quot;monthly units produced (1000s)&quot;,ylab=&quot;monthly production cost ($1000s)&quot;) yhat = A %*% xhat tt = seq(0,600,by=.1) lines(tt,xhat[1]+xhat[2]*tt,col=&#39;blue&#39;) } 26.5 Global Fossil Fuel Emissions Now let’s fit a curve to global fossil fuel emmissions between 1751 and 1998. The fossil fuel emissions are measured in megatons of carbon. Here is a plot of the data. year=c(1751:1998) emissions = c(3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 8, 8, 10, 9, 9, 9, 10, 10, 10, 10, 10, 11, 11, 11, 11, 12, 13, 14, 14, 14, 14, 14, 15, 16, 16, 17, 17, 18, 18, 18, 24, 23, 23, 24, 24, 25, 29, 29, 30, 31, 33, 34, 36, 37, 39, 43, 43, 46, 47, 50, 54, 54, 57, 59, 69, 71, 76, 77, 78, 83, 91, 95, 97, 104, 112, 119, 122, 130, 135, 142, 147, 156, 173, 184, 174, 188, 191, 194, 196, 210, 236, 243, 256, 272, 275, 277, 281, 295, 327, 327, 356, 372, 374, 370, 383, 406, 419, 440, 465, 507, 534, 552, 566, 617, 624, 663, 707, 784, 750, 785, 819, 836, 879, 943, 850, 838, 901, 955, 936, 806, 932, 803, 845, 970, 963, 975, 983, 1062, 1065, 1145, 1053, 940, 847, 893, 973, 1027, 1130, 1209, 1142, 1192, 1299, 1334, 1342, 1391, 1383, 1160, 1238, 1392, 1469, 1419, 1630, 1767, 1795, 1841, 1865, 2043, 2177, 2270, 2330, 2463, 2578, 2595, 2701, 2848, 3009, 3146, 3306, 3412, 3588, 3802, 4075, 4227, 4394, 4633, 4641, 4613, 4879, 5018, 5078, 5368, 5297, 5125, 5080, 5067, 5241, 5405, 5573, 5701, 5926, 6035, 6096, 6186, 6089, 6090, 6236, 6378, 6530, 6628, 6608) plot(year,emissions,pch=20,cex=.7,col=&quot;red&quot;) The data suggest that the fossil fuel emissions \\(f\\) follow an exponential model with respect to the year \\(y\\): \\[f = a e^{k(y-1750)},\\] where \\(a\\) and \\(k\\) are the unknown constants. How do we fit our data in this case? Just take the logarithm of both sides! Doing so yields the following linear system: \\[\\log(f)=\\log(a)+k(y-1750).\\] Note: This process works for any logarithm, but it is common to use the natural logarithm (use log() in R). Note: To simplify even further, define time=year-1750 before forming your set of linear constraints. This results in the model \\[ \\log(f)=d+kx,\\] where \\(b=\\log(a)\\) and \\(x\\) is time (since 1750), Use least squares approximation to find the best fitting exponential function for this data. This will give you the values for \\(b\\) and \\(k\\), and once you know \\(b\\), you can find \\(a = \\exp(b)\\). We have started the code for you by defining x=year-1750 and y=log(emissions). ### your code goes here. # be sure to define d and k and A x=year-1750 y=log(emissions) A=NULL b=1 k=1 ##### # your code above has found b and k (a = exp(b)) ## [1] 2.718282 k ## [1] 1 Run the code below to plot the original data along with your exponential model curve \\(f(t)\\). Note: This code assumes that you have already defined the values for A and k and a. Otherwise, it will not work! if (! is.null(A)) { f=function(y){a * exp(k*(y-1750))} plot(year,f(year),type=&quot;l&quot;,lwd=3,ylim=c(0,10000),ylab=&quot;emissions&quot;, main=&quot;best fit exponential function&quot;) points(year,emissions,pch=20,cex=.7,col=&quot;red&quot;) } "],["svd-and-image-compression.html", "Vector 27 SVD and Image Compression 27.1 Singular Value Decomposition 27.2 Image Compression", " Vector 27 SVD and Image Compression Download this Rmd file Today we will use the svd command to find the Singular Value Decomposition of an \\(m \\times n\\) matrix. We will practice reading the output of this command and finding bases for the four fundamental subspaces of a matrix. Then we will look at a cool application of SVD: image compression! 27.1 Singular Value Decomposition Let’s start by looking at the SVD for a couple of matrices. For each one: Determine the rank of \\(A\\) Identify the basis for each of the four fundamental subspaces \\[ \\mbox{Nul}(A), \\qquad \\mbox{Col}(A), \\qquad \\mbox{Row}(A), \\qquad \\mbox{Nul}(A^{\\top}). \\] Pick a vector \\(\\mathsf{x}\\) from the basis for \\(\\mbox{Nul}(A)\\) and confirm that \\(A \\mathsf{x} = \\mathbf{0}\\). Pick a vector \\(\\mathsf{y}\\) from the basis for \\(\\mbox{Col}(A)\\) and confirm that \\(\\mathsf{y}\\) can be written as the a linear combination of the columns of \\(A\\). Find the spectral decomposition of \\(A\\) Use the spectral decomposition to create approximations of matrix \\(A\\) and then compare the quality of the approximation to the size of the gaps of the singular values. 27.1.1 SVD of a Wide Matrix Here is a \\(4 \\times 5\\) matrix \\(A\\). (A = cbind(c(1,-1,1,0),c(-2,3,0,2),c(1,-1,1,0),c(0,1,4,0),c(1,-1,5,-4))) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 -2 1 0 1 ## [2,] -1 3 -1 1 -1 ## [3,] 1 0 1 4 5 ## [4,] 0 2 0 0 -4 Let’s call svd(A) and see what we get. (decomp = svd(A)) ## $d ## [1] 7.657063e+00 4.528454e+00 1.965323e+00 1.981025e-16 ## ## $u ## [,1] [,2] [,3] [,4] ## [1,] -0.2248593 0.3962930 -0.4593417 -0.7624929 ## [2,] 0.2098332 -0.7003225 0.3056557 -0.6099943 ## [3,] -0.8000554 -0.5005874 -0.2933731 0.1524986 ## [4,] 0.5150919 -0.3192373 -0.7807125 0.1524986 ## ## $v ## [,1] [,2] [,3] [,4] ## [1,] -0.1612561 0.13161844 -0.5385224 0.1709288 ## [2,] 0.2754845 -0.77996334 0.1395320 -0.4039829 ## [3,] -0.1612561 0.13161844 -0.5385224 -0.7769031 ## [4,] -0.3905399 -0.59682006 -0.4415746 0.4039829 ## [5,] -0.8482805 -0.02856877 0.4533541 -0.2019914 The return value has three attributes: decomp$d: these are the singular values \\(\\sigma_1, \\sigma_2, \\sigma_3, \\sigma_4\\). Note that \\(\\sigma_4=0\\). decomp$u: the \\(4 \\times 4\\) matrix whose columns are the left singular vectors \\(\\mathsf{u}_1, \\mathsf{u}_1, \\mathsf{u}_1, \\mathsf{u}_1\\) corresponding to the singular values. decomp$v: the \\(5 \\times 4\\) matrix whose columns are right singular vectors the right singular vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4\\) corresponding to the singular values. The final vector \\(\\mathsf{v}_5\\) is missing! Note: The columns of the matrix decomp$v$ are singular vectors (not the rows). Warning! We were expecting decomp$v to be a \\(5 \\times 5\\) matrix, and R only returned a \\(5 \\times 4\\) matrix. What is wrong!? The missing column is the final orthonormal vector \\(\\mathsf{v}_5\\) from \\(\\mbox{Nul}(A)\\). 27.1.1.1 Why has R Omitted this Vector? The answer is that RStudio is being efficient. R has omitted vector \\(\\mathsf{v}_5\\) because we do not need this vector to create the spectral decomposition of \\(A\\). That spectral decomposition is \\[ A = \\sigma_1 \\mathsf{u}_1 \\mathsf{v}_1^{\\top} + \\sigma_2 \\mathsf{u}_2 \\mathsf{v}_2^{\\top} + \\sigma_3 \\mathsf{u}_3 \\mathsf{v}_3^{\\top} + \\sigma_4 \\mathsf{u}_4 \\mathsf{v}_4^{\\top}. \\] Note that we do not use the vector \\(\\mathsf{v}_5\\). So R didn’t calculate it for us (to save computation time). If \\(A\\) had been a \\(10 \\times 4\\) matrix, this would be a pretty good idea: why find 6 vectors that we don’t need? And don’t worry: we can easily extend the columns of decomp$u into an orthonormal basis for \\(\\mathbb{R}^5\\). One simple way is to find a basis for the orthogonal complement of \\(\\mbox{span}(\\mathsf{v}_1\\mathsf{v}_2,\\mathsf{v}_3,\\mathsf{v}_4)\\). We could do this by manually by finding an orthogonal basis for the nullspace of t(decomp$v). If you take MATH 365 Computational Linear Algebra, you will learn more about the \\(QR\\)-decomposition. That gives a fast way to fill in the missing vectors if you need them. Pro Tip: When you use svd on a rectangular matrix, just remember that some of the singular vectors are missing. If \\(m &gt; n\\) then there are \\(m-n\\) missing columns of \\(U\\), all lying in \\(\\mbox{Nul}(A^{\\top})\\). If \\(n &gt; m\\) then there are \\(n-m\\) missing columns of \\(V\\), all lying in \\(\\mbox{Nul(A)}\\). 27.1.1.2 The Four Fundamental Subspaces of \\(A\\) Keeping this is mind, let’s characterize the four fundamental subspaces of our example \\(5 \\times 4\\) matrix \\(A.\\) The matrix \\(A\\) has rank \\(3\\) because there are three nonzero signular values. \\(\\mbox{Row}(A)\\) is 3-dimensional with basis decomp$v[,1:3] \\(\\mbox{Nul}(A)\\) is 2-dimensional because \\(\\sigma_4=0\\) and there is \\(5-4=1\\) missing left right singular vector. We can use decomp$v[,4] as one basis vector. We can find the remaining \\(5-4=1\\) vector by finding the nullspace of t(decomp$v). \\(\\mbox{Col}(A)\\) is 3-dimensional with basis decomp$u[,1:3] \\(\\mbox{Nul}(A^{\\top})\\) is 1-dimensional. The vector decomp$u[,4] is a basis. 27.1.2 Your Turn: SVD of a Square Matrix Characterize the four subspaces of this \\(5x5\\) matrix. Note that since \\(A\\) is square, the matrices \\(U\\) and \\(V\\) will both contain a full basis for \\(\\mathbb{R}^5\\). (A = cbind(c(-3,5,-3,6,-1),c(-1,-2,-1,2,-4),c(9,-4,9,-18,14),c(3,-4,3,-6,3),c(-2,11,-2,4,11))) ## [,1] [,2] [,3] [,4] [,5] ## [1,] -3 -1 9 3 -2 ## [2,] 5 -2 -4 -4 11 ## [3,] -3 -1 9 3 -2 ## [4,] 6 2 -18 -6 4 ## [5,] -1 -4 14 3 11 27.1.3 Your Turn: SVD of a Tall Matrix Characterize the four subspaces of this \\(6x4\\) matrix. This time around, we will be missing \\(6-4=2\\) of the left singular vectors that we need to extend `svd(A)\\(u\\) into a basis of \\(\\mathbb{R}^6\\). These two singular vectors complete the basis for \\(\\mbox{Nul(A^{\\top})}\\). x = c(1,2,3,4,5,6) (A = cbind(x^0, x^1, x^2, x^3)) ## [,1] [,2] [,3] [,4] ## [1,] 1 1 1 1 ## [2,] 1 2 4 8 ## [3,] 1 3 9 27 ## [4,] 1 4 16 64 ## [5,] 1 5 25 125 ## [6,] 1 6 36 216 27.2 Image Compression 27.2.1 Converting a JPEG image into a Matrix You may need to install the jpeg and raster packages. Let’s find out. Click on the ‘Packages’ tab in the lower right window Either search or scroll to see if the the jpeg and raster are there. If so, then click the checkbox to load the package into memory. If the package is missing, then uncomment and run the folllowing code chunk #install.packages(&#39;jpeg&#39;) #install.packages(&#39;raster&#39;) You’ve probably heard of JPEG image files. The jpeg package will allow us to import those images into R. The JPEG format uses image compression. So we need to turn them into a raster (bitmap) image which is a rectangular grid of pixels or dots. In other words a raster image is a matrix. The package raster will do this conversion for us. library(jpeg) library(raster) The following defines two helper functions get_image(filename) reads in a JPEG file and convert it into a raster image. plot_image(img) creates a plot of the raster image # converts a JPEG file into a raster image (a numerical matrix) # if the JPEG is a color image, it is converted to black and white. get_image &lt;- function(filename) { # read the jpeg file img = readJPEG(readBin(filename,&quot;raw&quot;,1e6)) img.dim = dim(img) # if this is a color image, we need to turn it into a grayscale image img = img[,,1]+img[,,2]+img[,,3] img &lt;- img/max(img) img.dim = dim(img) return (img) } plot_image = function(img,...) { plot(2:1, type=&#39;n&#39;,xlab=&quot; &quot;,ylab= &quot; &quot;,...) rasterImage(img, 1.0, 1.0, 2.0, 2.0) } Let start by reading in a picture of a tartan. where.tartan = &quot;https://upload.wikimedia.org/wikipedia/commons/e/ec/Burberry.jpg&quot; img = get_image(where.tartan) #plot_image(img,main=&quot;Image&quot;) dim(img) ## [1] 335 333 prod(dim(img)) # prod(vec) = product of the entries of vec ## [1] 111555 Each entry of img is a value in \\([0,1]\\). This is the grayscale value of a single pixel: value 0 corresponds to white and value 1 corresponds to black. The matrix img is a \\(335 \\times 333\\) matrix. So to store the image, we need to store \\(111,555\\) floating point numbers (!). You can see compression methods are essential in practice. 27.2.2 SVD of a Raster Image The img variable is just a matrix. So we can find its singular value decomposition. We find that there are some large gaps in the singular values. decomp =svd(img)$d #plot(decomp,pch=19,cex=.5,col=&#39;blue&#39;) #plot(decomp,pch=19,cex=.5,col=&#39;blue&#39;,ylim=c(0,5)) decomp[1:10] ## [1] 243.100715 23.459676 23.070452 22.687676 22.469706 ## [6] 11.317300 1.732057 1.647588 1.195523 1.161641 27.2.3 SVD Approximation of the Image Now let’s create an SVD approximation of the image. Here is some helper code for us to use. The functions that you will call are plot_svd_approx(img, k): create the spectral decomposition corresponding to the \\(k\\) largest singular values in the spectral decomposition. get_svd_approx_error(img, k): reports the average pixel error of the approximation. # returns the spectral decomposition matrix for the first k singular values svd_approx = function(A,k = floor(1/2*min(nrow(A),ncol(A)))) { decomp = svd(A) sings = decomp$d U = decomp$u V = decomp$v if(k==1) D=matrix(sings[1],nrow=1,ncol=1) else D=diag(sings[1:k]) M=U[,1:k]%*%D%*%t(V[,1:k]) return(M) } # gets the svd approximation of the image get_svd_approx_img &lt;- function(img,k) { approxIm = svd_approx(img,k) approxIm[approxIm&lt;0] = 0 approxIm[approxIm&gt;1] = 1 return (approxIm) } # returns the average pixel error for the svd approximation of the image get_svd_approx_error &lt;- function(img, k) { approxImg = get_svd_approx_img(img,k) return = mean(abs(img - approxImg)) } # plots the SVD approximation of the image plot_svd_approx=function(img,k){ approxIm = get_svd_approx_img(img,k) plot(1:2, type=&#39;n&#39;) rasterImage(approxIm, 1.0, 1.0, 2.0, 2.0) } And here we show the singular value approximation with increasing numbers of singular values: #plot_svd_approx(img,1) get_svd_approx_error(img,1) 27.2.4 SVD of a Lighthouse Now let’s see how SVD does with this picture of a lighthouse. Use the appropriate code above to explore the SVD approximations of this lighthouse. How big must \\(k\\) be in order to get an okay approximation? to get an approximation that you can’t distinguish from the original? where.lighthouse = &quot;https://images.unsplash.com/6/lighthouse.jpg&quot; 27.2.5 Your Turn Here are some JPEG files available on the web for you to try. Or you can use an image of your own choice! Have fun! # some jpeg images available on the web where.cameraman = &quot;https://www.macalester.edu/~dshuman1/data/cameraman_small.jpg&quot; where.tiger = &quot;https://i.pinimg.com/originals/f2/b5/0b/f2b50b1cbdb7cd16fef50f5641d41e77.jpg&quot; where.flower= &quot;https://www.amylamb.com/wp-content/uploads/2013/04/Gerbera-320x320.jpg&quot; where.pattern = &quot;https://previews.123rf.com/images/noppanun/noppanun1411/noppanun141100046/33287656-black-and-white-geometric-seamless-pattern-with-triangle-and-trapezoid-abstract-background-vector-ep.jpg&quot; "],["quiz-1-review.html", "Vector 28 Quiz 1 Review 28.1 Overview 28.2 Practice Problems 28.3 Solutions to Practice Problems", " Vector 28 Quiz 1 Review 28.1 Overview Our first quiz covers sections 1.1 - 1.9 in Lay’s book. This corresponds to Problem Sets 1,2,3. 28.1.1 Vocabulary and Concepts You should understand these concepts and be able to read and use these terms correctly: elementary row operations REF and RREF pivot position linear combination span linear independence homogeneous and nonhomogeneous equations Understand the geometric relationship between the solutions to \\(Ax = 0\\) and \\(Ax=b\\) Understand Theorem 4 in Section 1.4 which says that the following are equivalent (they are all true or are all false) for an \\(m \\times n\\) matrix \\(A\\) For each \\(b\\) in \\(\\mathbb{R}^m\\), \\(A x = b\\) has a solution Each \\(b\\) in \\(\\mathbb{R}^m\\) is a linear combination of the columns of \\(A\\) The columns of \\(A\\) span \\(\\mathbb{R}^m\\) \\(A\\) has a pivot in every row. The columns of \\(A\\) are linearly independent if and only if \\(Ax=0\\) only has the trivial solution Understand Theorem 8 in Section 1.7: if you have more than \\(n\\) vectors in \\(\\mathbb{R}^n\\) they must be linearly dependent. linear transformation one-to-one and onto Understand Theorem 12 in Section 1.9 which states that \\(T(x)=Ax\\) is onto if and only if the column of \\(A\\) span \\(\\mathbb{R}^m\\) \\(T(x)=Ax\\) is one-to-one if and only if the columns of \\(A\\) are linearly independent. 28.1.2 Skills You should be able to perform these linear algebra tasks. Identify linear systems from nonlinear systems Make the augmented matrix from a set of equations Row reduce a system of equations into Row Echelon Form (REF) and Reduced Row Echelon Form (RREF) Write the solution set to \\(Ax=b\\) as a parametric vector equation. Convert back and forth between systems of equations, vector equations, and matrix equations. Compute the matrix-vector product \\(Ax\\) Determine whether a set of vectors is linearly dependent or independent Find a dependence relation among a set of vectors Decide if a set of vectors span \\(\\mathbb{R}^n\\) Manipulate matrix vector products using: \\(A(x + y) = Ax + Ay\\) and \\(A(c x) = c A x\\) Determine whether a linear transformation is one-to-one and/or onto Find the standard matrix for a linear transformation Give geometric descriptions of linear transformations \\(T: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2\\) 28.2 Practice Problems 28.2.1 I have performed some row operations below for you on a matrix \\(A\\). Write out the complete set of solutions to \\(A \\mathsf{x} = {\\bf 0}\\). \\[ A= \\begin{bmatrix} 1&amp; 2&amp; 0&amp; 2&amp; 0&amp; -1 \\\\ 1&amp; 2&amp; 1&amp; 1&amp; 0&amp; -2 \\\\ 2&amp; 4&amp; -2&amp; 6&amp; 1&amp; 2 \\\\ 1&amp; 2&amp; 0&amp; 2&amp; -1&amp; -3 \\\\ \\end{bmatrix} \\longrightarrow \\begin{bmatrix} 1&amp; 2&amp; 0&amp; 2&amp; 0&amp; -1\\\\ 0&amp; 0&amp; 1&amp; -1&amp; 0&amp; -1\\\\ 0&amp; 0&amp; 0&amp; 0&amp; 1&amp; 2\\\\ 0&amp; 0&amp; 0&amp; 0&amp; 0&amp; 0\\\\ \\end{bmatrix} \\] 28.2.2 I have performed some row operations below for you on a matrix \\(B\\). \\[ B= \\begin{bmatrix} 1&amp; 1&amp; 0 \\\\ 0&amp; 1&amp; 1 \\\\ 2&amp; 1&amp; 2 \\\\ 1&amp; -1&amp; 1 \\\\ 2&amp; 3&amp; 1 \\\\ \\end{bmatrix} \\longrightarrow \\begin{bmatrix} 1&amp; 0&amp; 0 \\\\ 0&amp; 1&amp; 0 \\\\ 0&amp; 0&amp; 1 \\\\ 0&amp;0&amp;0 \\\\ 0&amp;0&amp;0 \\\\ \\end{bmatrix} \\] a. Describe the solutions to the equation \\(B \\mathsf{x} = {\\bf 0}\\). Fill in the boxes: the transformation \\(T(\\mathsf{x}) = B\\mathsf{x}\\) is a linear transformation from \\(\\mathbb{R}^{\\square}\\) to \\(\\mathbb{R}^{\\square}\\). 28.2.3 I want to know if it is possible to write \\(\\mathsf{w}\\) as a linear combination of the vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3\\) below. Write down, but do not solve, a matrix equation that would solve this problem. Your answer should be of the form \\(A \\mathsf{x} = \\mathsf{b}\\), where I can clearly see what \\(A, \\mathsf{x}\\), and \\(\\mathsf{b}\\) are. I should also be able to tell how many unknowns there are. \\[ \\mathsf{v}_1 = \\left[ \\begin{matrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ \\end{matrix}\\right] , \\quad \\mathsf{v}_2 = \\left[ \\begin{matrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ \\end{matrix}\\right] , \\quad \\mathsf{v}_3 = \\left[ \\begin{matrix} 1 \\\\ 1 \\\\ 0 \\\\ -2 \\\\ \\end{matrix}\\right] , \\quad \\mathsf{w} = \\left[ \\begin{matrix} 1 \\\\ -8 \\\\ -11 \\\\ -24 \\\\ \\end{matrix}\\right] . \\] 28.2.4 Describe all vectors that are not in the span of the columns of the matrix \\(A\\) below: \\[ A= \\begin{bmatrix} 1&amp; 2&amp; 4 \\\\ -3&amp; -5&amp; -11\\\\ 1&amp; 1&amp; 3 \\\\ \\end{bmatrix} \\] 28.2.5 The matrix below is \\(3 \\times 3\\) but the third column is missing. Add a nonzero third column so that the columns of \\(A\\) are linearly dependent and add a 3rd column so that the columns of \\(A\\) are linearly independent. Briefly describe your strategy. \\[ \\begin{bmatrix} 1&amp; 0 &amp; \\quad \\\\ 0&amp; 1&amp; \\quad \\\\ 2&amp; 2&amp; \\quad \\\\ \\end{bmatrix} \\qquad\\qquad \\begin{bmatrix} 1&amp; 0 &amp; \\quad \\\\ 0&amp; 1&amp; \\quad \\\\ 2&amp; 2&amp; \\quad \\\\ \\end{bmatrix} \\] 28.2.6 In each case below, find the matrix of the linear transformation that is described, if you believe that the matrix exists. Otherwise, demonstrate that the transformation is not linear. The transformation \\(T\\) is given by: \\[T \\left( \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\end{bmatrix}\\right) = \\begin{bmatrix} x_1 + x_2 \\\\ 2 x_1 \\\\ -x_2 \\\\\\end{bmatrix}. \\] The transformation \\(T\\) is given by: \\[T \\left( \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\right)= \\begin{bmatrix} x_1 + x_2 + x_3 \\\\ x_1 x_2 \\\\ -x_2 + 2 x_3 \\end{bmatrix}. \\] The transformation \\(L: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) sends the shaded region on the left to the the shaded region on the right such that \\(A\\) maps to \\(A\\), \\(B\\) maps to \\(B\\), \\(C\\) maps to \\(C\\), and \\(D\\) maps to \\(D\\). \\(\\qquad \\qquad\\) The transformation \\(R: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) sends the shaded region on the left to the the shaded region on the right such that \\(A\\) maps to \\(A\\), \\(B\\) maps to \\(B\\), \\(C\\) maps to \\(C\\), and \\(D\\) maps to \\(D\\). \\(\\qquad \\qquad\\) 28.2.7 Write the following systems of equations in vector and matrix form. \\[ \\begin{array} {ccccccccccc} 5 x_1 &amp;+&amp; 3 x_2 &amp;+&amp; x_3 &amp;+&amp; 11 x_4 &amp;-&amp; x_5 &amp;=&amp; 10 \\\\ 4 x_1 &amp;+&amp; x_2 &amp;+&amp; 3 x_3 &amp;+&amp; 2 x_4 &amp;+&amp; 6 x_5 &amp;=&amp; 11 \\\\ - x_1 &amp;+&amp; 3 x_2 &amp;-&amp; 2 x_3 &amp;+&amp; x_4 &amp;+&amp; 6 x_5 &amp;=&amp; 12 \\\\ \\end{array} \\] 28.2.8 Let \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4\\) be the vectors in the columns of the matrix \\(A\\) below. \\[ A = \\left[ \\begin{array}{cccc} 1 &amp; 1 &amp; 3 &amp; 1 \\\\ 2 &amp; 0 &amp; 2 &amp; 3 \\\\ 1 &amp; 1 &amp; 3 &amp; 1 \\\\ -1 &amp; 0 &amp; -1 &amp; 0 \\end{array} \\right] \\longrightarrow \\left[ \\begin{array}{cccc} 1 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array} \\right] \\] a. Are the vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4\\) linear independent or dependent? If they are linearly dependent, please give a dependence relation among them. b. Describe the span of the vectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_4\\) inside of \\(\\mathbb{R}^4\\)? 28.2.9 Find a solution to \\(A \\mathsf{x}=0\\) that no one else in the class has. \\[ A = \\begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 1 &amp; 4 \\\\ 2 &amp; 0 &amp; 4 &amp; 1 &amp; 4 \\\\ 1 &amp; 1 &amp; 1 &amp; 1 &amp; 4 \\\\ 1 &amp; 0 &amp; 2 &amp; 1 &amp; 3 \\end{bmatrix} \\longrightarrow \\begin{bmatrix} 1 &amp; 0 &amp; 2 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; -1 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 2 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix} \\] 28.2.10 Let \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) be a linear transformation. Let \\(\\mathsf{u}, \\mathsf{v}, \\mathsf{w}\\) be a linearly independent set in \\(\\mathbb{R}^n\\), and suppose that \\(T(\\mathsf{u}), T(\\mathsf{v}), T(\\mathsf{w})\\) is a linearly dependent set in \\(\\mathbb{R}^m\\). State precisely what it means for \\(T(\\mathsf{u}), T(\\mathsf{v}), T(\\mathsf{w})\\) to be linearly dependent. Use the properties of a linear transformation to demonstrate that \\(T(\\mathsf{x}) = \\mathbf{0}\\) has a nontrivial solution. 28.3 Solutions to Practice Problems 28.3.1 The parametric vector form of the solution is \\[\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6 \\\\ \\end{bmatrix} = s \\begin{bmatrix} -2 \\\\ 1 \\\\ 0 \\\\ 0 \\\\0 \\\\ 0 \\end{bmatrix} + t \\begin{bmatrix} -2 \\\\ 0 \\\\ 1 \\\\ 1 \\\\0 \\\\ 0 \\end{bmatrix} u \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\-2 \\\\ 1 \\end{bmatrix}\\] 28.3.2 There is one solution: \\(\\mathsf{x} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\\). The transformation \\(T(\\mathsf{x}) = B\\mathsf{x}\\) is a linear transformation from \\(\\mathbb{R}^{3}\\) to \\(\\mathbb{R}^{5}\\). 28.3.3 \\[ \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 2 &amp; 0 &amp; 1 \\\\ 3 &amp; 1 &amp; 0 \\\\ 4 &amp; 0 &amp; -2 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ -8 \\\\ -11 \\\\ -24 \\end{bmatrix} \\] 28.3.4 We want to find all target vectors \\(\\mathsf{b}\\) such that \\(A \\mathsf{x} = \\mathsf{b}\\) is inconsistent. So we want the augmented matrix \\(\\begin{bmatrix} A \\,| \\, b \\end{bmatrix}\\) to have a pivot in the last column. \\[ \\left[ \\begin{array}{ccc|c} 1&amp; 2&amp; 4 &amp; b_1 \\\\ -3&amp; -5&amp; -11 &amp; b_2\\\\ 1&amp; 1&amp; 3 &amp; b_3 \\\\ \\end{array} \\right] \\longrightarrow \\left[ \\begin{array}{ccc|c} 1&amp; 2&amp; 4 &amp; b_1 \\\\ 0&amp; 1&amp; 1 &amp; 3b_1 +b_2\\\\ 0&amp; -1&amp; -1 &amp; -b_1+b_3 \\\\ \\end{array} \\right] \\longrightarrow \\left[ \\begin{array}{ccc|c} 1&amp; 2&amp; 4 &amp; b_1 \\\\ 0&amp; 1&amp; 1 &amp; 3b_1 +b_2\\\\ 0&amp; 0&amp; 0 &amp; 2b_1+b_2+b_3 \\\\ \\end{array} \\right] \\] So the set of target vectors that are not in the span of the columns of \\(A\\) are the vectors \\[ \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix} \\qquad \\mbox{where} \\qquad 2b_1 + b_2 + b_3 \\neq 0. \\] 28.3.5 There are lots of right answers to this one. In my first matrix, I will just add the first two columns to get the thrid column. Then I will add one to one to the bottom right entry of the matrix. \\[ \\begin{bmatrix} 1&amp; 0 &amp; 1 \\\\ 0&amp; 1&amp; 1 \\\\ 2&amp; 2&amp; 4 \\\\ \\end{bmatrix} \\qquad\\qquad \\begin{bmatrix} 1&amp; 0 &amp; 1 \\\\ 0&amp; 1&amp; 1 \\\\ 2&amp; 2&amp; 5 \\\\ \\end{bmatrix} \\] 28.3.6 This is a linear transformation with \\[A = \\begin{bmatrix} 1 &amp; 1 \\\\ 2 &amp; 0 \\\\ 0 &amp; -1 \\end{bmatrix}.\\] This is not a linear transformation because \\[ 2 \\, T \\left( \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\right)= 2 \\begin{bmatrix} 3 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 2 \\\\ 2 \\end{bmatrix} \\quad \\mbox{while} \\quad T \\left( \\begin{bmatrix} 2 \\\\ 2 \\\\ 2 \\end{bmatrix} \\right)= 2 \\begin{bmatrix} 6 \\\\ 4 \\\\ 2 \\end{bmatrix}. \\] \\(A= \\begin{bmatrix} 1/2 &amp; 1/2 \\\\ 1/2 &amp; 1/2 \\end{bmatrix}\\) \\(A= \\begin{bmatrix} 0 &amp; -1 \\\\ -1 &amp; 0 \\end{bmatrix}\\) 28.3.7 Vector Form: \\[ x_1 \\begin{bmatrix} 5 \\\\ 4 \\\\ -1 \\end{bmatrix} + x_2 \\begin{bmatrix} 3 \\\\ 1 \\\\ 3 \\end{bmatrix} + x_3 \\begin{bmatrix} 1 \\\\ 3 \\\\ -2 \\end{bmatrix} + x_4 \\begin{bmatrix} 11 \\\\ 2 \\\\ 1 \\end{bmatrix} + x_5 \\begin{bmatrix} -1 \\\\ 6 \\\\ 6 \\end{bmatrix} = \\begin{bmatrix} 10 \\\\ 11 \\\\ 12 \\end{bmatrix} \\] Matrix Form: \\[ \\begin{bmatrix} 5 &amp; 3 &amp; 1 &amp; 11 &amp; -1 \\\\ 4 &amp; 1 &amp; 3 &amp; 2 &amp; 6 \\\\ -1 &amp; 3 &amp; -2&amp; 1 &amp; 6 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\end{bmatrix} = \\begin{bmatrix} 10 \\\\ 11 \\\\ 12 \\end{bmatrix} \\] 28.3.8 \\(-\\mathsf{v}_1 - 2\\mathsf{v}_2 + \\mathsf{v}_3 + 0 \\mathsf{v}_4 = 0\\). \\(\\mathrm{span}(\\mathsf{v}_1,\\mathsf{v}_2,\\mathsf{v}_3,\\mathsf{v}_4)\\) looks like a copy of \\(\\mathbb{R}^3\\) sitting inside \\(\\mathbb{R}^4\\). In other words, is 3-dimensional subset of \\(\\mathbb{R}^4\\). 28.3.9 The general solution is \\[ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\end{bmatrix} = s \\begin{bmatrix} -2 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix} + t \\begin{bmatrix} -1 \\\\ -1 \\\\ 0 \\\\ -2 \\\\ 1 \\end{bmatrix}. \\] My solution is \\[77,083,679 \\begin{bmatrix} -2 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix} - 72,159,215 \\begin{bmatrix} -1 \\\\ -1 \\\\ 0 \\\\ -2 \\\\ 1 \\end{bmatrix}. \\] 28.3.10 There are scalars \\(c_1, c_2, c_3\\) not all zero such that \\(c_1 T(\\mathsf{u}) + c_2 T(\\mathsf{v}) + c_3 T(\\mathsf{w}) = \\mathbf{0}\\). We have \\[ \\mathbf{0} = c_1 T(\\mathsf{u}) + c_2 T(\\mathsf{v}) + c_3 T(\\mathsf{w}) = T(c_1 \\mathsf{u} + c_2 \\mathsf{v}+ c_3 \\mathsf{w}) \\] However, we know that \\(\\mathsf{u}, \\mathsf{v},\\mathsf{w}\\) are linearly independent, so \\[ c_1 \\mathsf{u} + c_2 \\mathsf{v}+ c_3 \\mathsf{w} \\neq \\mathbf{0}. \\] This is a nonzero vector that maps to the zero vector. Therefore \\(T\\) is not one-to-one. "],["quiz-2-review.html", "Vector 29 Quiz 2 Review 29.1 Overview 29.2 Practice Problems 29.3 Solutions to Practice Problems", " Vector 29 Quiz 2 Review 29.1 Overview Our second quiz covers sections 2.1 - 2.3 and 4.1-4.6 in Lay’s book. This corresponds to Problem Sets 4 and 5. The best way to study is to do practice problems. The Quiz will have calculation problems (like Edfinity) and more conceptual problems (like the problem sets). Here are some ways to practice: Make sure that you have mastered the Vocabulary, Skills and Concepts listed below. Look over the Edfinity homework assingments Do practice problems from the Edfinity Practice assignments. These allow you to “Practice Similar” by generating new variations of the same problem. Redo the Jamboard problems Try to resolve the Problem Sets and compare your answers to the solutions. Do the practice problems below. Compare your answers to the solutions. 29.1.1 Vocabulary and Concepts You should understand these concepts and be able to read and use these terms correctly: all of the Important Definitions found here. matrix inverses the Invertible Matrix Theorem homogeneous coordinates subspaces null space and column space of a matrix kernel and image of a linear transformation basis (span and linearly independent) coordinate vector with respect to a basis \\(\\mathcal{B}\\) change-of-coordinates matrix dimension 29.1.2 Skills You should be able to perform these linear algebra tasks. solve matrix algebra equations find a matrix inverse use homogeneous coordinates to transform and translate images in the plane. show that a subset is a subspace or demonstrate that it is not a subspace describe the null space and the column space determine if a vector is in a null space or column space find a basis of a subspace answer questions about the connections between all these ideas write short proofs of basic statements using the Important Definitions 29.2 Practice Problems 29.2.1 Here is a picture of some boats. The blue, yellow and gray boats are linear transformations of the red boat (using homogeneous coordinates). Find the \\(3 \\times 3\\) matrix corresponding to the linear transformation that creates: The shadow gray boat The fast blue boat The funny yellow boat. For your convenience, here is the code to draw the red boat as well as some commented code that you can adapt to create the other boats. &#39;```{r,fig.height=4,fig.width=4} # the red boat boat =cbind(c(0,0), c(-6,0), c(-7,3), c(-1,3), c(-1,5), c(-6,5), c(-1,10), c(-1,11), c(-.5,11), c(-.5,3), c(2,3) ) boat = rbind(boat,rep(1,11)) ##### update these matrices graymap = cbind(c(1,0,0), c(0,1,0),c(0,0,1)) bluemap = cbind(c(1,0,0), c(0,1,0),c(0,0,1)) yellowmap = cbind(c(1,0,0), c(0,1,0),c(0,0,1)) # plot all of the boats grayboat = graymap %*% boat blueboat = bluemap %*% boat yellowboat = yellowmap %*% boat plot(boat[1,],boat[2,],type=&quot;n&quot;,xlim=c(-16,16),ylim=c(-16,16),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-16:16, v=-16:16, col=&quot;gray&quot;, lty=&quot;dotted&quot;) polygon(grayboat[1,], grayboat[2,], col = &quot;gray&quot;, border = &quot;gray&quot;) polygon(blueboat[1,], blueboat[2,], col = &quot;blue&quot;, border = &quot;gray&quot;) polygon(yellowboat[1,], yellowboat[2,], col = &quot;yellow&quot;, border = &quot;gray&quot;) polygon(boat[1,], boat[2,], col = &quot;red&quot;, border = &quot;blue&quot;) 29.2.2 Find the inverse of the matrix \\[ \\left[ \\begin{array}{rrr} 1 &amp; -2 &amp; 2 \\\\ 1 &amp; 0 &amp; 0 \\\\ 2 &amp;-4 &amp; 5 \\end{array} \\right] \\] 29.2.3 Suppose that a linear transformation \\(T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) has the property that \\(T(\\mathsf{u}) = T(\\mathsf{v})\\) for some pair of distinct vectors \\(\\mathsf{u}, \\mathsf{v} \\in \\mathbb{R}^n\\). Can \\(T\\) map \\(\\mathbb{R}^n\\) onto \\(\\mathbb{R}^n\\)? Why or why not? 29.2.4 Let \\(U\\) and \\(W\\) be subspaces of a vector space \\(\\mathbb{R}^n\\). Prove or disprove the following statements a. \\(U \\cap W = \\{ \\mathsf{v} \\in \\mathbb{R}^n \\mid \\mathsf{v} \\in U \\mbox{ and } \\mathsf{v} \\in W \\}\\) is a subspace b. \\(U \\cup W = \\{ \\mathsf{v} \\in \\mathbb{R}^n \\mid \\mathsf{v} \\in U \\mbox{ or } \\mathsf{v} \\in W \\}\\) is a subspace c. \\(U+W = \\{\\mathsf{u} + \\mathsf{w} \\mid \\mathsf{u} \\in U \\mbox{ and } \\mathsf{w} \\in W \\}\\) is a subspace 29.2.5 Let \\(T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) be a linear transformation. Suppose that \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is one-to-one. Prove that if \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3\\) are linearly independent, then \\(T(\\mathsf{v}_1), T(\\mathsf{v}_2), T(\\mathsf{v}_3)\\) are linearly independent. Suppose that \\(T: \\mathbb{R}^n \\to \\mathbb{R}^m\\) is onto. Prove that if \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3\\) span \\(\\mathbb{R}^n\\) then \\(T(\\mathsf{v}_1), T(\\mathsf{v}_2), T(\\mathsf{v}_3)\\) span \\(\\mathbb{R}^m\\). 29.2.6 I have performed some row operations below for you on a matrix \\(A\\). Find a basis for the column space and the null space of \\(A\\). \\[ A= \\left[ \\begin{matrix} 1&amp; 2&amp; 0&amp; 2&amp; 0&amp; -1 \\\\ 1&amp; 2&amp; 1&amp; 1&amp; 0&amp; -2 \\\\ 2&amp; 4&amp; -2&amp; 6&amp; 1&amp; 2 \\\\ 1&amp; 2&amp; 0&amp; 2&amp; -1&amp; -3 \\\\ \\end{matrix}\\right] \\longrightarrow \\left[ \\begin{matrix} 1&amp; 2&amp; 0&amp; 2&amp; 0&amp; -1\\\\ 0&amp; 0&amp; 1&amp; -1&amp; 0&amp; -1\\\\ 0&amp; 0&amp; 0&amp; 0&amp; 1&amp; 2\\\\ 0&amp; 0&amp; 0&amp; 0&amp; 0&amp; 0\\\\ \\end{matrix}\\right] \\] 29.2.7 Consider the matrix \\[ A = \\left[ \\begin{array}{cccc} 1 &amp; 5 &amp; 2 &amp; -4 \\\\ 3 &amp; 10 &amp; 2 &amp; 8 \\\\ 4 &amp; 15 &amp; 4 &amp; 4 \\end{array} \\right] \\] Find a basis for \\(\\mathrm{Col}(A)\\). Find a basis for \\(\\mathrm{Nul}(A)\\). 29.2.8 Are the vectors in \\({\\mathcal B}\\) a basis of \\(\\mathbb{R}^3\\)? If not, find a basis of \\(\\mathbb{R}^3\\) that consists of as many of the vectors from \\({\\mathcal B}\\) as is possible. Explain your reasoning. \\[ \\mathcal{B}=\\left\\{ \\begin{bmatrix} 1 \\\\ -1 \\\\ -2 \\end{bmatrix},\\begin{bmatrix} 2 \\\\ -1 \\\\ 1 \\end{bmatrix},\\begin{bmatrix} -1 \\\\ -1 \\\\ -8 \\end{bmatrix} \\right\\} \\] 29.2.9 I have the vectors below: \\[ \\begin{bmatrix} 5\\\\ 4\\\\ 3\\\\ 1\\\\ 2 \\end{bmatrix}, \\begin{bmatrix} 4\\\\ 4\\\\ 3\\\\ 1\\\\ 2 \\end{bmatrix}, \\begin{bmatrix} 1\\\\ 1\\\\ 1\\\\ 1\\\\ 1\\end{bmatrix}. \\] I know that they do not span \\(\\mathbb{R}^5\\), but I want to extend them to a basis of \\(\\mathbb{R}^5\\) by adding some vectors to the set. I created the matrix below and row reduced. Give a basis for \\(\\mathbb{R}^5\\) that uses my three vectors and explain why this method works in general. \\[ \\left[ \\begin{array}{cccccccc} 5 &amp; 4 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 4 &amp; 4 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 3 &amp; 3 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 2 &amp; 2 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ \\end{array} \\right] \\rightarrow \\left[ \\begin{array}{cccccccc} 1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 2 &amp; -3 \\\\ 0 &amp; 1 &amp; 0 &amp; -1 &amp; 0 &amp; 0 &amp; -3 &amp; 4 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 2 &amp; -1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 2 &amp; -3 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; -2 \\\\ \\end{array} \\right] \\] 29.2.10 Find the coordinates of \\(\\mathsf{w}\\) in the standard basis and of \\(\\mathsf{v}\\) in the \\(\\mathcal{B}\\)-basis. \\[ \\mathcal{B} = \\left\\{ \\mathsf{v}_1=\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\mathsf{v}_2=\\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\mathsf{v}_3=\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\mathsf{v}_4=\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix} \\right\\}, \\qquad \\mathsf{w} = \\begin{bmatrix} 3 \\\\ -2 \\\\ 0 \\\\ -1 \\end{bmatrix}_{\\mathcal{B}}, \\qquad \\mathsf{v} = \\begin{bmatrix} 10 \\\\ 9 \\\\ 7 \\\\ 4 \\end{bmatrix}_{\\mathcal{S}} \\] 29.2.11 The subspace \\(S \\subset \\mathbb{R}^5\\) is given by \\[ \\mathsf{S} = \\mathsf{span} \\left( \\begin{bmatrix}1\\\\ 1\\\\ 0\\\\ -1\\\\ 2 \\end{bmatrix}, \\begin{bmatrix} 0\\\\ 1\\\\ 1\\\\ 1\\\\ 1 \\end{bmatrix}, \\begin{bmatrix} 3\\\\ 1\\\\ -2\\\\ -5\\\\ 4 \\end{bmatrix}, \\begin{bmatrix} 1\\\\ 0\\\\ 1\\\\ 0\\\\ 1 \\end{bmatrix}, \\begin{bmatrix} 2\\\\ -1\\\\ -1\\\\ -3\\\\ 1 \\end{bmatrix}, \\right)\\] Use the following matrix to find a basis for \\(S\\). What is the dimension of \\(S\\)? \\[ A=\\left[ \\begin{array}{ccccc} 1 &amp; 0 &amp; 3 &amp; 1 &amp; 2 \\\\ 1 &amp; 1 &amp; 1 &amp; 0 &amp; -1 \\\\ 0 &amp; 1 &amp; -2 &amp; 1 &amp; -1 \\\\ -1 &amp; 1 &amp; -5 &amp; 0 &amp; -3 \\\\ 2 &amp; 1 &amp; 4 &amp; 1 &amp; 1 \\\\ \\end{array} \\right] \\rightarrow \\left[ \\begin{array}{ccccc} 1 &amp; 0 &amp; 3 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; -2 &amp; 0 &amp; -2 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\] Find a basis for \\(\\mathrm{Nul}(A)\\). What is the dimension of this nullspace? 29.2.12 A \\(6 \\times 8\\) matrix \\(A\\) contains 5 pivots. For each of \\(\\mathrm{Col}(A)\\) and \\(\\mathrm{Nul}(A)\\), Determine the dimension of the subspace, Indicate whether it is subspace of \\(\\mathbb{R}^6\\) or \\(\\mathbb{R}^8\\), and Decide how you would find a basis of the subspace. 29.3 Solutions to Practice Problems 29.3.1 Here are the matrices to make the boats. Shadow gray boat: \\[ \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; -1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] Fast blue boat: \\[ \\begin{bmatrix} 1 &amp; -0.25 &amp; 13 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] Funny yellow boat: \\[ \\begin{bmatrix} 0.35 &amp; -0.35 &amp; -3 \\\\ 0.35 &amp; 0.35 &amp; 8 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] And here is the updated code &#39;```{r,fig.height=4,fig.width=4,echo=TRUE} # the red boat boat =cbind(c(0,0), c(-6,0), c(-7,3), c(-1,3), c(-1,5), c(-6,5), c(-1,10), c(-1,11), c(-.5,11), c(-.5,3), c(2,3) ) boat = rbind(boat,rep(1,11)) # updated mappings graymap = cbind(c(1,0,0), c(0,-1,0),c(0,0,1)) bluemap = cbind(c(1,0,0), c(0,1,0),c(13,0,1)) %*% cbind(c(1,0,0), c(-.25,1,0),c(0,0,1)) t=pi/4 yellowmap = cbind(c(1,0,0), c(0,1,0),c(-3,8,1)) %*% cbind(c(cos(t),sin(t),0), c(-sin(t),cos(t),0),c(0,0,1)) %*% cbind(c(1/2,0,0),c(0,1/2,0),c(0,0,1)) # plot all of the boats grayboat = graymap %*% boat blueboat = bluemap %*% boat yellowboat = yellowmap %*% boat plot(boat[1,],boat[2,],type=&quot;n&quot;,xlim=c(-16,16),ylim=c(-16,16),xlab=&quot;x&quot;,ylab=&quot;y&quot;) abline(h=-16:16, v=-16:16, col=&quot;gray&quot;, lty=&quot;dotted&quot;) polygon(grayboat[1,], grayboat[2,], col = &quot;gray&quot;, border = &quot;gray&quot;) polygon(blueboat[1,], blueboat[2,], col = &quot;blue&quot;, border = &quot;gray&quot;) polygon(yellowboat[1,], yellowboat[2,], col = &quot;yellow&quot;, border = &quot;gray&quot;) polygon(boat[1,], boat[2,], col = &quot;red&quot;, border = &quot;blue&quot;) 29.3.2 The inverse is \\[ \\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ -5/2 &amp; 1/2 &amp;1 \\\\ -2 &amp; 0 &amp; 1 \\end{bmatrix} \\] require(pracma) A = cbind(c(1,1,2),c(-2,0,-4),c(2,0,5),c(1,0,0),c(0,1,0),c(0,0,1)) rref(A) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 0 0 0.0 1.0 0 ## [2,] 0 1 0 -2.5 0.5 1 ## [3,] 0 0 1 -2.0 0.0 1 29.3.3 No \\(T\\) cannot be an onto mapping by the Invertible Matrix Theorem. Since \\(T\\) is not one-to-one, the mapping cannot be onto. 29.3.4 True Since \\(U\\) and \\(W\\) are subspaces, we know that \\(\\mathbb{0} \\in U\\) and \\(\\mathbb{0} \\in W\\). Therefore \\(\\mathbb{0} \\in U \\cap W\\). Let \\(\\mathsf{v}_1 \\in U \\cap W\\) and \\(\\mathsf{v}_2 \\in U \\cap W\\). We know that \\(\\mathsf{v}_1 \\in U\\) and \\(\\mathsf{v}_2 \\in U\\). Since \\(U\\) is a subspace, we have \\(\\mathsf{v}_1 + \\mathsf{v}_2 \\in U\\). We know that \\(\\mathsf{v}_1 \\in W\\) and \\(\\mathsf{v}_2 \\in W\\). Since \\(W\\) is a subspace, we have \\(\\mathsf{v}_1 + \\mathsf{v}_2 \\in W\\). Therefore \\(\\mathsf{v}_1 + \\mathsf{v}_2 \\in U \\cap W\\). Let \\(\\mathsf{v} \\in U \\cap W\\) and \\(c \\in \\mathbb{R}\\). We know that \\(\\mathsf{v} \\in U\\) and \\(c \\in R\\). Since \\(U\\) is a subspace, we have \\(c \\mathsf{v} \\in U\\). We know that \\(\\mathsf{v} \\in W\\) and \\(c \\in R\\). Since \\(W\\) is a subspace, we have \\(c \\mathsf{v} \\in W\\). Therefore \\(c \\mathsf{v} \\in U \\cap W\\). False. Here is an example that shows this is not always true. Let \\(V= \\mathbb{R}^2\\), \\(U = \\{ { x \\choose 0} \\mid x \\in \\mathbb{R} \\}\\) and \\(W= \\{ { 0 \\choose y} \\mid y \\in \\mathbb{R} \\}\\). The set \\(U \\cup W\\) is not closed under addition. For example, \\({1 \\choose 0} + {0 \\choose 1} = { 1 \\choose 1} \\notin U \\cup W\\). True. Since \\(U\\) and \\(W\\) are subspaces, we know that \\(\\mathbb{0} \\in U\\) and \\(\\mathbb{0} \\in W\\). Therefore \\(\\mathbb{0} = \\mathbb{0} + \\mathbb{0} \\in U + W\\). Let \\(\\mathsf{u}_1 + \\mathsf{w}_1 \\in U + W\\) and \\(\\mathsf{u}_1 + \\mathsf{w}_2 \\in U + W\\), where \\(\\mathsf{u}_1, \\mathsf{u}_2 \\in U\\) and \\(\\mathsf{w}_1, \\mathsf{w}_2 \\in W\\). Then \\[ (\\mathsf{u}_1 + \\mathsf{w}_1) + (\\mathsf{u}_2 + \\mathsf{w}_2) = (\\mathsf{u}_1 + \\mathsf{u}_2) + (\\mathsf{w}_1 + \\mathsf{w}_2) \\] and \\(\\mathsf{u}_3 = (\\mathsf{u}_1 + \\mathsf{u}_2) \\in U\\) (because \\(U\\) is a subspace) and \\(\\mathsf{w}_3 = (\\mathsf{w}_1 + \\mathsf{w}_2) \\in W\\) (because \\(W\\) is a subspace). Therefore \\((\\mathsf{u}_1 + \\mathsf{v}_1) + (\\mathsf{u}_2 + \\mathsf{w}_2) = \\mathsf{u}_3 + \\mathsf{w}_3 \\in U + W\\). Let \\(\\mathsf{u} + \\mathsf{w} \\in U + W\\) and \\(c \\in \\mathbb{R}\\). Then \\(c(\\mathsf{u} + \\mathsf{w}) = c \\mathsf{u} + c \\mathsf{w}\\). We know that \\(c \\mathsf{u} \\in U\\) (since \\(U\\) is a subspace) and \\(c \\mathsf{w} \\in W\\) (since \\(W\\) is a subspace). Therefore \\(c(\\mathsf{u} + \\mathsf{w}) = c \\mathsf{u} + c \\mathsf{w} \\in U+W\\). 29.3.5 Suppose that \\(c_1 T(\\mathsf{v}_1) + c_2 T(\\mathsf{v}_2) + c_3 T(\\mathsf{v}_3) = 0\\). We must show that \\(c_1 = c_2 = c_3 = 0\\). Since \\(T\\) is a linear transformation, this means that \\(T(c_1 \\mathsf{v}_1+ c_2 \\mathsf{v}_2 + c_3 \\mathsf{v}_3 )= 0\\). Since \\(T\\) is one-to-one and \\(T(\\mathbf{0}) = \\mathbf{0}\\), we must have \\(c_1 \\mathsf{v}_1+ c_2 \\mathsf{v}_2 + c_3 \\mathsf{v}_3 = \\mathbf{0}\\). Because \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3\\) are linearly independent, this means that \\(c_1 = c_2 = c_3 = 0\\). This proves that \\(T(\\mathsf{v}_1), T(\\mathsf{v}_2), T(\\mathsf{v}_3)\\) are linearly independent. Given \\(\\mathsf{w} \\in W\\). We must show that there exist constants \\(c_1, c_2, c_3\\) such that \\(\\mathsf{w} = c_1 T(\\mathsf{v}_1) + c_2 T(\\mathsf{v}_2) + c_3 T(\\mathsf{v}_3)\\). Here we go! Since \\(T\\) is onto, we know that there exists \\(\\mathsf{v} \\in \\mathbb{R}^n\\) such that \\(T(\\mathsf{v}) = \\mathsf{w}\\). Since \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3\\) span \\(\\mathbb{R}^n\\), we know that there exist constants \\(c_1, c_2, c_3\\) such that \\(\\mathsf{v}= c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + c_3 \\mathsf{v}_k\\) Therefore \\[ \\mathsf{w} = T(\\mathsf{v})= T(c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2 + c_3 \\mathsf{v}_k) = c_1 T(\\mathsf{v}_1) + c_2 T(\\mathsf{v}_2) + c_3 T(\\mathsf{v}_k) \\] because \\(T\\) is a linear transformation. This proves that \\(T(\\mathsf{v}_1), T(\\mathsf{v}_2), T(\\mathsf{v}_3)\\) span \\(W\\). 29.3.6 A basis for \\(\\mathrm{Col}(A)\\) is \\[ \\begin{bmatrix} 1 \\\\1 \\\\ 2 \\\\ 1 \\end{bmatrix}, \\quad \\begin{bmatrix} 0 \\\\1 \\\\ -2 \\\\ 0 \\end{bmatrix}, \\quad \\begin{bmatrix} 0 \\\\0 \\\\ 1 \\\\ -1 \\end{bmatrix} \\] and a basis for \\(\\mathrm{Nul}(A)\\) is \\[ \\begin{bmatrix} -2 \\\\1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad \\begin{bmatrix} -2 \\\\0 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad \\begin{bmatrix} 1 \\\\0 \\\\ 1\\\\ 0 \\\\ -2 \\\\ 1 \\end{bmatrix}. \\] 29.3.7 Using RStudio we find: ## [,1] [,2] [,3] [,4] ## [1,] 1 5 2 -4 ## [2,] 3 10 2 8 ## [3,] 4 15 4 4 ## [,1] [,2] [,3] [,4] ## [1,] 1 0 -2.0 16 ## [2,] 0 1 0.8 -4 ## [3,] 0 0 0.0 0 A basis for \\(\\mathrm{Col}(A)\\) is \\[ \\begin{bmatrix} 1 \\\\ 3 \\\\ 4 \\end{bmatrix}, \\quad \\begin{bmatrix} 5 \\\\ 10 \\\\ 15 \\end{bmatrix}. \\] A basis for \\(\\mathrm{Nul}(A)\\) is \\[ \\begin{bmatrix} 2 \\\\ -0.8 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad \\begin{bmatrix} -16 \\\\ 4 \\\\ 0 \\\\ 1 \\end{bmatrix}. \\] 29.3.8 A = cbind(c(1,-1,-2),c(2,-1,1),c(-1,-1,-8)) A ## [,1] [,2] [,3] ## [1,] 1 2 -1 ## [2,] -1 -1 -1 ## [3,] -2 1 -8 rref(A) ## [,1] [,2] [,3] ## [1,] 1 0 3 ## [2,] 0 1 -2 ## [3,] 0 0 0 No they are not a basis. The corresponding matrix only has two pivots. Let’s add the three elementary vectors to create matrix \\(B\\) and then row reduce. B = cbind(A, c(1,0,0),c(0,1,0),c(0,0,1)) B ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 2 -1 1 0 0 ## [2,] -1 -1 -1 0 1 0 ## [3,] -2 1 -8 0 0 1 rref(B) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 0 3 0 -0.3333333 -0.3333333 ## [2,] 0 1 -2 0 -0.6666667 0.3333333 ## [3,] 0 0 0 1 1.6666667 -0.3333333 From this matrix, we can see that the vectors \\[ \\begin{bmatrix} 1 \\\\ -1 \\\\ -2 \\end{bmatrix}, \\quad \\begin{bmatrix} 2 \\\\ -1 \\\\ 1 \\end{bmatrix}, \\quad \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}. \\] are linearly independent because they correspond to the basis columns of \\(B\\). 29.3.9 A basis for \\(\\mathbb{R}^5\\) is \\[ \\begin{bmatrix} 5\\\\ 4\\\\ 3\\\\ 1\\\\ 2 \\end{bmatrix}, \\begin{bmatrix} 4\\\\ 4\\\\ 3\\\\ 1\\\\ 2 \\end{bmatrix}, \\begin{bmatrix} 1\\\\ 1\\\\ 1\\\\ 1\\\\ 1\\end{bmatrix}, \\begin{bmatrix} 0\\\\ 1\\\\ 0\\\\ 0\\\\ 0\\end{bmatrix}, \\begin{bmatrix} 0\\\\ 0\\\\ 1\\\\ 0\\\\ 0 \\end{bmatrix} \\] because these are basic columns in the given matrix. This will always work: place your desired vectors in the first columns. Since they are linearly independent, they will be basic columns. The remaining \\(n\\) elementary basis vectors span $. So the columns of the matrix span \\(\\mathbb{R}^n\\) and Gaussian Elimination will identify \\(n\\) pivots. The corresponding columns are a basis. 29.3.10 We need the matrices \\[ P_{\\cal B} = \\begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\qquad \\mbox{and} \\qquad P_{\\cal B}^{-1} = \\begin{bmatrix} 1 &amp; -1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; -1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp;-1 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] The desired coordinate vectors are \\[ \\mathsf{w} = \\begin{bmatrix} 0 \\\\ -3 \\\\ -1 \\\\ -1 \\end{bmatrix}_{\\mathcal{S}}, \\qquad \\mathsf{v} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\end{bmatrix}_{\\mathcal{B}} \\] Here is the R code for this solution: A = cbind(c(1,0,0,0),c(1,1,0,0),c(1,1,1,0),c(1,1,1,1)) Ainv = solve(A) A %*% c(3,-2,0,-1) ## [,1] ## [1,] 0 ## [2,] -3 ## [3,] -1 ## [4,] -1 Ainv %*% c(10,9,7,4) ## [,1] ## [1,] 1 ## [2,] 2 ## [3,] 3 ## [4,] 4 29.3.11 \\(\\dim(S) = 3\\) and a basis for \\(S\\) is \\[ \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\\\ -1 \\\\2 \\end{bmatrix}, \\quad \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\quad \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\1 \\end{bmatrix}. \\] \\(\\dim(\\mathrm{Nul}(A))=2\\) and a basis is \\[ \\begin{bmatrix} -3 \\\\ 2 \\\\ 1 \\\\ 0 \\\\0\\end{bmatrix}, \\quad \\begin{bmatrix} -1 \\\\ 2 \\\\ 0 \\\\ -1 \\\\1 \\end{bmatrix}. \\] 29.3.12 \\(\\mathrm{Col}(A)\\) has dimension 5, and it is a subspace of \\(\\mathbb{R}^6\\). You would find a basis by taking the pivot columns of \\(A\\). \\(\\mathrm{Nul}(A)\\) has dimension 3, and it is a subspace of \\(\\mathbb{R}^8\\). You would find a basis by finding the parametric solution to \\(A \\mathsf{x}= \\mathbb{0}\\). "],["quiz-3-review.html", "Vector 30 Quiz 3 Review 30.1 Overview 30.2 Practice Problems 30.3 Solutions to Practice Problems", " Vector 30 Quiz 3 Review 30.1 Overview Our third quiz covers sections 5.1-5.3 and 5.5-5.6 in Lay’s book. This corresponds to Problem Sets 6 and 7. The best way to study is to do practice problems. The Quiz will have calculation problems (like Edfinity) and more conceptual problems (like the problem sets). Here are some ways to practice: Make sure that you have mastered the Vocabulary, Skills and Concepts listed below. Look over the Edfinity homework assingments Do practice problems from the Edfinity Practice assignments. These allow you to “Practice Similar” by generating new variations of the same problem. Redo the Jamboard problems. Try to resolve the Problem Sets and compare your answers to the solutions. Do the practice problems below. Compare your answers to the solutions. 30.1.1 Vocabulary, Concepts and Skills See the Week 5-6 Learning Goals for the list of vocabulary, concepts and skills. 30.2 Practice Problems 30.2.1 Consider the \\(3 \\times 3\\) matrix \\[ A = \\left[ \\begin{array}{rrr} 2 &amp; -1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ -2 &amp; 5 &amp; -2 \\\\ \\end{array} \\right] \\] with characteristic equation \\[ p(\\lambda) = -(\\lambda -1)(\\lambda -2)(\\lambda +2). \\] Find the eigenvalues and corresponding eigenvectors for \\(A\\). 30.2.2 Let \\(A\\) be a \\(2 \\times 2\\) matrix. We view \\(A\\) as a linear transformation from \\(\\mathbb{R}^2\\) to \\(\\mathbb{R}^2\\). Describe the eigenvalues for each of the following types of matrices. \\(A\\) maps all of \\(\\mathbb{R}^2\\) onto a line through the origin in \\(\\mathbb{R}^2\\). \\(A\\) is a reflection of \\(\\mathbb{R}^2\\) about a line through the origin \\(A\\) is a reflection of \\(\\mathbb{R}^2\\) through the origin \\(A\\) is a horizontal shear 30.2.3 Below are the eigenvalues of four different \\(5 \\times 5\\) matrices. For each, decide if the matrix is invertible and if it is diagonalizable. Answer Yes, No or “Not enough information to determine this.” \\(A\\) has eigenvalues \\(\\lambda = -4, -3,0,1, 2\\) \\(B\\) has eigenvalues \\(\\lambda = -3, -1, 1, \\sqrt{2}, 8.\\) \\(C\\) has eigenvalues \\(\\lambda = 1, 2, 2, 7, 8.\\) \\(D\\) has eigenvalues \\(\\lambda = -1, 0, 3,3, 10\\) 30.2.4 Here the diagonalization of a matrix: \\[ \\mathsf{A}=\\left[ \\begin{array}{ccc} 5 &amp; 2 &amp; -1 \\\\ 2 &amp; 1 &amp; 0 \\\\ -1 &amp; 0 &amp; 1 \\\\ \\end{array} \\right] = \\left[ \\begin{array}{ccc} -5 &amp; 0 &amp; 1 \\\\ -2 &amp; 1 &amp; -2 \\\\ 1 &amp; 2 &amp; 1 \\\\ \\end{array} \\right] \\left[ \\begin{array}{ccc} 6 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right]\\left[ \\begin{array}{ccc} -\\frac{1}{6} &amp; -\\frac{1}{15} &amp; \\frac{1}{30} \\\\ 0 &amp; \\frac{1}{5} &amp; \\frac{2}{5} \\\\ \\frac{1}{6} &amp; -\\frac{1}{3} &amp; \\frac{1}{6} \\\\ \\end{array} \\right]. \\] Is the matrix \\(\\mathsf{A}\\) invertible? Find a nonzero vector in \\(\\mathrm{Nul}(\\mathsf{A})\\) if one exists. Find a steady-state vector \\(\\mathsf{v}\\) such that \\(\\mathsf{A} \\mathsf{v} = \\mathsf{v}\\) if one exists. Give the coordinates of \\(\\mathsf{v} = [1,2,3]^T\\) in the eigenbasis without row reductions. Find a formula for \\(\\mathsf{A}^{2020} \\mathsf{v}\\) if \\(\\mathsf{v} = [1,2,3]^T\\) in terms of the eigenbasis. 30.2.5 The eigensystem of matrix \\(A\\) is given below. It has complex eigenvalues. What angle does it rotate by? What factor does it scale by? \\[ \\begin{bmatrix} 3 &amp; -5 \\\\ 1 &amp; -1 \\end{bmatrix}, \\qquad \\lambda = 1 \\pm i, \\qquad v = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} \\pm \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} i. \\] 30.2.6 Using the matrix \\(B = = \\begin{bmatrix} .97 &amp; -.71 \\\\ .71 &amp; .97 \\end{bmatrix}\\) and the starting vector \\(\\mathsf{v} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\), I plotted the points \\[\\mathsf{v}, B \\mathsf{v}, B^2\\mathsf{v}, B^3 \\mathsf{v}, \\ldots.\\] I saw that these points are, roughly, going around in a circle. How many multiplications by \\(B\\) does it take to get back around to the positive \\(x\\)-axis? When I come full circle, am I closer to the origin, farther from the origin, or the same distance to the origin? 30.2.7 For each matrix below, decide if it is diagonalizable. You do not need to diagonalize the matrix (though you can!), but you must give a reason for why the matrix is or is not diagonalizable. \\(A = \\begin{bmatrix} 0 &amp; -4 &amp; 2 \\\\ 2 &amp; -4 &amp; -1 \\\\ -6 &amp; 4 &amp; 7 \\end{bmatrix}\\) has eigenvalues \\(4, -1, 0\\). \\(B = \\begin{bmatrix} 3 &amp; -1 &amp; 2 \\\\ -1 &amp; 3 &amp; 2 \\\\ 2&amp;2 &amp; 0 \\end{bmatrix}\\) has eigenvalues \\(4,4,-2\\). 30.2.8 Consider the matrix with eigenvalues and eigenvectors \\[ A = \\begin{bmatrix} 0.7 &amp; 0.2 \\\\ 0.3 &amp; 0.8 \\end{bmatrix} \\qquad \\begin{array}{cc} \\lambda_1 = 1 &amp; \\lambda_2 = .5 \\\\ \\mathsf{v}_1 = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} &amp; \\mathsf{v}_2 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} \\end{array} \\] Diagonalize \\(A\\). What can you say about \\(\\displaystyle{\\lim_{n \\to \\infty}} A^n\\)? Give a formula for \\(A^n \\mathsf{x}_0\\) if \\(\\mathsf{x}_0 = \\begin{bmatrix} 25 \\\\ 0 \\end{bmatrix}\\) in terms of the eigenbasis. What is \\(\\displaystyle{\\lim_{n \\to \\infty}} A^n \\begin{bmatrix} 25 \\\\ 0 \\end{bmatrix}\\)? 30.2.9 The matrix \\(A\\) below has the given eigenvalues and eigenvectors. \\[ A = \\left[ \\begin{array}{cc} \\frac{1}{2} &amp; \\frac{1}{5} \\\\ -\\frac{2}{5} &amp; \\frac{9}{10} \\\\ \\end{array} \\right] \\qquad \\begin{array}{c} \\lambda = .7 \\pm .2 i \\\\ \\mathsf{v} = \\begin{bmatrix} \\frac{1}{2} \\\\ 1 \\end{bmatrix} \\pm \\begin{bmatrix} -\\frac{1}{2} \\\\ 0 \\end{bmatrix} i \\end{array}\\hskip5in \\] Factor \\(A=PCP^{-1}\\) where \\(C\\) is a rotation-scaling matrix. What is the angle of rotation? What is the factor of dilation? 30.2.10 In a 1962 study of rainfall in Tel Aviv, it was determined that if today is a wet day, then the probability that tomorrow will be wet is 0.662 and the probability that tomorrow it will be dry is 0.338. If today is a dry day, then the probability that tomorrow is wet is 0.250 and the probability that tomorrow is dry will be 0.75. From this I computed the following: \\[ A = \\begin{bmatrix} 0.662 &amp; 0.25 \\\\ 0.338 &amp; 0.75\\end{bmatrix}; \\qquad \\begin{array}{cc} \\lambda_1 = 1.0 &amp; \\lambda_2 = 0.412 \\\\ \\mathsf{v}_1 = \\begin{bmatrix}-0.595 \\\\ -0.804 \\end{bmatrix} &amp; \\quad \\mathsf{v}_2 = \\begin{bmatrix}-0.707\\\\ 0.707 \\end{bmatrix} \\end{array} \\] If Monday is a dry day, what is the probability that Wednesday will be wet? In the long-run, what is the distribution of wet and dry days? 30.2.11 Conservation biologists in the Minnesota boundary waters have modeled the predator-prey dynamics of Grey Wolves \\(X\\) and White Tailed Deer \\(Y\\), which have a predator-prey relationship: \\[ \\left[ \\begin{array}{r} X_{t+1} \\\\ Y_{t+1} \\end{array} \\right] = \\left[ \\begin{array}{rr} 0.7 &amp; 0.4 \\\\ -0.2 &amp; 1.2 \\end{array} \\right] \\left[ \\begin{array}{r} X_{t} \\\\ Y_{t} \\end{array} \\right] \\qquad \\mbox{where} \\qquad X_0=1 \\mbox{ and } Y_0=2. \\] Here, \\(t\\) is measured in years and \\(X\\) and \\(Y\\) are measured in \\(100,000\\) animals. The plot below shows the vector field for this dynamical system. Draw or describe the trajectory of the populations starting at \\([X_0, Y_0] = [1,2]\\). In one or two sentences, describe what happens to these populations over time. The population biologists consider two options to fix this system. The first option is a deer breeding program to increase their productivity. Their adjusted model becomes: \\[ \\left[ \\begin{array}{r} X_{t+1} \\\\ Y_{t+1} \\end{array} \\right] = \\left[ \\begin{array}{rr} 0.7 &amp; 0.4 \\\\ -0.2 &amp; 1.3 \\end{array} \\right] \\left[ \\begin{array}{r} X_{t} \\\\ Y_{t} \\end{array} \\right] \\] whose transition matrix has eigenvectors and eigenvalues \\[ \\lambda_1 = 1.1 \\mbox{ with eigenvector } \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\quad \\mbox{and} \\quad \\lambda_2 = 0.9 \\mbox{ with eigenvector } \\begin{bmatrix} 1 \\\\ 0.5 \\end{bmatrix}. \\] Draw or describe the trajectory of the populations starting at \\((X_0, Y_0) = (1,2)\\) in the above vector plot. Describe what happens to these populations over time, and relate these observations to the eigenvectors and eigenvalues of the transition matrix. The second option they consider is a limited wolf hunting season. This reduces the productivity of the wolves, giving the new model: \\[ \\left[ \\begin{array}{r} X_{t+1} \\\\ Y_{t+1} \\end{array} \\right] = \\left[ \\begin{array}{rr} 0.6 &amp; 0.4 \\\\ -0.2 &amp; 1.2 \\end{array} \\right] \\left[ \\begin{array}{r} X_{t} \\\\ Y_{t} \\end{array} \\right] \\] whose transition matrix has eigenvectors and eigenvalues \\[ \\lambda_1 = 1 \\mbox{ with eigenvector } \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\quad \\mbox{and} \\quad \\lambda_2 = 0.8 \\mbox{ with eigenvector } \\begin{bmatrix} 1 \\\\ 0.5 \\end{bmatrix} \\] Once again, draw or desctibe the trajectory of the populations starting at \\((X_0, Y_0) = (1,2)\\) in the above vector plot. Describe what happens to these populations over time, and relate these observations to the eigenvectors and eigenvalues of the transition matrix. The conservation biologists want to create a stable ecosystem, where the populations of deer and wolves remain constant over time. Which option (deer breeding or limited wolf hunting) would you recommend? 30.2.12 A population of female bison is split into three groups: juveniles who are less than one year old; yearlings between one and two years old; and adults who are older than two years. Each year, * 80% of the juveniles survive to become yearlings. * 90% of the yearlings survive to become adults. * 80% of the adults survive. * 40% of the adults give birth to a juvenile Let \\(\\mathsf{x}_t = \\begin{bmatrix} J_t \\\\ Y_t \\\\ A_t \\end{bmatrix}\\) be the state of the system in year \\(t\\). Find the Leslie matrix \\(L\\) such that \\(\\mathsf{x}_{t+1} = B \\mathsf{x}_t.\\). Find the eigenvalues of \\(L\\). The matrix \\(L\\) has two complex eigenvalues and one real eigenvalue. How do the complex eigenvectors manifest in the trajectory of a population? What is the long-term behavior of the herd? Will the size of the herd grow, stablilize or shrink? What will be the proportions of juveniles, yearlings and adults in the herd? 30.3 Solutions to Practice Problems 30.3.1 There are three eigenvalues: 1, 2, and \\(-2\\). We find an eigenvector for each of them. * Eigenvalue \\(\\lambda = 1\\) \\[ A - I = \\left[ \\begin{array}{rrr} 1 &amp; -1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ -2 &amp; 5 &amp; -3 \\\\ \\end{array} \\right] \\sim \\left[ \\begin{array}{rrr} 1 &amp; -1 &amp; 0 \\\\ 0 &amp; 3 &amp; -3 \\\\ 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\sim \\left[ \\begin{array}{rrr} 1 &amp; -1 &amp; 0 \\\\ 0 &amp; 1 &amp; -1 \\\\ 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\sim \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; -1 \\\\ 0 &amp; 1 &amp; -1 \\\\ 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\] So one eigenvector is \\([1,1,1]^{\\top}\\) Eigenvalue \\(\\lambda = 2\\) \\[ A - 2I = \\left[ \\begin{array}{rrr} 0 &amp; -1 &amp; 0 \\\\ 0 &amp; -1 &amp; 0 \\\\ -2 &amp; 5 &amp; -4 \\\\ \\end{array} \\right] \\sim \\left[ \\begin{array}{rrr} -2 &amp; 5 &amp; -4 \\\\ 0 &amp; -1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\sim \\left[ \\begin{array}{rrr} -2 &amp; 0 &amp; -4 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\sim \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 2 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\] So one eigenvector is \\([-2,0,1]^{\\top}\\) Eigenvalue \\(\\lambda = -2\\) \\[ A - 2I = \\left[ \\begin{array}{rrr} 4 &amp; -1 &amp; 0 \\\\ 0 &amp; 3 &amp; 0 \\\\ -2 &amp; 5 &amp; 0 \\\\ \\end{array} \\right] \\sim \\left[ \\begin{array}{rrr} 4 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ -2 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\sim \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\] So one eigenvector is \\([0,0,1]^{\\top}\\) 30.3.2 This is a mapping that maps 2D space onto 1D space. We “lose a dimension” which means that \\(\\dim(\\mbox{Nul}(A))=1\\). This means that \\(\\lambda=0\\) is an eigenvalue because there is a nonzero vector \\(\\mathsf{x}\\) such that \\(A \\mathsf{x} = \\mathbf{0} = 0 \\mathsf{x}\\). We know that the other eigenvalue is nonzero, since we are mapping onto a line. However, we can’t say anything more without additional information. The eigenvalues are \\(\\lambda_1 = 1\\) and \\(\\lambda_2=-1\\). Let \\(L = \\mbox{span}( [a,b]^{\\top}\\) ) be the line of reflection. If \\(\\mathsf{v} = [a,b]^{\\top}\\) is on \\(L\\), then \\(T(\\mathsf{v}) = \\mathsf{v}\\).Since \\(\\mathsf{v}\\) is on the line of reflection, it doesn’t change. The vector \\(\\mathsf{w} = [-b,a]^{\\top}\\) is perpendicular to \\(L\\), so \\(T(\\mathsf{w}) = -\\mathsf{w}\\). Perhaps this one is easiest to undertand by looking at a particular example. Let’s look at a reflection across the \\(y\\)-axis. This means that \\(A [ x, y]^{\\top} = [-x, y]^{\\top}\\). In particular, we have \\(A [ 1, 0]^{\\top} = [ -x, 0]^{\\top} = - [ x, 0]^{\\top}\\). So \\(-1\\) is an eigenvalue. Meanwhile, we also have \\(A [ 0, 1]^{\\top} = [ 0, 1]^{\\top}\\) So 1 is an eigenvalue. The eigenvalues are \\(\\lambda_1 = \\lambda_2=-1\\) because \\[ T\\left( \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\right) = \\begin{bmatrix} -x_1 \\\\ -x_2 \\end{bmatrix} = \\begin{bmatrix} -1 &amp; 0 \\\\ 0 &amp; -1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\] There is one eigenvalue \\(\\lambda=1\\) with algebraic multiplicity 2. However, it only has geometric multiplicity 1. We know that a shear matrix is not diagonalizable. A shear always fixes one direction (the \\(x\\)-axis or the \\(y\\)-axis). Since these vectors map to themselves, we know that 1 is an eigenvalue. A shear matrix is always of the form We can also give a matrix proof. A 2D horizontal shear matrix is of the form \\(\\begin{bmatrix} 1 &amp; a \\\\ 0 &amp; 1 \\end{bmatrix}\\). A 2D vertical shear matrix is of the form \\(\\begin{bmatrix} 1 &amp; 0 \\\\ a &amp; 1 \\end{bmatrix}\\). Direct calculation shows that \\(\\lambda=1\\) has algebraic multiplicity 2 and geometric multiplicity 1. So there are no other eigenvalues. 30.3.3 \\(A\\) is not invertible because \\(0\\) is an eigenvalue. \\(A\\) is diagonalizable because it have 5 distinct eigenvalues. \\(B\\) is invertible because \\(0\\) is not an eigenvalue. \\(B\\) is diagonalizable because it have 5 distinct eigenvalues. \\(C\\) is invertible because \\(0\\) is not an eigenvalue. We cannot tell whether \\(C\\) is diagonalizable without more information. The eigenvalue \\(\\lambda=2\\) has algebraic multiplicity 2. We need to know whether the geometric multiplicity is 1 or 2. \\(D\\) is not invertible because \\(0\\) is an eigenvalue. We cannot tell whether \\(D\\) is diagonalizable without more information. The eigenvalue \\(\\lambda=3\\) has algebraic multiplicity 2. We need to know whether the geometric multiplicity is 1 or 2. 30.3.4 No, \\(A\\) is not invertible because \\(0\\) is an eigenvalue. \\(\\mathsf{v} = [1, -2, 1]^{\\top}\\) is an eigenvector for \\(\\lambda=0\\). Therefore \\(\\mathsf{v} \\in \\mbox{Nul}(A)\\). The vector \\(\\mathsf{v} = [0,1,2]^{\\top}\\) is an eigenvector for \\(\\lambda=1\\). So this is a steady-state vector. (However, the dynamical system will not converge to this steady state because \\(\\lambda=6\\) is the dominant eigenvalue.) When \\(A=P D P^{-1}\\), we can find the coordinates of a vector with respect to the eigenbasis via multiplication by \\(P^{-1}\\). Pinv =cbind(c(-1/6,0,1/6),c(-1/15,1/5,-1/3),c(1/30,2/5,1/6)) v = c(1,2,3) Pinv %*% v ## [,1] ## [1,] -0.2 ## [2,] 1.6 ## [3,] 0.0 So \\([ \\mathsf{v}]_{\\mathcal{B}} = [-1/5, 8/5, 0]^{\\top}\\). \\(-\\frac{1}{5} \\cdot 6^{2020} \\cdot \\begin{bmatrix} -5 \\\\ -2 \\\\ 1 \\end{bmatrix} + \\frac{8}{5} \\cdot \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\end{bmatrix}\\) 30.3.5 This system scales by \\(\\sqrt{1+1} = \\sqrt{2}\\) and it rotates by \\(\\tan^{-1} (1/1) = \\pi/4\\). 30.3.6 We have \\[ \\begin{bmatrix} a &amp; -b \\\\ b &amp; a \\end{bmatrix} = \\begin{bmatrix} .97 &amp; -.71\\\\ .71 &amp; .97 \\end{bmatrix} \\] Let’s turn to RStudio a = .97 b = .71 scale = sqrt(a^2+b^2) angle = atan (b/a) scale ## [1] 1.202082 angle ## [1] 0.6318544 2 * pi / angle ## [1] 9.94404 It takes 10 iterations to rotate past the \\(x\\)-axis. We are further from the origin because \\(| \\lambda| \\approx 1.2 &gt; 1\\). 30.3.7 The matrix \\(A\\) is diagonalizable because it has 3 distinct eigenvalues We must see whether \\(\\lambda=4\\) has geometric multiplicty 2 (to match its algebraic multiplicity). rref( cbind(c(-1,-1,2), c(-1,-1,2), c(2,2,-4))) ## [,1] [,2] [,3] ## [1,] 1 1 -2 ## [2,] 0 0 0 ## [3,] 0 0 0 We see that \\(B - 4I\\) has two free columns, so \\(\\dim ( \\mbox{Nul}(B-4I))=2\\). This means that \\(\\lambda=4\\) has geometric multiplicity 2. Therefore \\(B\\) is diagonalizable. 30.3.8 We set \\(P = \\begin{bmatrix} 2 &amp; 1 \\\\ 3 &amp; -1 \\end{bmatrix}\\). So \\[ P^{-1} = - \\frac{1}{5} \\begin{bmatrix} -1 &amp; -1 \\\\ -3 &amp; 2 \\end{bmatrix} = \\begin{bmatrix} 0.2 &amp; 0.2 \\\\ 0.6 &amp; -0.4 \\end{bmatrix} \\] Or we can find this inverse using RStudio. A = cbind(c(2,3),c(1,-1)) solve(A) ## [,1] [,2] ## [1,] 0.2 0.2 ## [2,] 0.6 -0.4 Therefore \\[ A = \\begin{bmatrix} 0.7 &amp; 0.2 \\\\ 0.3 &amp; 0.8 \\end{bmatrix} = \\begin{bmatrix} 2 &amp; 1 \\\\ 3 &amp; -1 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 0.5 \\end{bmatrix} \\begin{bmatrix} 0.2 &amp; 0.2 \\\\ 0.6 &amp; -0.4 \\end{bmatrix} \\] \\(\\lim_{n \\rightarrow \\infty} A^n = \\begin{bmatrix} 0.4 &amp; 0.6 \\\\ 0.4 &amp; 0.6 \\end{bmatrix}\\) because \\(\\lambda=1\\) is the dominant eigenvalue. So each column of \\(A\\) convergence to vector in the eigenspace for the dominant eigenvalue. Basically, we treat each column of \\(A\\) as a “starting vector” repeated multiplication by \\(A\\) converges to the dominant eigenspace. Here are the details. Let \\(\\mathsf{v}_1 = \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}\\) and \\(\\mathsf{v}_2 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\\). Let’s see why we can treat each column \\(\\mathsf{a}_1, \\mathsf{a}_2\\) of \\(A\\) as a “starting vector.” We can choose to view \\[A^2 = A \\begin{bmatrix} \\mathsf{a}_1 &amp; \\mathsf{a}_2 \\end{bmatrix} = \\begin{bmatrix} A \\mathsf{a}_1 &amp; A \\mathsf{a}_2 \\end{bmatrix} \\quad \\mbox{and} \\quad A^3 = A \\begin{bmatrix} A \\mathsf{a}_1 &amp; A \\mathsf{v}_2 \\end{bmatrix} = \\begin{bmatrix} A^{2} \\mathsf{a}_1 &amp; A^{2} \\mathsf{v}_2 \\end{bmatrix} \\] and in general \\[ A^{n} = \\begin{bmatrix} A^{n-1} \\mathsf{a}_1 &amp; A^{n-1} \\mathsf{a}_2 \\end{bmatrix}. \\] Therefore \\[ \\lim_{n \\rightarrow \\infty} A^n = \\lim_{n \\rightarrow \\infty} \\begin{bmatrix} A^{n-1} \\mathsf{a}_1 &amp; A^{n-1} \\mathsf{a}_2 \\end{bmatrix}. \\] So we need to write the columns of \\(A\\) as linear combinations of the eigenvectors \\(\\mathsf{v}_1, \\mathsf{v}_2\\). For any constants \\(c_1, c_2\\), we have \\[ \\lim_{n \\rightarrow \\infty} A^n (c_1 \\mathsf{v}_1 + c_2 \\mathsf{v}_2) = \\lim_{n \\rightarrow \\infty} \\left( c_1 A^n \\mathsf{v}_1 + c_2 A^n \\mathsf{v}_2 \\right) = \\lim_{n \\rightarrow \\infty} \\left( c_1 \\mathsf{v}_1 + c_2 (0.5)^n \\mathsf{v}_2 \\right) = c_1 \\mathsf{v}_1. \\] So we need to find \\(c_1,c_2\\) for each column of \\(A\\). Let’s use R Studio. a1 = c(.7,.3) a2 = c(.2,.8) P = cbind(c(2,3), c(1,-1)) solve(P,a1) ## [1] 0.2 0.3 solve(P,a2) ## [1] 0.2 -0.2 In each case, \\(c_1 = 0.2\\). Therefore \\[ \\lim_{n \\rightarrow \\infty} A^n = \\begin{bmatrix} 0.4 &amp; 0.4 \\\\ 0.6 &amp; 0.6 \\end{bmatrix}. \\] We need to find the coefficients for \\(x_0 = [25, 0]^{\\top}\\). P = cbind(c(2,3), c(1,-1)) v = c(25,0) solve(P,v) ## [1] 5 15 So the formula is \\[ 5 \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix} + 15 \\left( \\frac{1}{2} \\right)^n \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} \\] This converges to \\(5 \\begin{bmatrix} 2 \\\\ 3 \\end{bmatrix}\\). 30.3.9 We have \\[ A = \\left[ \\begin{array}{cc} \\frac{1}{2} &amp; \\frac{1}{5} \\\\ -\\frac{2}{5} &amp; \\frac{9}{10} \\\\ \\end{array} \\right] = \\begin{bmatrix} -1/2 &amp; 1/2 \\\\ 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} 0.7 &amp; -0.2 \\\\ 0.2 &amp; 0.7 \\end{bmatrix} \\begin{bmatrix} -2 &amp; 1 \\\\ 0 &amp; 1 \\end{bmatrix} \\] Here are some R calculations to check the answer for (a) and to find the values for (b) and (c). A = cbind(c(1/2,-2/5), c(1/5,9/10)) A ## [,1] [,2] ## [1,] 0.5 0.2 ## [2,] -0.4 0.9 eigen(A) ## eigen() decomposition ## $values ## [1] 0.7+0.2i 0.7-0.2i ## ## $vectors ## [,1] [,2] ## [1,] 0.4082483-0.4082483i 0.4082483+0.4082483i ## [2,] 0.8164966+0.0000000i 0.8164966+0.0000000i P = cbind(c(-1/2,0),c(1/2,1)) C = cbind(c(.7,.2),c(-.2,.7)) Pinv = solve(P) Pinv ## [,1] [,2] ## [1,] -2 1 ## [2,] 0 1 P %*% C %*% Pinv ## [,1] [,2] ## [1,] 0.5 0.2 ## [2,] -0.4 0.9 atan(.2/.7) ## [1] 0.2782997 sqrt(.7^2 + .2^2) ## [1] 0.728011 The angle of rotation is \\(\\tan^{-1} (.2/.7) = 0.278\\) radians The dilation factor is \\(\\sqrt{0.49 + 0.04} = \\sqrt{0.53} = 0.728\\). 30.3.10 Let’s use RStudio. A = cbind(c(0.662, 0.338),c(0.25, 0.75)) A %*% A %*% c(0,1) ## [,1] ## [1,] 0.353 ## [2,] 0.647 v1 = c(-0.595, -0.804 ) v1/sum(v1) ## [1] 0.4253038 0.5746962 If Monday is dry, then the probability of a wet Wednesday is \\(0.353\\). The easiest way to calculate this \\(A^2 \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}.\\) In the long run, \\(42.5\\%\\) of days are wet and \\(57.5\\%\\) of days are dry. 30.3.11 Initially, both populations increase from \\([1, 2]\\) to \\([2.25,2.5]\\), with wolves increasing more quickly. The wolves continue to increase, while the deer start to decrease. After reachaing \\([3,2]\\), both populations decrease. The deer die out first at the point \\([.8,0]\\). Deer breeding: Both populations thrive and grow every year, converging slowly to a \\(1:1\\) ratio. The long-term growth rate is \\(1.1\\), or \\(10\\%\\) growth. Wolf hunting: Once again, both populations increase. But this time we converge on a stable population of about \\([3, 3]\\). Since they want a stable ecosystem, the wolf hunting is a better option. We reach a stable equilibrium population of about \\([3, 3]\\) 30.3.12 Here is the Leslie matrix, as well as some eigensystem computations. L = cbind(c(0,.8,0),c(0,0,.9),c(.4,0,.8)) L ## [,1] [,2] [,3] ## [1,] 0.0 0.0 0.4 ## [2,] 0.8 0.0 0.0 ## [3,] 0.0 0.9 0.8 syst = eigen(L) syst ## eigen() decomposition ## $values ## [1] 1.0575217+0.0000000i -0.1287609+0.5057227i ## [3] -0.1287609-0.5057227i ## ## $vectors ## [,1] [,2] [,3] ## [1,] 0.3417536+0i -0.1097728+0.4311448i -0.1097728-0.4311448i ## [2,] 0.2585317+0i 0.6820256+0.0000000i 0.6820256+0.0000000i ## [3,] 0.9035297+0i -0.5097632-0.2775729i -0.5097632+0.2775729i syst$vectors[,1] / sum(syst$vectors[,1]) ## [1] 0.2272578+0i 0.1719172+0i 0.6008250+0i The eigenvalues are \\(1.058, -0.129 \\pm 0.506 i\\). If we start outside of the span of the dominant eigenvalue, then the trajectory will wiggle with a mild oscillation with an overall growth trend of \\(1.058\\), or \\(5.8\\%\\). The size of the herd grows. THe proportions are \\([0.227, 0.172, 0.601]\\). "],["quiz-4-review.html", "Vector 31 Quiz 4 Review 31.1 Overview 31.2 Practice Problems 31.3 Solutions to Practice Problems", " Vector 31 Quiz 4 Review 31.1 Overview Our fourth quiz covers sections 6.1-6.5, 7.1 and 7.4 in Lay’s book. This corresponds to Problem Set 8. The best way to study is to do practice problems. The Quiz will have calculation problems (like Edfinity) and more conceptual problems (like the problem sets). Here are some ways to practice: Make sure that you have mastered the Vocabulary, Skills and Concepts listed below. Look over the Edfinity homework assingments Do practice problems from the Edfinity Practice assignments. These allow you to “Practice Similar” by generating new variations of the same problem. Redo the Jamboard problems Try to resolve the Problem Sets and compare your answers to the solutions. Do the practice problems below. Compare your answers to the solutions. 31.1.1 Vocabulary, Concepts and Skills See the Week 7-8 Learning Goals for the list of vocabulary, concepts and skills. 31.2 Practice Problems 31.2.1 Let \\(\\mathsf{v} = \\begin{bmatrix}1 \\\\ -1 \\\\ 1 \\end{bmatrix}\\) and \\(\\mathsf{w}= \\begin{bmatrix}5 \\\\ 2 \\\\ 3 \\end{bmatrix}\\). Find \\(\\| \\mathsf{v} \\|\\) and \\(\\| \\mathsf{w} \\|\\). Find the distance between \\(\\mathsf{v}\\) and \\(\\mathsf{w}\\). Find the cosine of the angle between \\(\\mathsf{v}\\) and \\(\\mathsf{v}\\). Find \\(\\mbox{proj}_{\\mathsf{v}} \\mathsf{w}\\). Let \\(W=\\mbox{span} (\\mathsf{v}, \\mathsf{w})\\). Create an orthonormal basis \\(\\mathsf{u}_1, \\mathsf{u}_2\\) for \\(W\\) such that \\(\\mathsf{u}_1\\) is a vector in the same direction as \\(\\mathsf{v}\\). 31.2.2 Let \\(\\mathsf{u} \\neq 0\\) be a vector in \\(\\mathbb{R}^n\\). Define the function \\(T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) by \\(T(\\mathsf{x}) = \\mbox{proj}_{\\mathsf{u}} \\mathsf{x}\\). Prove that \\(T: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n\\) is a linear transformation. Recall that the kernel of \\(T\\) is the subspace \\(\\mbox{ker}(T) = \\{ \\mathsf{x} \\in \\mathbb{R}^n \\mid T(x) = \\mathbf{0} \\}\\). Describe \\(\\mbox{ker}(T)\\) as explicitly as you can. 31.2.3 The vectors \\(\\mathsf{u}_1, \\mathsf{u}_2\\) form an orthonormal basis of a subspace \\(W\\) of \\(\\mathbb{R}^4\\). Find the projection of \\(\\mathsf{v}\\) onto \\(W\\) and determine how close \\(\\mathsf{v}\\) is to \\(W\\). \\[ \\mathsf{u}_1 = \\frac{1}{2}\\begin{bmatrix} 1\\\\ -1\\\\ -1\\\\ 1 \\end{bmatrix}, \\quad \\mathsf{u}_2 = \\frac{1}{2}\\begin{bmatrix} 1\\\\ -1\\\\ 1\\\\ -1 \\end{bmatrix}, \\quad \\mathsf{v} = \\begin{bmatrix} 2\\\\ 2\\\\ 4\\\\ 2 \\end{bmatrix} \\] 31.2.4 Consider vectors \\(\\mathsf{v}_1 = \\begin{bmatrix} 1 \\\\ 1 \\\\-1 \\end{bmatrix}\\) and \\(\\mathsf{v}_2= \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\\) in \\(\\mathbb{R}^3\\). Let \\(W=\\mbox{span}(\\mathsf{v}_1, \\mathsf{v}_2)\\). Show that \\(\\mathsf{v}_1\\) and \\(\\mathsf{v}_2\\) are orthogonal. Find a basis for \\(W^{\\perp}\\). Use orthogonal projections to find the representation of \\(\\mathsf{y} = \\begin{bmatrix} 8 \\\\ 0 \\\\ 2 \\end{bmatrix}\\) as \\(\\mathsf{y} = \\hat{\\mathsf{y}} + \\mathsf{z}\\) where \\(\\hat{\\mathsf{y}} \\in W\\) and \\(\\mathsf{z} \\in W^{\\perp}\\). 31.2.5 Let \\(W\\) be the span of the vectors \\[ \\begin{bmatrix} 1 \\\\ -2 \\\\ 1 \\\\ 0 \\\\1 \\end{bmatrix}, \\quad \\begin{bmatrix} -1 \\\\ 3 \\\\ -1 \\\\ 1 \\\\ -1 \\end{bmatrix}, \\quad \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 3 \\\\1 \\end{bmatrix}, \\quad \\begin{bmatrix} 0 \\\\ 2 \\\\ 0 \\\\ 0 \\\\4 \\end{bmatrix} \\] Find a basis for \\(W\\). What is the dimension of this subspace? Use the Gram-Schmidt process on your answer to part (a) to find an orthonormal basis for \\(W\\) Find a basis for \\(W^{\\perp}\\) Use the Gram-Schmidt process on your answer from part (c) to find an orthogonal basis for \\(W^{\\perp}\\). 31.2.6 Let \\(\\mathsf{u}_1, \\mathsf{u}_1, \\ldots, \\mathsf{u}_n\\) be an orthonormal basis for \\(\\mathbb{R}^n\\). Pick any \\(\\mathsf{v} \\in \\mathbb{R}^n\\). Show that \\[ \\| \\mathsf{v} \\| = \\sqrt{ ( \\mathsf{v} \\cdot \\mathsf{u}_1)^2 + (\\mathsf{v} \\cdot \\mathsf{u}_2)^2 + \\cdots +(\\mathsf{v} \\cdot \\mathsf{u}_n)^2}. \\] 31.2.7 Consider the system \\(A \\mathsf{x} = \\mathsf{b}\\) given by \\[ \\begin{bmatrix} 1 &amp; 1 &amp; 1 \\\\ 1 &amp; 2 &amp; -1 \\\\ 1 &amp; 1 &amp; -1 \\\\ 1 &amp; 2 &amp; 1 \\end{bmatrix} \\begin{bmatrix} x_1\\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} 4\\\\ 1 \\\\ -2 \\\\ -1 \\end{bmatrix}. \\] Show that this system is inconsistent. Find the projected value \\(\\hat{\\mathsf{b}}\\), and the residual \\(\\mathsf{z}\\). How close is your approximate solution to the desired target vector? 31.2.8 According to the COVID Tracking Project, Minnesota had \\(54,463\\) positive COVID-19 cases between March 6 and 31 July, 2020. As of 16 December, 2020, that count has reached \\(386,412\\). The vector covid.mn lists the total number of new COVID-19 cases in Minnesota between August 1, 2020 and December 16, 2020 (on top of the previously reported \\(54,463\\)). covid.start = 54463 covid.mn = c(725, 1484, 2097, 2699, 3316, 4177, 4722, 5638, 6435, 7053, 7376, 7840, 8530, 9260, 9950, 10689, 11253, 11598, 12155, 12845, 13670, 14404, 15121, 15835, 16244, 16773, 17927, 18777, 19794, 20726, 21401, 21892, 22622, 23660, 24503, 25417, 26124, 26762, 27145, 27405, 27786, 28253, 29125, 29848, 30486, 30888, 31350, 32259, 33344, 34258, 35554, 36479, 36959, 37637, 38549, 39726, 41196, 42271, 43175, 43984, 44671, 45737, 46903, 48324, 49363, 50336, 51277, 52188, 53459, 54849, 56365, 57805, 58976, 60111, 61480, 62643, 64933, 66627, 68349, 69976, 71068, 72128, 73689, 75400, 77659, 79339, 80909, 83073, 84981, 87848, 91002, 94009, 96209, 99157, 102633, 106460, 110402, 115844, 120491, 126399, 130325, 135218, 140107, 147332, 152876, 161565, 169118, 176555, 182486, 187580, 195443, 202237, 208489, 215694, 222037, 228453, 234840, 234840, 240538, 249560, 258506, 264300, 267849, 273014, 279163, 284510, 290818, 296399, 301689, 304740, 309256, 312755, 316505, 320935, 324360, 327378, 329701, 331949) Find the best fitting exponential function \\(f(t) = a e^{k t}\\) for the number of COVID-19 cases in Minnesota since 31 July, 2020. Here \\(t\\) is the number of days since 31 July. Run the following code to plot your function. This code assumes that your least squares solution is given by xhat. Does it look like a good fit? a = exp(xhat[1]) k = xhat[2] f=function(y){a * exp(k*(y))} plot(x,f(x)+ covid.start,type=&quot;l&quot;,lwd=3,ylab=&quot;new positive COVID-19 cases&quot;, xlab=&quot;days since July 31, 2020&quot;, main=&quot;best fit exponential function&quot;) points(x,covid.mn + covid.start,pch=20,cex=.7,col=&quot;red&quot;) 31.2.9 Consider the symmetric matrix \\[A = \\begin{bmatrix} 3 &amp; 0 &amp; 34 &amp; 3 \\\\ 0 &amp; 6 &amp; -34 &amp; 0 \\\\ 34 &amp; -34 &amp; 74 &amp; 34 \\\\ 3 &amp; 0 &amp; 34 &amp; 3 \\end{bmatrix} \\] a. Use RStudio to find the eigenvalues \\(\\lambda_1 &gt; \\lambda_2 &gt; \\lambda_3 &gt; \\lambda_4\\) and their corresponding eigenvectors \\(\\mathsf{v}_1, \\mathsf{v}_2, \\mathsf{v}_3, \\mathsf{v}_3\\). Confirm that these eigenvectors form an orthonormal set. Is the linear transformation \\(T(\\mathsf{x}) = Ax\\) invertible? How do you know? Confirm that \\[ A = \\lambda_{1} \\mathsf{v}_1 \\mathsf{v}_1^{\\top} + \\lambda_{2} \\mathsf{v}_2 \\mathsf{v}_2^{\\top} + \\lambda_{3} \\mathsf{v}_3 \\mathsf{v}_3^{\\top} + \\lambda_{4} \\mathsf{v}_4 \\mathsf{v}_4^{\\top}. \\] Use your answer in part (d) to find the best rank 2 approximation for \\(A\\). (Be careful!) 31.2.10 The matrix \\[ A = \\left[ \\begin{array}{cccc} 1 &amp; 2 &amp; 5 \\\\ 2 &amp; 3 &amp; 6 \\\\ -1 &amp; 2 &amp; 3 \\\\ -3 &amp; 0 &amp; 1 \\\\ 1 &amp; 4 &amp; 5 \\\\ \\end{array} \\right] \\] has singular value decomposition \\[ \\left[ \\begin{array}{cccccc} 0.48 &amp; -0.02 &amp; 0.55 &amp; 0.63 &amp; -0.27 \\\\ 0.61 &amp; -0.24 &amp; 0.32 &amp; -0.63 &amp; 0.27 \\\\ 0.30 &amp; 0.43 &amp; -0.24 &amp; -0.32 &amp; -0.75 \\\\ 0.032 &amp; 0.87 &amp; 0.22 &amp; 0.00 &amp; 0.44 \\\\ 0.56 &amp; -0.01 &amp; -0.70 &amp; 0.32 &amp; 0.31 \\\\ \\end{array} \\right] \\left[ \\begin{array}{cccc} 11.4 &amp; 0 &amp; 0 \\\\ 0 &amp; 3.60 &amp; 0 \\\\ 0 &amp; 0 &amp; 1.39 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\left[ \\begin{array}{cccc} 0.16 &amp; -0.98 &amp; 0.062 \\\\ 0.49 &amp; 0.026 &amp; -0.87 \\\\ 0.86 &amp; 0.17 &amp; 0.49 \\\\ \\end{array} \\right]. \\] Find an orthonormal basis for each of the four fundamental subspaces \\(\\mbox{Nul}(A), \\mbox{Col}(A), \\mbox{Row}(A), \\mbox{Nul}(A^{\\top})\\). Confirm that your basis for \\(\\mbox{Nul}(A)\\) is orthogonal to your basis for \\(\\mbox{Row}(A)\\) Explain how we know that the mapping \\(T(\\mathsf{x}) = A\\mathsf{x}\\) is one-to-one. Since the mapping \\(T: \\mathbb{R}^3 \\rightarrow \\mathbb{R}^5\\) is one-to-one. It maps a cube in \\(\\mathbb{R}^3\\) to a 3D “retangular cuboid” in \\(\\mathbb{R}^5\\). Does the 3D volume expand, contract, or stay constant after the mapping? How do you know? What is the best rank 1 approximation of the matrix \\(A\\)? 31.2.11 Here is a matrix \\(A\\) and its reduced row echelon form \\(B\\) \\[ A = \\begin{bmatrix} 1 &amp; 3 &amp; -3 &amp; 1 &amp; 0 \\\\ 2 &amp; 1 &amp; 0 &amp; 6 &amp; 5 \\\\ 3 &amp; 3 &amp; -3 &amp; 6 &amp; 3 \\\\ -1 &amp; 4 &amp; -3 &amp; -3 &amp; -1 \\end{bmatrix} \\qquad \\longrightarrow \\qquad B = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 2.5 &amp; 1.5 \\\\ 0 &amp; 1 &amp; 0 &amp; 1.0 &amp; 2.0 \\\\ 0 &amp; 0 &amp; 1 &amp; 1.5 &amp; 2.5 \\\\ 0 &amp; 0 &amp; 0 &amp; 0.0 &amp; 0.0 \\end{bmatrix}. \\] Find a basis for \\(\\mbox{Nul}(A)\\) and \\(\\mbox{Col}(A)\\). Is the linear transformation \\(T(\\mathsf{x}) = A \\mathsf{x}\\) one-to-one? Onto? How is the SVD for \\(A\\) related to the SVD for \\(B\\)? What properties will they share? What properties will be different? Make some conjectures. Now find the SVD of both \\(A\\) and \\(B\\), and test your conjectures. Compare the singular values, the right singular vectors and the left singular vectors. Be sure to compare each of the four fundamental subspaces: \\(\\mbox{Nul}(M), \\mbox{Col}(M), \\mbox{Row}(M), \\mbox{Nul}(M^{\\top})\\). 31.2.12 Singular value decomposition can be used for clustering of high dimensional data. Indeed, the singular vectors encode the patterns in a matrix. So let’s try this out on the Senate votes from the 109th Congress (from Problem Set 8). Let’s load in the data and assign a color to each senator according to their party. library(readr) senate.vote.file = &quot;https://raw.github.com/mathbeveridge/math236_f20//main/data/SenateVoting109.csv&quot; senators &lt;- read_csv(senate.vote.file, col_names = TRUE) ## Rows: 99 Columns: 49 ## ── Column specification ──────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (3): Name, Party, State ## dbl (46): V01, V02, V03, V04, V05, V06, V07, V08, V09, V10, ... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. sen.mat &lt;- as.matrix(senators[4:49]) sen.color=rep(&quot;goldenrod&quot;, dim(senators)[1]) sen.color[senators$Party==&#39;D&#39;]=&quot;cornflowerblue&quot; sen.color[senators$Party==&#39;R&#39;]=&quot;firebrick&quot; Find the singular value decomposition. Define singval to be the singular values. Plot them using the command plot(singval,pch=19,cex=.5,col='blue'). How many “important” singular values are there? Define U to be the matrix of left singular vectors. Plot the important singular vectors one by one. Here is a command to plot the first singular vector: `plot(U[,1],pch=19,cex=.75,col=sen.color). Do any of the singular values distinguish Democrats from Republicans along the \\(y\\)-axis? Remark: When we explored Voting Patterns in the US Senate, we were actually using singular value decomposition to explore those other datasets. In this problem, you looked at a rectangular matrix \\(A\\) for senators (rows) and votes (columns). In the previous exploration, you were given a square matrix that was (essentially) \\(AA^{\\top} = (A^{\\top}A)^{\\top}\\). So the eigenvectors of that square matrix are the singular values of the rectangular matrix \\(A\\). 31.3 Solutions to Practice Problems 31.3.1 \\[\\begin{align} \\| \\mathsf{v} \\| &amp;= \\sqrt{ \\mathsf{v} \\cdot \\mathsf{v}} = \\sqrt{1+1+1} = \\sqrt{3} \\\\ \\| \\mathsf{w} \\| &amp;= \\sqrt{ \\mathsf{w} \\cdot \\mathsf{vw}} = \\sqrt{25+4+9} = \\sqrt{38} \\\\ \\end{align}\\] We have \\(\\mathsf{v} - \\mathsf{w} = \\begin{bmatrix} -4 \\\\ -3 \\\\ -2 \\end{bmatrix}\\) and so \\[ \\| \\mathsf{v} - \\mathsf{w}\\| = \\sqrt{16+9+4} = \\sqrt{29} \\] \\[ \\cos \\theta = \\frac{\\mathsf{v} \\cdot \\mathsf{w}}{\\| \\mathsf{v} \\| \\, \\|\\mathsf{w} \\| } = \\frac{5-2+3}{\\sqrt{3} \\, \\sqrt{38} } = \\frac{2\\sqrt{3}}{\\sqrt{38} } \\] \\[ \\hat{\\mathsf{w}} = \\mbox{proj}_{\\mathsf{v}} \\mathsf{w} = \\frac{\\mathsf{v} \\cdot \\mathsf{w}}{ \\mathsf{v} \\cdot \\mathsf{v} } \\, \\mathsf{v} = \\frac{5-2+3}{1+1+1} \\mathsf{v} = 2 \\mathsf{v} = \\begin{bmatrix} 2 \\\\ -2 \\\\ 2 \\end{bmatrix} \\] Using \\(\\hat{\\mathsf{w}}\\) from the previous problem, we know that \\[ \\mathsf{z} = \\mathsf{w} - \\hat{\\mathsf{w}} = \\begin{bmatrix} 5 \\\\ 2 \\\\ 3 \\end{bmatrix} - \\begin{bmatrix} 2 \\\\ -2 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 4 \\\\ 1 \\end{bmatrix} \\] is orthogonal to \\(\\mathsf{v}\\).So an orthonormal basis is \\[ \\frac{1}{\\sqrt{3}} \\begin{bmatrix} 1 \\\\ -1 \\\\ 1 \\end{bmatrix} \\quad \\mbox{and} \\quad \\frac{1}{\\sqrt{26}} \\begin{bmatrix} 3 \\\\ 4 \\\\ 1 \\end{bmatrix} \\] 31.3.2 We check that \\(T(\\mathsf{x} + \\mathsf{y}) = T(\\mathsf{x}) + T(\\mathsf{y})\\) and that \\(T(c\\mathsf{x} ) = c \\, T(\\mathsf{x}).\\) To make things a little more clear, let’s write \\[ T(\\mathsf{x}) = (\\mathsf{x} \\cdot \\mathsf{u}) \\, \\frac{1}{\\mathsf{u} \\cdot \\mathsf{u}} \\mathsf{u} = (\\mathsf{x} \\cdot \\mathsf{u}) \\, \\frac{1}{\\|\\mathsf{u} \\|^2} \\mathsf{u}. \\] We have \\[\\begin{align} T(\\mathsf{x} + \\mathsf{y}) &amp;= \\big((\\mathsf{x}+\\mathsf{y}) \\cdot \\mathsf{u} \\big) \\, \\frac{1}{\\|\\mathsf{u} \\|^2} \\mathsf{u} = \\big(\\mathsf{x} \\cdot \\mathsf{u}+ \\mathsf{y} \\cdot \\mathsf{u} \\big) \\, \\frac{1}{\\|\\mathsf{u} \\|^2} \\mathsf{u} \\\\ &amp;= \\big(\\mathsf{x} \\cdot \\mathsf{u} \\big) \\, \\frac{1}{\\|\\mathsf{u} \\|^2} \\mathsf{u} + \\big(\\mathsf{y} \\cdot \\mathsf{u} \\big) \\, \\frac{1}{\\|\\mathsf{u} \\|^2} \\mathsf{u} = T(\\mathsf{x}) + T( \\mathsf{y}) \\end{align}\\] and \\[ T(c\\mathsf{x}) = \\big(c\\mathsf{x}\\cdot \\mathsf{u} \\big) \\, \\frac{1}{\\|\\mathsf{u} \\|^2} \\mathsf{u} = c \\big(\\mathsf{x}\\cdot \\mathsf{u} \\big) \\, \\frac{1}{\\|\\mathsf{u} \\|^2} \\mathsf{u} = c \\, T(\\mathsf{x}) \\] Here are a few ways to describe \\(\\mbox{ker}(T)\\). \\(\\mbox{ker}(T) = \\{ \\mathsf{x} \\in \\mathbb{R}^n \\mid \\mathsf{x} \\cdot \\mathsf{u} = 0 \\}\\). \\(\\mbox{ker}(T)\\) is the set of vectors that are orthogonal to \\(\\mathsf{u}\\). Let \\(A\\) be the \\(1 \\times n\\) matrix \\(\\mathsf{u}^{\\top}\\). Then \\(\\mbox{ker}(T)= \\mbox{Nul}(A)\\). 31.3.3 We have \\(\\mathsf{u}_1 \\cdot \\mathsf{v} = 2-2-4+2=-2\\) and \\(\\mathsf{u}_1 \\cdot \\mathsf{v} = 2-2+4-2=2\\) so \\[ \\hat{\\mathsf{v}} = \\mbox{proj}_W \\mathsf{v} = -2 \\mathsf{u}_1 + 2 \\mathsf{u}_2 = \\begin{bmatrix} 1 \\\\ -1 \\\\ -1 \\\\ 1 \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ -1 \\\\ 1 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ -2 \\\\ 0 \\\\ 0 \\end{bmatrix} \\] with residual vector \\[ \\mathsf{z} = \\mathsf{v} - \\hat{\\mathsf{v}} = \\begin{bmatrix} 2 \\\\ 2 \\\\ 4 \\\\ 2 \\end{bmatrix} - \\begin{bmatrix} 2 \\\\ -2 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 6 \\\\ 4 \\\\ 2 \\end{bmatrix} \\] and the distance is \\(\\| \\mathsf{z} \\| = \\sqrt{36 + 16 + 4} = \\sqrt{56}\\). 31.3.4 \\(\\mathsf{v}_1 \\cdot \\mathsf{v}_2 = 1 +2 - 3 =0\\). We must find \\(\\mbox{Nul}(A)\\) where \\(A = \\begin{bmatrix} \\mathsf{v}_1^{\\top} \\\\ \\mathsf{v}_2^{\\top}\\end{bmatrix}\\). \\[ \\begin{bmatrix} 1 &amp; 1 &amp; -1 \\\\ 1 &amp; 2 &amp; 3 \\end{bmatrix} \\longrightarrow \\begin{bmatrix} 1 &amp; 1 &amp; -1 \\\\ 0 &amp; 1 &amp; 4 \\end{bmatrix} \\longrightarrow \\begin{bmatrix} 1 &amp; 0 &amp; -5 \\\\ 0 &amp; 1 &amp; 4 \\end{bmatrix} \\] so the vector \\(\\begin{bmatrix} 5 \\\\ -4 \\\\ 1 \\end{bmatrix}\\) is a basis for \\(W^{\\perp}\\) We have \\[\\begin{align} \\hat{\\mathsf{y}} &amp;= \\frac{\\mathsf{y} \\cdot \\mathsf{v_1}}{\\mathsf{v_1} \\cdot \\mathsf{v_1}} \\, \\mathsf{v_1} + \\frac{\\mathsf{y} \\cdot \\mathsf{v_2}}{\\mathsf{v_2} \\cdot \\mathsf{v_2}} \\, \\mathsf{v_2} = \\frac{8-2}{1+1+1} \\mathsf{v_1} + \\frac{8+6}{1+4+9} \\mathsf{v_2} \\\\ &amp;= 2\\mathsf{v_1} +\\mathsf{v_2} = \\begin{bmatrix} 2 \\\\ 2 \\\\ -2 \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ 4 \\\\ 1 \\end{bmatrix} \\end{align}\\] and so \\[ \\mathsf{z} = \\mathsf{y} - \\hat{\\mathsf{y}} = \\begin{bmatrix} 8 \\\\ 0 \\\\ 2 \\end{bmatrix} - \\begin{bmatrix} 3 \\\\ 4 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ -4 \\\\ 1 \\end{bmatrix}. \\] 31.3.5 a .We will answer this one using RStudio. A = cbind(c(1,-2,1,0,1), c(-1,3,-1,1,-1), c(0,0,1,3,1), c(0,2,0,0,4)) rref(A) ## [,1] [,2] [,3] [,4] ## [1,] 1 0 0 0 ## [2,] 0 1 0 0 ## [3,] 0 0 1 0 ## [4,] 0 0 0 1 ## [5,] 0 0 0 0 So we need all four vectors to span the column space. RStudio makes this so easy! gramSchmidt(A) ## $Q ## [,1] [,2] [,3] [,4] ## [1,] 0.3779645 0.2390457 -8.164966e-01 -0.26233033 ## [2,] -0.7559289 0.3585686 -4.082483e-01 0.26233033 ## [3,] 0.3779645 0.2390457 2.719480e-16 -0.09837388 ## [4,] 0.0000000 0.8366600 4.082483e-01 -0.26233033 ## [5,] 0.3779645 0.2390457 2.719480e-16 0.88536488 ## ## $R ## [,1] [,2] [,3] [,4] ## [1,] 2.645751 -3.401680 0.7559289 0.0000000 ## [2,] 0.000000 1.195229 2.9880715 1.6733201 ## [3,] 0.000000 0.000000 1.2247449 -0.8164966 ## [4,] 0.000000 0.000000 0.0000000 4.0661202 We obtain a basis for \\(W^{\\perp}\\) by finding \\(\\mbox{Nul(A^{\\top})}\\) So let’s row reduce \\(A^{\\top}\\) rref(t(A)) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 0 0 0 -2 ## [2,] 0 1 0 0 2 ## [3,] 0 0 1 0 7 ## [4,] 0 0 0 1 -2 The vector \\(\\begin{bmatrix} 2 \\\\ -2 \\\\ -7 \\\\ 2 \\\\ 1\\end{bmatrix}\\) spans \\(W^{\\perp}\\) Using Gram-Schmidt is overkill for this problem. I’ll just divide by the length of the vector. vec = c(2,-2,-7,2,1) vec / Norm(vec) ## [1] 0.2540003 -0.2540003 -0.8890009 0.2540003 0.1270001 31.3.6 We will show that \\(\\| \\mathsf{v} \\|^2 = ( \\mathsf{v} \\cdot \\mathsf{u}_1)^2 + (\\mathsf{v} \\cdot \\mathsf{u}_2)^2 + \\cdots +(\\mathsf{v} \\cdot \\mathsf{u}_n)^2.\\) Let’s write \\(\\mathsf{v}\\) in terms of the orthonormal basis: \\(\\mathsf{v} = c_1 \\mathsf{u}_1 + c_2 \\mathsf{u}_2 + \\cdots + c_n \\mathsf{u}_n\\). We then have \\[\\begin{align} \\| \\mathsf{v} \\|^2 &amp;= \\mathsf{v} \\cdot \\mathsf{v} \\\\ &amp;= (c_1 \\mathsf{u}_1 + c_2 \\mathsf{u}_2 + \\cdots + c_n \\mathsf{u}_n) \\cdot (c_1 \\mathsf{u}_1 + c_2 \\mathsf{u}_2 + \\cdots + c_n \\mathsf{u}_n) \\\\ &amp;= c_1^2 + c_2 + \\cdots + c_n^2 \\end{align}\\] because \\(\\mathsf{u}_i \\cdot \\mathsf{u}_i =1\\) and \\(\\mathsf{u}_i \\cdot \\mathsf{u}_j=0\\) for \\(i \\neq j\\). Finally, we note that \\(\\mathsf{v} \\cdot \\mathsf{u_i} = c_i\\) using the same facts about the dot projects for the orthonormal basis. So we have verified the claim above. 31.3.7 A = cbind(c(1,1,1,1), c(1,2,1,2),c(1,-1,-1,1)) b = c(4,1,-2,-1) rref(cbind(A,b)) ## b ## [1,] 1 0 0 0 ## [2,] 0 1 0 0 ## [3,] 0 0 1 0 ## [4,] 0 0 0 1 There is a pivot in the last column of this augmented matrix, so this system is inconsistent. Here is the least squares calculation. #solve the normal equation (xhat = solve(t(A) %*% A, t(A) %*% b)) ## [,1] ## [1,] 2 ## [2,] -1 ## [3,] 1 # find the projection (bhat = A %*% xhat) ## [,1] ## [1,] 2.000000e+00 ## [2,] -1.000000e+00 ## [3,] 6.661338e-16 ## [4,] 1.000000e+00 # find the residual vector (z = b - bhat) ## [,1] ## [1,] 2 ## [2,] 2 ## [3,] -2 ## [4,] -2 # check that z is orthogonal to Col(A) t(A) %*% z ## [,1] ## [1,] -8.881784e-16 ## [2,] 0.000000e+00 ## [3,] 0.000000e+00 # measure the distance between bhat and b sqrt( t(z) %*% z) ## [,1] ## [1,] 4 The projection is \\(\\hat{\\mathsf{b}} = [2,-1,0,1]^{\\top}\\). The residual is \\(\\mathsf{z} = [2,2,-2,-2]^{\\top}\\) The distance of between \\(\\mathsf{b}\\) and \\(\\hat{\\mathsf{b}}\\) is \\[ \\| = \\| \\mathsf{z} \\| = \\sqrt{4+4+4+4} = \\sqrt{16} = 4. \\] ### x = 1:length(covid.mn) y = log(covid.mn) A = cbind(x^0, x) (xhat = solve(t(A) %*% A, t(A) %*% y)) ## [,1] ## 8.66588980 ## x 0.03138331 a = exp(xhat[1]) k = xhat[2] f=function(y){a * exp(k*(y))} plot(x,f(x)+ covid.start,type=&quot;l&quot;,lwd=3,ylab=&quot;new positive COVID-19 cases&quot;, xlab=&quot;days since July 31, 2020&quot;, main=&quot;best fit exponential function&quot;) points(x,covid.mn + covid.start,pch=20,cex=.7,col=&quot;red&quot;) The curve is a pretty good fit (unfortunately?). However, it does look like the additional restrictions of the last two weeks are slowing the COVID spread. 31.3.8 A = cbind(c(3,0,34,3), c(0,6,-34,0), c(34,-34,74,34),c(3,0,34,3)) A ## [,1] [,2] [,3] [,4] ## [1,] 3 0 34 3 ## [2,] 0 6 -34 0 ## [3,] 34 -34 74 34 ## [4,] 3 0 34 3 syst = eigen(A) (P = syst$vectors) ## [,1] [,2] [,3] [,4] ## [1,] -0.2886751 4.082483e-01 -7.071068e-01 0.5 ## [2,] 0.2886751 8.164966e-01 -3.295975e-15 -0.5 ## [3,] -0.8660254 -3.330669e-16 7.771561e-16 -0.5 ## [4,] -0.2886751 4.082483e-01 7.071068e-01 0.5 zapsmall(P %*% t(P)) ## [,1] [,2] [,3] [,4] ## [1,] 1 0 0 0 ## [2,] 0 1 0 0 ## [3,] 0 0 1 0 ## [4,] 0 0 0 1 The eigenvectors are the columns of the second matrix \\(P\\) shown above. The last matrix shows that \\(P P^{\\top} = I\\), so the columns are orthonormal. ## [1] 108 6 0 -28 \\(A\\) is not invertible because 0 is an eigenvalue. v1 = syst$vectors[,1] v2 = syst$vectors[,2] v3 = syst$vectors[,3] v4 = syst$vectors[,4] syst$values[1] * v1 %*% t(v1) + syst$values[2] * v2 %*% t(v2) + syst$values[3] * v3 %*% t(v3) + syst$values[4] * v4 %*% t(v4) ## [,1] [,2] [,3] [,4] ## [1,] 3.000000e+00 -1.776357e-15 34 3.000000e+00 ## [2,] -1.776357e-15 6.000000e+00 -34 5.595524e-14 ## [3,] 3.400000e+01 -3.400000e+01 74 3.400000e+01 ## [4,] 3.000000e+00 5.595524e-14 34 3.000000e+00 We must remember to use the eigenvalues of largest magnitude: these are \\(\\lambda_1 = 108\\) and \\(\\lambda_4 = -28\\). syst$values[1] * v1 %*% t(v1) + syst$values[4] * v4 %*% t(v4) ## [,1] [,2] [,3] [,4] ## [1,] 2 -2 34 2 ## [2,] -2 2 -34 -2 ## [3,] 34 -34 74 34 ## [4,] 2 -2 34 2 31.3.9 This SVD factorization is \\(U \\Sigma V^{\\top}\\). \\(\\mbox{Nul}(A) = \\{ 0 \\}\\) so it has no basis. An orthonormal basis for \\(\\mbox{Col}(A)\\) is the first three columns of \\(U\\). An orthonormal basis for \\(\\mbox{Row}(A)\\) is the three rows of \\(V^{\\top}\\). +An orthonormal basis for \\(\\mbox{Nul}(A^{\\top})\\) is the last two columns of \\(U\\). This is true because the zero vector is orthogonal to every vector. The matrix \\(\\Sigma\\) has 3 pivots. So the nullspace of \\(A\\) is trivial. The 3D volume expands because the product of the singular values is greater than 1. 11.4 * cbind(c(0.48,0.61,0.30,0.032,0.56)) %*% rbind(c(0.16,-0.98,0.062)) ## [,1] [,2] [,3] ## [1,] 0.875520 -5.362560 0.3392640 ## [2,] 1.112640 -6.814920 0.4311480 ## [3,] 0.547200 -3.351600 0.2120400 ## [4,] 0.058368 -0.357504 0.0226176 ## [5,] 1.021440 -6.256320 0.3958080 31.3.10 The null space is the span of \\([-2.5,-1,-1.5,1,0]^{\\top}\\) and \\([-1.5,-2,-2.5,0,1]^{\\top}\\). The column space is the span of \\([1,2,3,-1]^{\\top}\\) and \\([3,1,3,4]^{\\top}\\) and \\([-3,0,-3,-3]^{\\top}\\) This mapping is not one-to-one because the null space is two-dimensional. The mapping is not onto because there is no pivot in the final row of \\(B\\). This is a conceptual question without a particular “right answer.” Here are some observations. The nullspace of \\(A\\) is the same as the nullspace of \\(B\\), so an orthogonal basis for \\(\\mbox{Nul}(A)\\) is also an orthogonal basis for \\(\\mbox{Nul}(B)\\). However, the orthonormal basis vector in the SVD for \\(A\\) do not need to be the same as the orthonormal basis in the SVD for \\(B\\). The columnspace of \\(A\\) is different from the columnspace of \\(B\\). However, each of them is three-dimensional. The singular values for \\(A\\) do not need to equal the singular values of \\(B\\). Row reduction change the determinant of a square matrix. So it is safe to assume that it will have a similar effect on singular values. However, we konw that both will have 3 singular values since \\(A\\) and \\(B\\) have the same rank. A = rbind(c(1,3,-3,1,0),c(2,1,0,6,5),c(3,3,-3,6,3),c(-1,4,-3,-3,-1)) B = rref(A) A ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 3 -3 1 0 ## [2,] 2 1 0 6 5 ## [3,] 3 3 -3 6 3 ## [4,] -1 4 -3 -3 -1 B ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 0 0 2.5 1.5 ## [2,] 0 1 0 1.0 2.0 ## [3,] 0 0 1 1.5 2.5 ## [4,] 0 0 0 0.0 0.0 (Asvd = svd(A)) ## $d ## [1] 1.170522e+01 7.291802e+00 1.953827e+00 1.048298e-15 ## ## $u ## [,1] [,2] [,3] [,4] ## [1,] -0.2074163 -0.5081310 0.3142951 0.7745967 ## [2,] -0.6655245 0.2640904 -0.6485883 0.2581989 ## [3,] -0.7060981 -0.2378931 0.4220968 -0.5163978 ## [4,] 0.1244229 -0.7845164 -0.5498965 -0.2581989 ## ## $v ## [,1] [,2] [,3] [,4] ## [1,] -0.3230339 0.01246428 0.4264992 -0.6959338 ## [2,] -0.2484683 -0.70106777 -0.3270500 0.2609756 ## [3,] 0.2022409 0.62969638 -0.2863539 0.1988388 ## [4,] -0.7526916 0.27463749 0.3096671 0.5095232 ## [5,] -0.4758851 0.19080183 -0.7302360 -0.3852494 (Bsvd = svd(B)) ## $d ## [1] 4.649483 1.543473 1.000000 0.000000 ## ## $u ## [,1] [,2] [,3] [,4] ## [1,] 0.6088198 0.7877604 -0.09365858 0 ## [2,] 0.4779319 -0.4584513 -0.74926865 0 ## [3,] 0.6331821 -0.4114072 0.65561007 0 ## [4,] 0.0000000 0.0000000 0.00000000 1 ## ## $v ## [,1] [,2] [,3] [,4] ## [1,] 0.1309436 0.5103818 -9.365858e-02 -0.82024886 ## [2,] 0.1027925 -0.2970259 -7.492686e-01 0.05990659 ## [3,] 0.1361833 -0.2665465 6.556101e-01 -0.04871373 ## [4,] 0.6344264 0.5791090 6.938894e-17 0.49438789 ## [5,] 0.7424586 -0.4948451 -5.551115e-17 -0.27714724 # check that the singular values are different Asvd$d ## [1] 1.170522e+01 7.291802e+00 1.953827e+00 1.048298e-15 Bsvd$d ## [1] 4.649483 1.543473 1.000000 0.000000 Arow = Asvd$v[,1:3] Acol = Asvd$u[,1:3] Brow = Bsvd$v[,1:3] Bcol = Bsvd$u[,1:3] # check that the rowspaces are the same # this also means that the nullspaces are the same rref(cbind(Arow, Brow)) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 0 0 -0.8711504 0 0.4023226 ## [2,] 0 1 0 0.3312215 0 0.9197649 ## [3,] 0 0 1 -0.3624766 0 -0.1264565 ## [4,] 0 0 0 0.0000000 1 0.1543445 ## [5,] 0 0 0 0.0000000 0 0.0000000 rref(cbind(Brow, Arow)) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 0 0 -0.8711504 0.3312215 0 ## [2,] 0 1 0 -0.3453768 0.1113812 0 ## [3,] 0 0 1 0.3490156 0.9369560 0 ## [4,] 0 0 0 0.0000000 0.0000000 1 ## [5,] 0 0 0 0.0000000 0.0000000 0 # check that the columnspaces are different rref(cbind(Acol, Bcol)) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 0 0 0 2.77467300 -1.955659 ## [2,] 0 1 0 0 0.45357378 -1.059136 ## [3,] 0 0 1 0 -0.01928201 1.068529 ## [4,] 0 0 0 1 2.62771978 -2.255687 31.3.11 decomp = svd(sen.mat) singval = decomp$d plot(singval,pch=19,cex=.5,col=&#39;blue&#39;) There are two very important eigenvectors. After those two gaps, the eigenvalues taper off more slowly. We certainly need more than the first two for a good approximation of the matrix. But the importance of the rest is more even. The first left singular vector picks up a mild (but not definitive) layering of Republicans and Democrats. The second left singular vector provides a clear separation! U = decomp$u plot(U[,1],pch=19,cex=.75,col=sen.color) plot(U[,2],pch=19,cex=.75,col=sen.color) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
